,video_id,paragraph,questions
0,-1BnXEwHUok,"So what we got with the sample was 0.
So what's the obvious thing to do? If you're doing a simulation of an event and the event is pretty rare, you want to try it on a very large number of trials.
So let's go back to our code.
And we'll change it to instead of 1,000, 1,000,000.
You can see up here, by the way, where I set the seed.
And now, let's run it.
We did a lot better.
If we look at here our estimated probability, it's three 0's 128, still not quite the actual probability but darn close.
And maybe if I had done 10 million, it would have been even closer.
So if you're writing a simulation to compute the probability of an event and the event is moderately rare, then you better run a lot of trials before you believe your estimated probability.
In a week or so, we'll actually look at that more mathematically and say, what is a lot, how do we know what is enough.
What are the morals here? Moral one, I've just told you-- takes a lot of trials to get a good estimate of the frequency of a rare event.
Moral two, we should always, if we're getting an estimated probability, know that, and probably say that, and not confuse it with the actual probability.",How can we determine the number of samples needed to ensure that the estimated probability matches the actual probability? Thank you.
1,-DP1i2ZU9gk,"And the second thing that represents a list is going to be this second part, which is a pointer.
And internally this pointer is going to tell Python where is the memory location in the computer where you can access the element index 1.
So it's just essentially going to be a chain, going from one index to the other.
And at the next memory location you have the value at index 1, and then you have another pointer that takes you to the location in memory where the index 2 is located.
And in index 2 you have the value and then the next pointer, and so on and so on.
So this is how Python internally represents a list.
OK? How you manipulate lists, we've done this a lot, right? You can index into a list, you can add two lists together, you can get the length, you can append to the end of a list, you can sort a list, reverse a list, and so many other things, right? So these are all ways that you can interact with the list object as soon as you've created it.","I would like to seek clarification on the following point: The diagram suggests that Python lists are internally represented as linked lists. However, based on my understanding, this is incorrect; Python lists are actually represented as dynamic arrays, as explained in this video: https://youtu.be/CHhwJjR0mZA?t=2082."
2,-DP1i2ZU9gk,"And then I've also implemented some other special methods.
How do I add two fractions? How do I subtract two fractions? And how do I convert a fraction to a float? The add and subtract are almost the same, so let's look at the add for the moment.
How do we add two fractions? We're going to take self, which is the instance of an object that I want to do the add operation on, and we're going to take other, which is the other instance of an object that I want to do the operation on, so the addition, and I'm going to figure out the new top.
So the new top of the resulting fraction.
So it's my numerator multiplied by the other denominator plus my denominator multiplied by the other numerator and then divided by the multiplication of the two denominators.
So the top is going to be that, the bottom is going to be that.
Notice that we're using self dot, right? Once again, we're trying to access the data attributes of each different instance, right, of myself and the other object that I'm working with.
So that's why I have to use self dot here.
Once I figure out the top and the bottom of the addition, I'm going to return, and here notice I'm returning a fraction object.
It's not a number, it's not a float, it's not an integer.
It's a new object that is of the exact same type as the class that I'm implementing.
So as it's the same type of object, then on the return value I can do all of the exact same operations that I can do on a regular fraction object.
Sub is going to be the same.
I'm returning a fraction object.
Float is just going to do the division for me, so it's going to take the numerator and then divide it by the denominator, just divide the numbers.","Firstly, thank you for this great tutorial! Why do we have an inverse function without the leading and trailing underscores? What does it mean when those underscores make a function ""magical""?
""In the expression 'c = a + b,' how are 'a' and 'b' assigned to 'self' and 'other'? How can you determine which value is assigned to each? I would appreciate an explanation of what occurs and where the values of 'a' and 'b' are directed."""
3,-DP1i2ZU9gk,"And this one on the right essentially says, what's the name of the class, dot, dot notation, what's the method you want to call, and then in parentheses you give it all of the variables including self.
OK.
So in this case you're explicitly telling Python that self is C and other is 0.
So this is a little bit easier to understand, like that.
But it's a little cumbersome because you always have to write coordinate dot, coordinate dot, coordinate dot, for every data attribute you might want to access, for every procedural attribute you might want to access.
So by convention, it's a lot easier to do the one on the left.
And as I mentioned, Python implicitly says, if you're doing the one on the left, you can call this method on a particular object and it's going to look up the type of the object and it's going to essentially convert this on the left to the one on the right.
And this is what you've been using so far.
So when you create a list, you say L is equal to 1, 2, and then you say L.append, you know, 3 or whatever.",Can we write zero.distance(c) instead of c.distance(zero)?
4,09mb78oiPkA,"PATRICK WINSTON: Yes? [? SPEAKER 1: Are we, ?] necessarily, have it done with some sort of a [? politidy distance ?] metric? PROF.
PATRICK WINSTON: Oh, here we go.
We're not going to use any [? politidy distance ?] metric.
We're going to use some other metric.
SPEAKER 1: Like alogrithmic, or whatnot? PROF.
PATRICK WINSTON: Well, algorithmic, gees, I don't know.
[LAUGHTER] PROF.
PATRICK WINSTON: Let me give you a hint.
Let me give you a hint.
There are all those articles up there, out there, and out there, just for example.
And here are the Town and Country articles.
They're out there, and out there, for example.
And now our unknown is out there.
Anybody got an idea now? Hey Brett, what do you think? BRETT: So you sort of want the ratio.
Or in this case, you can take the angle-- PROF.
PATRICK WINSTON: Let's be-- ah, there we go, we're getting a little more sophisticated.
The angle between what? BRETT: The angle between the vectors.
PROF.
PATRICK WINSTON: The vectors.
Good.
So we're going to use a different metric.
What we're going to do is, we're going to forget including a distance, and we're going to measure the angle between the vectors.
So the angle between the vectors, well let's actually measure the cosine of the angle between the vectors.
Let's see how we can calculate that.
So we'll take the cosine of the angle between the vectors, we'll call it theta.
That's going to be equal to the sum of the unknown values times the article values.
Those are just the values in various dimensions.
And then we'll divide that by the magnitude of the other vectors.
So we'll divide by the magnitude of u, and we'll divide by the magnitude of the art vector to the article.
So that's just the dot product right? That's a very fast computation.
So with a very fast computation you can see if these things are going to be in the same direction.","The professor says, ""So that's just the dot product, right?"" But that implies cosine similarity is equal to the dot product, which isn't accurate, is it? In this context, the dot product actually serves as the numerator."
5,09mb78oiPkA,"So we'll call this the feature detector.
And out comes a vector of values.
And that vector of values goes into a comparator of some sort.
And that comparator compares the feature vector with feature vectors coming from a library of possibilities.
And by finding the closest match the comparator determines what some object is.
It does recognition.
So let me demonstrate that with these electrical covers.
Suppose they arrived on an assembly line and some robot wants to sort them.
How would it go about doing that? Well it could easily use the nearest neighbor sorting mechanism.
So how would that work? Well here's how if would work.
You would make some measurements.
And it we'll just make some measurements in two dimensions.","""What is a comparator? I couldn't find it on the web. Is this a spelling mistake?"""
6,09mb78oiPkA,"What's the variance of that going to be? x over sigma sub x.
Anybody see, instantaneously, what the variance of that's going be? Or do we have to work it out? It's going to be 1, Work out the algebra for me.
It's obvious, it's simple.
Just substitute x prime into this formula for variance, and do the algebraic high school manipulation.
And you'll see that the variance turns out not to be of this new variable, this transformed variable you want.
So that problem, the non uniformity problem, the spread problem, is easy to handle.
What about that other problem? No cake without flour? What if it turns out that the data-- you have two dimensions and the answer, actually, doesn't depend on y at all.
What will happen? Then you're often going to get screwy results, because it'll be measuring a distance that is merely confusing the answer.
So problem number two is the what matters problem.
Write it down, what matters.
Problem number three is, what if the answer doesn't depend on the data at all? Then you've got the trying to build a cake without flour.
Once somebody asked me-- a classmate of mine, who went on to become an important executive in an important credit card company-- asked me if we could use artificial intelligence to determine when somebody was going to go bankrupt? And the answer was, no.
Because the data available was data that was independent of that question.
So he was trying to make a cake without flour, and you can't do that.
So that concludes what I want to say about nearest neighbors.
No I want to talk a little bit about sleep.","The professor indicates that AI cannot be used to predict bankruptcies in credit card companies, likening it to making a cake without flour. However, wouldn't the credit card company possess relevant data that could enable the use of AI for bankruptcy prediction? Why then is the answer ""no""?
Did anyone understand the derivative of x?"
7,0CdxkgAjsDA,"Among all those nodes that have a job in delta f, in delta.
This is probably a tricky part.
Then by definition, since delta f u is less than delta f v, right, and I defined v to be the one with the smallest delta that satisfies that.
So all the u's-- so u is a predecessor of v, so u shouldn't be one of those nodes that have a drop in delta.
So I know this is probably a tricky part.
Yeah, I'll stop for questions and make sure we resolve this part before we move on.","The challenging aspect can be demonstrated through a proof by contradiction. Let us assume that delta_f'(u) is less than delta_f(u). Under this assumption, u would belong to the set of vertices x for which delta_f'(x) is less than delta_f(x). However, since u is the predecessor of v, it follows that delta_f'(u) is less than delta_f'(v). This leads to a contradiction because it has been established that v has the smallest delta_f'(x) among the set of vertices x. Consequently, u must be the vertex with the smallest delta_f'(x). This contradiction indicates that our initial assumption (that delta_f'(u) is less than delta_f(u)) is incorrect, thereby confirming that delta_f'(u) must be greater than or equal to delta_f(u)."
8,0CdxkgAjsDA,"How many people get it? OK.
Only two.
That's not good.
OK.
AUDIENCE: I'm confused about how v can be the one with the smallest delta f if you have a predecessor with a smaller delta f.
PROFESSOR: OK.
So v to be the smallest-- the one with smallest f such that delta f prime is less than delta f.
AUDIENCE: OK.
PROFESSOR: OK, maybe, yeah, that's why I confused you guys.
Yeah.
Sorry about that.
So we have a bunch of nodes who have a drop in delta, and I defined v to be the one among them that has the smallest delta f prime.
Question? AUDIENCE: Sorry, I'm lost at what delta f prime is versus delta.
PROFESSOR: OK.
Delta f of a node is the shortest path from source to that node in G of f, which is the residual graph given a flow.
So f is, well, some flow, and f prime is the flow after we augmenting a certain path.
So f prime is one step after f.
OK.
How many people get that now? Still not everyone.
OK.
Any questions about that? How many people still haven't got that? OK, so some people-- it's like Schrodinger's cat.","He didn't ask the question: ""If delta_f'(v) is the smallest, then v must be the successor of s. How can there be a u between s and v?"""
9,0LixFSa7yts,"And that turns out to be pretty problematic.
So the next couple of slides sort of say a little bit about the why and how this happens.
What's presented here is a kind of only semi-formal wave your hands at the kind of problems that you might expect.
If you really want to sort of get into all the details of this, you should look at the couple of papers that are mentioned in small print at the bottom of the slide.
But at any rate, if you remember that this is our basic recurrent neural network equation, let's consider an easy case.
Suppose we sort of get rid of our non-linearity and just assume that it's an identity function.
OK, so then when we're working out the partials of the hidden state with respect to the previous hidden state, we can work those out in the usual way according to the chain rule.
And then, if sigma is simply the identity function, well then, everything gets really easy for us.
So only-- the sigma just goes away.
And only the first term involves h at time t minus 1.
So the later terms go away.
And so our gradient ends up as Wh.
Well, that's doing it for just one time step.
What happens when you want to work out these partials a number of time steps away? So we want to work it out, the partial of time step i with respect to j.
Well, what we end up with is a product of the partials of successive time steps.
And well, each of those is coming out as Wh and so we end up getting Wh raised to the l-th power.
And while our potential problem is that if Wh is small in some sense, then this term gets exponentially problematic.",Why is the partial derivative of J with respect to h (del J / del h) even necessary?
10,0UFwGJe6ubg,"The averages there and the distributions look moderately similar.
If you're coming from a skilled nursing facility, if you are in a skilled nursing facility, you're probably old because younger people don't typically need skilled nursing care.
And I'm not sure why transfers within the facility are significantly younger ages, but that's true from the MIMIC data.
What about age at admission by language? So some people speak English.
Some people speak not available.
Some people speak Spanish, et cetera.
So it turns out the Russians are the oldest.
And that may have to do with immigration patterns, or I don't know exactly why.
But that's what the data show.
If you do it by ethnicity, it turns out that African-Americans, on the whole, are somewhat younger than whites.
And Hispanics are somewhat younger yet.
So that means that those subpopulations apparently need intensive care earlier in life than whites.
So this is a topic that's very hot right now, discussions about how bias might play into health care.","I don't quite understand how the professor can interpret from the plot that ""African Americans are somewhat younger than whites."" Can someone explain to me how one could draw this conclusion? Thanks. :)"
11,0jljZRnHwOI,"And that forces you to indent everything that's a code block.
So you can easily see sort of where the flow of control is and where decision making points are and things like that.
So in this particular example, we have one if statement here, and it checks if two variables are equal.
And we have an if, elif, else.
And in this example, we're going to enter either this code block or this one or this one, depending on the variables of x and y.
And we're only going into one code block.
And we'll enter the first one that's true.
Notice you can have nested conditionals.
So inside this first if, we have another if here.
And this inner if is only going to be checked when we enter the first-- this outter if.
I do want to make one point, though.
So sometimes, you might forget to do the double equals sign when you are checking for equality, and that's OK.
If you just use one equals sign, Python's going to give you an error.
And it's going to say syntax error, and it's going to highlight this line.
And then you're going to know that there's a mistake there.
And you should be using equality, because it doesn't make sense to be using-- to assign-- to be making an assignment inside the if.","What is the use of the nested ""if"" statement, and when should it be used?"
12,0jljZRnHwOI,"So the for loop says, for some loop variable-- in this case, I named it n.
You can name it whatever you want.
In range 5-- we're going to come back to what range means in a little bit-- print n.
So every time through the loop, you're going to print out what the value of n is.
Range 5 actually creates internally a sequence of numbers starting from 0 and going to that number 5 minus 1.
So the sequence is going to be 0, 1, 2, 3, and 4.
The first time through the loop, you're going to say n is equal to 0.
Or internally, this is what happens.
N gets the value of 0.
You're going to print n.
Then you're going to go back to the top.
N gets the value 1.
Then you're going to go execute whatever is inside.
So you're going to print 1.
Then you're going to increment that to the next value in the sequence.
You're going to print out 2, and so on.","""Where do you subtract 1 from 5?""
She points at the whiteboard and says, ""That number, 5-1."" However, when the whiteboard is shown, there is neither a number nor an equation (5-1) visible. Where does it come from?"
13,0rt2CsEQv6U,"Um, and then expected value of [NOISE] this quadratic term.
Um, because this quadratic term here, kind of the inductive case was what we showed was V star for the- for the next time step, right? So it turns out that, um, let's see.
So this is a quadratic function, and this expectation is the expected value of a quadratic function with respect to s drawn from a Gaussian, right? With a certain mean and certain variance.
So it turns out that, um, the expected value of this thing, right? Well, this whole thing that I just circled.","""Shouldn't the 'Big Quadratic Function' include the second term, since a_t is present?"""
14,0zuiLBOIcsw,"And I can- I wanna contract them into a super-node, ah, and, uh, create a new network, a next level network, where super-nodes are connected if there is at least one edge between the nodes, ah, of the corresponding communities, um, and the, ah, weights, ah, of the edges between two super-nodes is the sum of the edge weights across all edges between their corresponding communities.
And now I will have a super graph.
And I simply go and run, ah, phase 1, ah, again.
All right, so the idea is, I have my original network, I run phase 1 to identify clusters.
Now I contract each cluster into a super-node.
I connect two clusters, if there is at least one edge, ah, between them.
And now this will be a weighted network where the, the edge weights are denoted here, so this will be the total number of edges between C_1 and C_2.
And this would be the total number of edges, ah, between the members of, ah, C_2 ah, and so on.
And now that I have the super graph, I simply apply my, ah, phase 1 algorithm again.
So the way this will work is, ah, you know, to summarize, I have my original network, I pick a node and, ah, initially I put every node into its own community.
Um, and then I ask, ah, a node, what if I move you to the same community as your member node 2 is up.","The slide indicates that Delta M_0,4 equals 0.26. However, since nodes 0 and 4 are not connected, I believe we should not calculate this change in modularity, right? We should only compute it for the neighbors of node 0, shouldn't we?"
15,1A6VoEkQnhQ,"And, uh, the reason why a, uh, uh, plain GNN cannot differentiate between, you know, node 1 in, uh, a cycle of length 3 versus a cycle of leng- length 4 is that if you look at the GNN computation graph, re- both- for both of these nodes V_1 and V_2, the computation graph is exactly the same.
Meaning, V_1- V_1 and V_2 have two neighbors, um, each, and then, you know, these neighbors have, uh, one neighbor each and the computation graph will always look like this.
Uh, unless, right, you have some way to discriminate nodes based on the features.
But if the nodes have the same, um, set of, uh, features that not- you cannot discriminate them based on their attributes, then you cannot learn, uh, to discriminate node V_1 from V_2.
They will- from the GNN point of view, they will all, uh, look the same.
I'm going to go into more, uh, depth, uh, uh, around this, uh, this example and, er, what are some very important implications of it and consequences of it when we are going to discuss the theory of, uh, graph neural networks.","The computational graphs are structurally the same for node v_1, but they will be fed different embeddings. These embeddings will implicitly include information about the nodes. However, a GNN may still struggle to distinguish between cycles of varying types and shapes."
16,247Mkqj_wRM,"So I take the coefficients e that we have just defined, I, uh, exponentiate them, and then, you know, divide by the s- exponentiated sum of them so that, uh, these, uh, attention weights, uh, alpha now are going to sum to 1.
And then, right when I'm doing message aggregation, I can now do a weighted sum based on the attention weights, uh, alpha.
So here are the alphas.
These are these alphas that depend on e, and e is the, uh, is the, um, is- depends on the previous layer embeddings of nodes, uh, u and v.
So, for example, if I now say, how would aggregation for node A look like? The way I would do this is I would compute these attention weights, uh- uh, Alpha_AB, Alpha_AC, and Alpha_AD because B, C, and D are its neighbors.
Uh, these alphas will be computed as I- as I show up here, and they will be computed by previous layer embeddings, uh, of these, uh, nodes on the endpoints of the edge.
And then my aggregation function is simply a weighted average of the messages coming from the neighbors, where message is, uh- uh- uh, multiplied by the weight Alpha that we have, uh, computed and defined up here.
So that's, um, [NOISE] basically the idea of the attention mechanism.
Um, now, what is the form of this attention mechanism a? We still haven't decided how embedding of one node and embedding of the other node get- get combined, computed into this, uh- uh, weight, uh, e.
Uh, the way it is usually done is, uh- um, you- you have many different choices.
Like you could use a simple, uh, linear layer, uh, one layer neural network to do this, um, or, uh, have alpha, uh, this, um, function a have trainable parameters.
Uh, so for example a p- uh, a popular choice is to simply to say: let me take the embeddings of nodes A and B at the previous layer, perhaps let me transform them, let me concatenate them, and then apply a linear layer to them, and that will give me this weight, uh, e_AB, to which then I can apply softmax, um, and then based on that, ah, softmax transformed weight, I use that weight as, uh- uh, in the aggregation function.","After watching this lecture, I had two questions: Is there a query key-value interpretation of the simple ""linear attention"" mechanism in graph attention networks? Also, when describing the normalization of the unnormalized attention weights a_{vu} in the graph attention network scheme, why don't we simply divide by the total sum of all the attention weights? Why do we use a softmax function to suppress the non-maximal weights and primarily focus on a single neighbor?"
17,247Mkqj_wRM,"So, last lecture, we talked about graph convolutional neural network or a GCN.
And I've wrote this equation, I said, ah-ha, the embedding of node v at layer l is simply an average of the embeddings of nodes u that are neighbors of we normalized the by the- by the N-degree of node v and transformed with matrix W and sent through a non-linearity.
So now the question is, how can I take this equation that I've written here and write it in this message transformation plus aggregation function.
And the way you can- you can do this is simply take this W and distribute it inside.
So basically now W times h divided by number of neighbors is the message transformation function.
And then the message aggregation function is simply a summation.
And then we have a non-linearity here.","This may be a minor detail, but for the sake of conceptual clarity, I would like to inquire: Wouldn't it be more logical to attribute the normalizing factor 1/|N(v)| to the aggregation phase in the classical GCN layer, which is understood as a combination of message transformation and aggregation? This is because the factor depends on the node receiving the messages, not solely on the message itself. In other words, shouldn't we apply the distributive law to separate it from the sum? By doing so, the message transformation step would be influenced only by the message, without regard to the destination node. To me, this approach appears more conceptually coherent for what is considered a ""message transformation."""
18,247Mkqj_wRM,"Then we have to aggregate these messages into a single message and pass it on.
So the way you can think of this is that we get messages, denoted as circles here, from the three neighbors, from the previous layer.
We also have our own message, right? Message of the node v from the previous layer.
Somehow we want to combine this information to create the next level embedding or to the next level message for this node of interest.
What is important here to note is that this is a set.
So the ordering in which we are aggregating these messages from the children is not important.
What is arbitrary? And for this reason, these aggregation functions that aggregate, that summarize, that compress in some sense, the messages coming from the children have to be order invariant because they shouldn't depend, in which ordering, am I considering the neighbors? Because there is no special ordering to the neighbors, to the lower level, to the children in the network.
That's an important detail.
Of course, another important detail is that we want to combine information coming from the neighbor- from the neighbors together with a node's own information from the previous layer as denoted here.","""When he mentioned that we have the value of the self node from the previous layer, was he referring to the value of V from the last iteration? This is because 'layer' here actually denotes the layer preceding the one where V does not yet exist. I realize this may be delving too deeply into semantics, but I just want to be absolutely certain. Or is it the self-loop that is considered the previous layer in this context?"""
19,2P-yW7LQr08,"So the last thing I'll do-- and I just have one more minute-- is give you a sense of a small change to interval scheduling that puts us in that NP complete domain.
So so far, we've just done two problems.
There's many others.
We did interval scheduling.
There was greedy linear time.
Weighted interval scheduling is order n squared according to this particular DP formulation.
It turns out there's a smarter DP formulation that runs an order n log n time that you'll hear about in section on Friday, but it's still polynomial time.
Let's make one reasonable change to this, which is to say that we may have multiple resources, and they may be non identical.","I don't understand. If recursive calls are O(1), shouldn't the complexity be O(n)? Why is it O(n^2)? Can somebody explain?"
20,2g9OSRKJuzM,"Now, this looks kind of random.
Anybody recognize these numbers? No one from the great City of New York? No? Yup, yup.
AUDIENCE: On the subway stops? SRINIVAS DEVADAS: Yeah, subway stops on the Seventh Avenue Express Line.
So this is exactly the notion of a skip list, the fact that you have-- could you stand up? Great.
All right.
So the notion here is that you don't have to make a lot of stops if you know you have to go far.
So if you want to go from 14th Street to 72nd Street, you just take the express line.
But if you want to go to 66th Street, what would you do? AUDIENCE: Go to 72nd and then go back.
SRINIVAS DEVADAS: Well, that's one way.
That's one way.
That's not the way I wanted.
The way we're going to do this is we're not going to overshoot.
So we want to minimize distance, let's say.
So our secondary thing is going to be minimizing distance travel.
And so you're going to pop up the express line, go all the way to 42nd Street, and you're going to say if I go to the next stop on the Express Line, I'm going too far.
And so you're going to pop down to the local line.
So you can think of this as being link list L0 and link list L1.","I don't understand why he avoids overshooting here. Since traveling on L1 is faster, shouldn't going to 72 and then back to 66 result in the fewest number of nodes?"
21,3pU-Hrz_xy4,"So this is actually equal to 2 in this case.
And this corresponds to this value in the table which is again the agent is following a maximizer assuming the opponent is a minimizer.
Opponent was not a minimizer, opponent was just following Pi 7.
And this is just equal to 2 .
Okay.
So so far, the things I've shown are actually very intuitive.
They seem a little complicated but they're very intuitive.
What I've shown is that this value of minimax, it's an upper bound.
If you're assuming our, our opponent is a terrible opponent, now it's going to be an upper bound because the best thing I can do is maximize.
I've also shown it's a lower bound if my opponent is not as bad.
So, so that's what I've shown so far.
A question.
So here the opponent's policy is completely hidden to the agent.
Yeah.
So here, like, because- Yeah, the agent actually doesn't see the opponent- where the opponent goes, right? Even in the expectimax case, it thinks the opponent is going to follow Pi 7, but maybe the opponent follows Pi 7, maybe not.
Right so, so like when we talk about expectimax and minimax, it's always the case that the opponent doesn't actually see what the opponent does.
But the opponent can think- the agent can think what the opponent does, okay? And I'm going to talk about one more property.","Why does V(pimax, pi7) equal 2 instead of 5, assuming that the agent will attempt to maximize his value while the opponent behaves stochastically (i.e., with distributions of 0, 2, 5)?"
22,3v5Von-oNUg,"I think that's right.
But now we have our original aR bR plus 2q plus 8q is equal to this thing.
And finally, we can divide this thing by R very cheaply.
Because we just discard the low four zeros.
Make sense? Question.
AUDIENCE: Is aR bR always going to end in, I guess, 1,024 zeros? PROFESSOR: No, and the reason is that-- OK, here is the thing that's maybe confusing.
A was, let's say, 512 bits.
Then you multiply it by R.
So here, you're right.
This value is that 1,000 bit number where the high bit is a, the high 512 bits are a.
And the low bits are all zeros.
But then, you're going [? to do it with ?] mod q to bring it down to make it smaller.","OMG, I was so confused the whole time, LOL. However, the lecture was wonderful, and I was able to follow easily and deduce things from his prompts. Nonetheless, I think many properties he mentioned, such as inverses, apply only to finite fields with prime order, right? I believe it should have been mentioned just once, unless I am mistaken. I will probably give it another look."
23,3v5Von-oNUg,"So instead of doing 500 mod qs for every multiplication step, you do it twice mod q.
And then you keep doing these divisions by R cheaply using this trick.
Question.
AUDIENCE: So when you're adding the multiples of q and then dividing by R, [INAUDIBLE] PROFESSOR: Because it's actually mod q means the remainder when you divide by q.
So x plus y times q, mod q is just x.
AUDIENCE: [INAUDIBLE] PROFESSOR: So in this case, dividing by-- so another sort of nice property is that because it's all modulus at prime number-- it's also true that if you have x plus yq divided by R, mod q is actually the same as x divided by R mod q.
The way to think of it is that there's no real division in modular arithmetic.
It's just an inverse.
So what this really says is this is actually x plus yq times some number called R inverse.
And then you compute this whole thing mod q.
And then you could think of this as x times R inverse mod q plus y [? u ?] R inverse mod q.
And this thing cancels out because it's something times q.
And there's some closed form for this thing.
So here I did it by bit by bit, 2q then 8q, et cetera.
It's actually a nice closed formula you can compute-- it's in the lecture notes, but it's probably not worth spending time on the board here-- for how do you figure out what multiple of q should you add to get all the low bits to turn to 0.","OMG, I was so confused the whole time, LOL. However, the lecture was wonderful, and I was able to follow easily and deduce things from his prompts. Nonetheless, I think many properties he mentioned, such as those of inverses, apply only to finite fields with prime order, right? I believe it should have been mentioned just once, unless I am mistaken. I will probably review it to be sure."
24,4b4MUYve_U8,"Okay? Um, so in order to- [NOISE] so in order to simplify the notation, [NOISE] um, [NOISE] in order to make that notation a little bit more compact, um, I'm also gonna introduce this other notation where, um, we want to write a hypothesis, as sum from J equals 0-2 of theta JXJ, so this is the summation, where for conciseness we define X0 to be equal to 1, okay? See we define- if we define X0 to be a dummy feature that always takes on the value of 1, then you can write the hypothesis h of x this way, sum from J equals 0-2, or just theta JXJ, okay? It's the same with that equation that you saw to the upper right.
And so here theta becomes a three-dimensional parameter, theta 0, theta 1, theta 2.
This index starting from 0, and the features become a three dimensional feature vector X0, X1, X2, where X0 is always 1, X1 is the size of the house, and X2 is the number of bedrooms of the house, okay? So, um, to introduce a bit more terminology.
Theta [NOISE] is called the parameters, um, of the learning algorithm, and the job [NOISE] of the learning algorithm is to choose parameters theta, that allows you to make good predictions about your prices of houses, right? Um, and just to lay out some more notation that we're gonna use throughout this quarter.","May I ask, what does O(theta) represent exactly?"
25,4b4MUYve_U8,"Uh, that's times the partial derivative of Theta J of X Theta X minus Y, right? So if you take the derivative of a square, the two comes down and then you take the derivative of what's inside and multiply that, right? [NOISE] Um, and so the two and one-half cancel out.
So this leaves you with H minus Y times partial derivative respect to Theta J of Theta 0X0 plus Theta 1X1 plus th- th- that plus Theta NXN minus Y, right? Where I just took the definition of H of X and expanded it out to that, um, sum, right? Because, uh, H of X is just equal to that.","What are x0 and x1? If we consider a single feature, such as the number of bedrooms, how can both x0 and x1 exist? Wouldn't x0 be nonexistent? This is confusing. To put it another way, if my Theta0 update function depends on x0 for the update, but x0 is nonexistent, then Theta0 will never change from its initial value."
26,4b4MUYve_U8,"Um, and then finally, by convention, we put a one-half there- put a one-half constant there because, uh, when we take derivatives to minimize this later, putting a one-half there would make some of the math a little bit simpler.
So, you know, changing one- adding a one-half.
Minimizing that formula should give you the same as minimizing one-half of that but we often put a one-half there so to make the math a little bit simpler later, okay? And so in linear regression, we're gonna define the cost function J of Theta to be equal to that.","Dear Dr. Andrew, I watched another one of your videos about the cost function in linear regression, which used 1/2m, but in this video, you used 1/2. Could you explain the difference between the two? (footnote)"
27,4dj1ogUwTEM,"Anyway, just as there's a decimal expansion of every real number, there's a binary expansion just using base 2.
So here's the binary expansion of 3 and 1/3.
So what I'm going to do is I'm going to map 3 and 1/3 to this binary sequence.
I'm going to ignore the decimal place.
Binary is not decimal place.
It's a [? becimal ?] place, or binary position.
And I'm just going to take this to mapping the sequence, 11010101.
And I claim that this is a surjection because you're going to hit every possible binary sequence in this way.
Well, almost.
Let's take a closer look.
There's a problem with mapping to things that start with 0, because let's examine that a half is 0.10000000.
So I would map it to that.
And it will end.
But there's an ambiguity, because a half is also equal to 0.011111, just as 0.999999 is equal to 1.000000 in decimal, you get the same infinite carry issue here in binary.
So numbers that end in all ones have another way to represent the very same number by a sequence that ends in all zeroes.",How do we know that we will encounter every possible infinite string?
28,4dj1ogUwTEM,"But we'll think about it as though we could.
Let's think about this matrix again.
So suppose A is this set of elements, a, b, s, t, d, e.
I'm scrambling up the alphabet on purpose, because I don't want you to get the idea that we're assuming that A is countable, that you can list all the elements of A.
I'm not assuming that.
But I'm just writing out a sample of elements of A.
And let's suppose that I was trying to get a surjection from A to the power set of A.
So suppose I have a function f that maps each of the successive elements of A to some subset of A.","""Why does f(a) have to be related to subsets? How does applying a function to every element of a set create its power set?"""
29,4nXw-f6NJ9s,"And you can even download the level-- an example of the level and play it, if you dare.
So that's a lot of-- we have a lot of fun in that world of hardness of different games and puzzles.
Where do I want to go next? OK.
Next topic is balloon twisting.
Totally different.
This is recreational, but not about hardness.
This is an octahedron twisted from one balloon.
I made another one on a stick.
Each of these is made for one balloon.
What graphs can you make for one balloon? Well, you should read our paper.
And you can characterize how many balloons you need to make each polyhedron.
And some of these problems are NP-hard, and it's a lot of fun.
Cool.
I think that's the end of the slides.
The last thing I wanted to show you is a problem, a puzzle/magic trick-- it comes from the puzzle world-- called the picture hanging problem.
So imagine you have a picture.
You want to hang it on a wall.
So you invested in some nice rope, and you hang it on a nail.","Is that essentially an AND gate implemented with rope? The picture hangs when both values are TRUE; otherwise, it falls when either value is FALSE."
30,51-b2mgZVNY,"Again, 8 factorial.
OK.
Finally, how many permutations are there that have all three patterns-- 60, 04, and 42? That of course, is exactly the same as the set of sequences with the single pattern 6042, the four digit pattern.
And again, we count that by saying that it's the number of permutations of the digits other than 6042-- six of them plus the 6042 object.
There are seven of these , and so there are 7 factorial permutations that have all three patterns.
So that means that I can go back to my inclusion-exclusion formula for the sequences that have one of the three patterns-- 60, 04, and 42-- and plug them in.
So I get 3 9 factorials for the first sum of three terms.
The intersections-- we all figured out each of them were-- I'm sorry it's 8 factorial.
So I'm going to subtract 3 times 8 factorial.
And this last term we figured out was 7 factorial.
Well, I can think of 3 times 9 factorial as 9 times 8 times 3 times 7 factorial, and this is 3 times 8 times 7 factorial.
And I wind up [NO AUDIO] PROFESSOR: 72,720.
That's how many permutations of the digits 0 through 9 there are that have one or another of these three patterns.
Turns out that's about 27% of the 10 factorial permutations of 0 through 9.
So that's the significance of applying the disjunction of constraints, this union of having either 60, 04, or 42.
","""Which method is he using to calculate the result?"""
31,5Bx5UhrJbJI,"This slide has a bunch of other annotations on it.
And the reason I included them is that the course repository includes a reference implementation of an autoencoder and all the other deep learning models that we cover in pure NumPy.
And so if you want it to understand all of the technical details of how the model is constructed and optimized, you could use this as a kind of cheat sheet to understand how the code works.
I think the fundamental idea that you want to have is simply that the model is trying to reconstruct its inputs.
The error signal that we get is the difference between the reconstructed and actual input.
And that error signal is what we use to update the parameters of the model.
Final thing I would mention here is that it could be very difficult for this model if you feed in the raw current vectors down here.
They have very high dimensionality.
And their distribution is highly skewed as we've seen.
So it can be very productive to do a little bit of reweighting and maybe even dimensionality reduction with LSA before you start feeding inputs into this model.","""Everything becomes confusing from that point on; could you at least explain the labels a bit?"""
32,5cF5Bgv59Sc,"So that's the triangle inequality.
Pretty intuitive notion, right? Why is this useful? OK, well, if I find-- if I find an edge in my graph, if there's an edge u, v, in my graph such that this condition is violated for the estimates that I have-- it obviously can't be violated on my shortest path distances, but if it violates it on the estimates-- u, v, is bigger than u, x-- sorry, u-- how am I going to do this? I want this to be s.
I'm calculating shortest path distances from s and shortest path distances from s to some incoming vertex u plus the edge weight from u to v.
All right, so what is this doing? I have some edge u, v in my graph.
Basically, what I've said is that I have some distance estimate to u, but going through-- making a path to v by going through u, and then the edge from u to v is better than my current estimate, my shortest path estimate to v.
That's bad, right? That's violating the triangle inequality.
These cannot be the right weights.
These cannot be the right distances.
So how we're going to do that is lower-- this is what we said, repeatedly lower these distance estimates.
I'm going to lower this guy down to equal this thing.
In a sense, this constraint was violated.
But now we're relaxing that constraint so that this is no longer violated.","Shouldn't it be d(s, v) > delta(s, u) + w(u, v)? Because d(s, u) is also an upper bound estimate, that is, infinity."
33,5rlIYGJdPy4,"Exactly what we did on the example on the previous slide.
So let's revisit the Australia example and apply AC-3.
OK.
So here is the empty assignment.
And here are all the domains of each of the variables.
So let's suppose we set WA to be red, OK? So as before, we eliminate the other values from WA's domain, of course.
And then we enforce arc consistency on the neighbors of WA.
In this case, NT and SA.
So out goes red on both of these.
And now we continue to try to enforce our consistency on the neighbors of NT and SA.
But in this case, I can't actually eliminate anything.","""Why not? Why couldn't that be possible?"""
34,5wCZqdCDafc,"ROFESSOR: So connectivity is more than just an all or nothing affair.
We can talk about how connected a graph is.
So let's begin with two vertices.
Two vertices are said to be k-edge connected if they remain connected if you remove fewer than k edges from the graph.
Let's look at an example.
So here's a graph, and let's focus on those two vertices that I've highlighted in magenta.
They are 1-edge connected because they're connected, and if you remove one edge they become disconnected.
So they're 1-edge connected, but they're not 2-edge connected.
In particular, if I delete that edge, then they no longer is a path between the two magenta vertices.","Why do I think that the initial definition ( ) is slightly incorrect? Shouldn't it state, ""Vertices v and w are k-edge connected if they become disconnected only when fewer than k edges are deleted""? Please correct me if I'm wrong. :)"
35,6LOwPhPDwVc,"And then let's look at merge sort and do one more visualization of this.
Again let me remove that.
If we run it-- again, I've just put some print statements in there.
Here you can see a nice behavior.
I start off calling Merge Sort with that, which splits down into doing Merge Sort of this portion.
Eventually it's going to come back down there and do the second one.
It keeps doing it until it gets down to simple lists that it knows are sorted.
And then it merges it.
Does the smaller pieces and then merges it.
And having now 2 merged things, it can do the next level of merge.
So you can see that it gets this nice reduction of problems until it gets down to the smallest size.
So let's just look at one more visualization of that and then get the complexity.
So if I start out with this list-- sorry about that.
What I need to do is split it.
Take the first one, split it.
Keep doing that until I get down to a base case where I know what those are and I simply merge them.
Pass it back up.
Take the second piece.
Split it until I get down to base cases.
Do the merge, which is nice and linear.
Pass that back up.
Having done those two pieces, I do one more merge.
And I do the same thing.
I want you to see this, because again you can notice how many levels in this tree log.
Log in the size.
Because at each stage here, I went from a problem of 8 to two problems of 4.
Each of those went to two problems of 2, and each of those went to two problems of size 1.
All right.
So the last piece is, what's the complexity? Here's a simple way to think about it.
At the top level, I start off with n elements.
I've got two sorted lists of size n over 2.
And to merge them together, I need to do order n work.","The merge process does not involve logarithmic iterations; it requires at least n iterations and at most 2n iterations. This can be observed from the number of prints (iterations), which are not logarithmic in nature. We are not halving the problem size; rather, we are dealing with a tree structure. It is the number of levels in the tree that is logarithmic (log n), not the iterations themselves. Each level of iterations collectively has a complexity of O(n), meaning that while the cost of each individual step (iteration) is O(n), the combined cost of all steps at the same level of the tree is also O(n). There is a common confusion where levels are mistaken for steps. Typically, we calculate the total complexity by multiplying the number of steps (ranging from O(n) to O(2n)) by their individual complexity (O(n)). However, in this scenario, we take advantage of the fact that at each level of the tree, the sum of the complexities of all steps is O(n). Therefore, the number of levels in the tree (O(log n)) multiplied by the complexity of each level (O(n)) results in an overall complexity of O(n log n)."
36,6LOwPhPDwVc,"Because as I said I got to do at least n comparisons where n is the length of the list.
And then I've got to do n plus n copies, which is just order n.
So I'm doing order n work.
At the second level, it gets a little more complicated.
Now I've got problems of size n over 4.
But how many of them do I have? 4.
Oh, that's nice.
Because what do I know about this? I know that I have to copy each element at least once.
So not at least once.
I will copy each element exactly once.
And I'll do comparisons that are equal to the length of the longer list.
So I've got four sublists of length n over 4 that says n elements.
That's nice.
Order n.
At each step, the subproblems get smaller but I have more of them.
But the total size of the problem is n.
So the cost at each step is order n.
How many times do I do it? Log n.
So this is log n iterations with order n work at each step.
And this is a wonderful example of a log linear algorithm.","The merge process does not involve logarithmic iterations; it requires at least n iterations and at most 2n iterations. This can be observed by the number of prints (iterations), which are not logarithmic. We are not halving the problem size; rather, we are dealing with a tree structure. It has logarithmic levels of iterations (not the iterations themselves), with the iterations at each level collectively having O(n) complexity. The cost of each individual step (iteration) is O(n), and it turns out that the total cost of steps at the same level of a tree is also O(n). There is a common confusion between levels and steps. Typically, we multiply the number of steps (ranging from O(n) to O(n)) by their complexity (O(n)), but in this case, we take advantage of the fact that at each level of a tree, the sum of the complexities of the steps is O(n). Therefore, the number of levels in a tree (O(logn)) multiplied by the complexity of each level (O(n)) equals O(nlogn)."
37,6LOwPhPDwVc,"Break the problem in half.
Keep doing it until I get sorted lists.
And then grow them back up.
So there's merge sort.
It says, if the list is either empty or of length 1, just return a copy of the list.
It's sorted.
Otherwise find the middle point-- there's that integer division-- and split.
Split the list everything up to the middle point and do merge sort on that.
Split everything in the list from the middle point on.
Do merge sort on that.
And when I get back those two sorted lists, just merge them.
Again, I hope you can see what the order of growth should be here.
Cutting the problem down in half at each step.","The merge process does not involve logarithmic iterations; it requires at least n iterations and at most 2n iterations. This can be observed by the number of prints (iterations), which are not logarithmic. We are not halving the problem; rather, we are dealing with a tree structure. It has logarithmic levels of iterations (not the iterations themselves), with the iterations at each level collectively having O(n) complexity. The cost of each step (iteration) is O(n), and it turns out that the combined cost of steps at the same level of a tree is also O(n). There is often confusion between levels and steps. Typically, we multiply the number of steps (ranging from O(n) to O(n)) by their complexity (O(n)), but in this case, we take advantage of the fact that at each level of a tree, the sum of the complexities of the steps is O(n). Therefore, the number of levels in a tree (O(logn)) multiplied by the complexity of each level (O(n)) equals O(nlogn)."
38,6stKGH6zI8g,"This doesn't seem right? Uh, [LAUGHTER] uh, [NOISE] and that is- that's a good intuition to have.
[NOISE] But, uh, in this case, we- now- are now moving from meta-training- from training set to the meta-train sets, and test sets to meta test sets.
So each of these tasks, the training set and the test sets for the task correspond to the meta-training dataset.
And then at meta-test time were given new tasks.
Uh, and we don't wanna train on the meta-test set.
Okay.
Um, so we have our meta-learning data, this corresponds to training and test sets for every task, uh, where each of the training set- datasets corresponds to K data points.
Each of the test data- datasets correspond to a new set of K data points.
Um, yeah.
Okay.
So the complete, kind of, optimization problem is that at test-time we're gonna be inferring a set of task specific parameters.
Which can be some, some function that takes as input the training dataset and outputs the task specific parameters.
Where the parameters of that function or the meta parameters are theta star.
Um, and we essentially wanna learn a set of meta parameters such that, this function is good for held-out data points, after being- after ge- getting the training dataset as input.
Okay.
Um, so essentially you can view theta star as optimizing this, uh, objective [NOISE] where we want to optimi- optimize the, the probability of the parameters, um, being effective at new data points.","Meta-learning is not inherently complicated in practice; however, the terminology used for datasets in the literature can be confusing. The concepts of support and query sets, as well as meta-training and meta-test sets, are not clearly defined. The former is not typically employed in standard neural networks, while the latter can be perplexing due to its length and potential for being mistaken for the regular model's dataset (optimizee). It might be clearer to reserve the terms 'training and test sets' for the optimizee dataset and use 'training and test tasks' for the meta-learning context."
39,6stKGH6zI8g,"[inaudible] Yeah exactly.
So basically the weights that are, uh, right after the Z_i, the kind of, if you have a fully connected layer right after Z_i concatenated with your features, the, the part of that matrix that corresponds to, uh, that is basically right after Z_i will have basically different, um, different components that, that are not shared for each of the tasks.
Other than that half of that matrix, all the other parameters are shared.
Yeah.
There's also kind of forces [inaudible] Yeah.
Yeah, exactly.
So in this case, we assume that all the inputs have the same size, the same, uh, dimensions.
One thing that you could do is, uh, if, if different tasks have different sizes, you can basically like, you would have some sort of, uh, recurrent neural network, or some sort of attention based model that basically aggregates over the variable dimens- like if you- if one of them is time for example, it aggregates over that, uh, whereas maybe some tasks have- are text and others image.
Images in that case, you would probably wanna have different, um, different first parts of that network to take in those different modalities of data.
And we'll show- we'll see like an explicit example of, of how that has been done in the past.
Yeah.
So [inaudible] So this is, yeah, this is a good question.
It's, it's a f- it's a fairly nuanced point.
So basically each, um, you can ba- you can view, uh, the first- a fully connected layer corresponding to, um, a weight matrix times a vector.
And when you, um, when you take the, the top part of that matrix will correspond to- or sorry the left, um, rows of that- the left columns of that matrix will correspond to the, the features and the right columns of that will correspond to the task, uh, task vector that's being processed in this, in this input.","The professor mentioned that ""the parameters are shared except for those after the concatenation."" Does this mean that all the fully connected layers that follow will be shared across different tasks?"
40,7lQXYl_L28w,"And inside of the loop, this is just constant.
It doesn't depend on the size of the integer.
So how many times do I go through the loop? Well, how many times can I divide i by 10? And that's log of i, right? So it's not i itself.
It's not the size of the integer.
It's the number of digits in the integer.
And here's another nice example of log.
I'll point you, again, right here.
I'm reducing the size of the problem by a constant factor-- in this case, by 10-- each time-- nice characteristic of a logarithmic algorithm.
OK, we've got constant.
We've got log.
What about linear? We saw it last time, right? Something like searching a list in sequence was an example of something that was linear.
In fact, most of the examples we saw last time were things with iterative loops.
So for example, fact, written intuitively-- factorial, right-- n times n minus 1 times n minus 2 all the way down to 1.","Aren't strings immutable? This means that the operation res = digits[i%10] + res would create a new string each time. Wouldn't that result in creating a new string log(n) times? Therefore, shouldn't the complexity be the sum of (1 + 2 + 3 + ... + log(n)), which equals (1 + log(n)) * log(n) / 2, and is equivalent to O(log(n)^2)?"
41,7lQXYl_L28w,"Something that grows linearly is not bad.
Something that grows, as we've seen down here, exponentially tends to say, this is going to be painful.
And in fact, you can see that graphically.
I'll just remind you here.
Something that's constant says, if I draw out the amount of time it takes as a function of the size of the input, it doesn't change.
Logarithmic glows-- gah, sorry-- grows very slowly.
Linear will grow, obviously, in a linear way.
And I actually misspoke last time.
I know it's rare for a professor to ever admit they misspeak, but I did.
Because I said linear is, if you double the size of the input, it's going to double the amount of time it takes.
Actually, that's an incorrect statement.
Really, what I should have said was, the increment-- if I go from, say, 10 to 100, the increase in time-- is going to be the same as the increment if I go from 100 to 1,000.
Might be more than double depending on what the constant is.
But that growth is linear.
If you want to think of it, take the derivative of time with respect to input size.
It's just going to be a constant.
It's not going to change within.
And of course, when we get down to here, things like exponential, it grows really fast.
And just as a last recap, again, I want to be towards the top of that.
There was my little chart just showing you things that grow constant, log, linear, log-linear, quadratic, and exponential.
If I go from n of 10, to 100, to 1,000, to a million, you see why I want to be at the top of that chart.
Something up here that grows logarithmically, the amount of time grows very slowly as I increase the input.
Down here, well, like it says, good luck.
It's going to grow up really quickly as I move up in that scale.","How is that possible? If it's linear, like k*n + b, which is O(n), then if the input (n) is doubled, it becomes 2k*n + b. The worst-case scenario is nearly doubled. How can it get significantly larger? He claimed it could more than double.
He says, ""Linear growth means that if you double the size of the input, the time it takes will also double."" However, that statement is incorrect. What I should have said is that the increase in time is consistent with the increase in input sizefor instance, if the input grows from 10 to 100, the time increase will be proportional to the increase from 100 to 1000. The actual time might more than double, depending on the constant factor, but the growth rate is linear. I'm confused about the difference. He's suggesting that the growth from 10 to 100 is tenfold, and from 100 to 1000 is also tenfold, right? And the initial example implied that doubling from 1 to 2 and then from 2 to 4 both result in a twofold increase. Aren't these examples illustrating the same concept? What am I not understanding?"
42,7lQXYl_L28w,"When you're given a new problem, how do I get this into a linear algorithm if I can? Log-linear, if I can, would be really great.
But you know, if I can't, how do I stay away from exponential algorithms? And finally, what we're going to show later on is that, in fact, there are some problems that, as far as we know, are fundamentally exponential.
And they're expensive to compute.
The very last thing is, you might have decided I was cheating in a different way.
So I'm using a set of built-in Python functions.
I'm not going to go through all of these.
But this is just a list, for example, for lists, of what the complexity of those built-in functions are.
And if you look through the list, they kind of make sense.
Indexing, you can go straight to that point.
Computing the length, you compute it once, you've stored it.
Comparison-- order n, because I've got to compare all the elements of the list.
Similarly, to remove something from the list, I've got to find where it is in the list and remove it.
Worst case, that's going to be order n.
So you can see that these operations are typically linear in the size of a list.
These are constant.
For dictionaries, remember, dictionaries were this nice thing.
They weren't ordered.
It gave me a power in terms of storing them.
But as a consequence, some of the costs then go up.
For a list, indexing, going to a particular point, I just go to that spot and retrieve it.
Indexing into a dictionary, I have to find that point in the dictionary that has the key and get the value back.
So that's going to be linear, because I have to, in principle, walk all the way down it.
It's a slight misstatement, as we'll see later on.
A dictionary actually uses a clever indexing scheme called a hash.
But in the worst case, this is going to be linear.","""I believe the complexity of determining the dictionary's length should be O(1). Why is it O(n)?"""
43,8C_T4iTzPCU,"All right, I'm on a roll not just with Frisbees.
I finished the proof with a finger to spare.
So f of S T equals c of S T.
All right, so that's exactly what we want.
We are saying f of S T is obviously a cardinality of f so I've shown this thing over here.
So that's why the Ford-Fulkerson algorithm works.
It's because of this analysis that the Ford-Fulkerson algorithm works.
So are we done? What are we missing in algorithm design, our algorithm analysis? Not you, yet.
AUDIENCE: [INAUDIBLE] PROFESSOR: Sorry? AUDIENCE: A runtime? PROFESSOR: Runtime, good runtime.","Isn't he simply verifying this for the specific cut he defined? However, the initial segment of the theorem asserts that |f| equals c(S,T) for ANY cut between S and T."
44,8C_T4iTzPCU,"And how many iterations do you really need if you did it right? Two.
So that's a billion factor slowdown.
So this is a pathological example, a simple pathological example, to just show you what the problem is.
But you can imagine that if you use depth-first search you might be a factor of five slower on average than if you use some of the technique.
And a factor of 5 is nothing to be scoffed at, especially if you're running for minutes.
And, you know, back in the day computers were horribly slow.
So how is this problem fixed? Well, any number of ways.
But the first real way that took into account asymptotitc complexity, did analysis, and did all of that was due to Edmonds and Karp, which is a few years after Ford-Fulkerson.
In fact, several years after Ford-Fulkerson.
And their contribution was not as much a new algorithm, though it is called Edmonds-Karp algorithm.","Initially, I didn't understand why there were 2 billion iterations. Doesn't it just oscillate between the two paths?"
45,8C_T4iTzPCU,"And the proof, the max flow, min-cut theorem, which is going to show this key result that we require, which is that when we terminate in the Ford-Fulkerson algorithm, we're going to have a max flow.
And that's the reason why it's a maximum flow algorithm.
If you don't have that proof, you don't have a max flow algorithm.
So hopefully all of that is clear.
Pipe up if you have questions.
And let's write out the max flow min-cut theorem, which I mentioned of it last time but never really got to even stating, but today we're going to state it and prove it.
So this is an interesting theorem.
I mean it's a legendary theorem.
And it's not your usual theorem in the sense that it makes a simple statement.
It actually makes three statements.
It says the following are equivalent, and it's got three things which are the following.","Revised sentence: ""Isn't he simply verifying this for the specific cut he defined? However, the initial segment of the theorem asserts that |f| equals c(S,T) for ANY cut S,T."""
46,8C_T4iTzPCU,"So it's simply the S to T edges and summing over the capacities.
And if you take a look, obviously S to T, you've got 4 plus 4 plus 4.
So you've got 4 plus 4 plus 4, corresponding to this one, this one, and that one.
Do I need to add this edge in here? Over here? Where does this edge go? From this to over there? That goes from T to S.
So that's good because that has a capacity of infinity.
That would cause trouble.
And so the other edges are I got 1, 5, and 7.","Could someone assist me? Srini mentioned that the capacity is infinite in this case, which seems logical for not including it in the minimum cut value because it is constrained by the source (S). However, we are considering flows from 'S' to 'T', whereas the infinite flow is from 'T' to 'S' (S and T represent the partitions we obtained after the cut), so shouldn't it be negative infinity? Why don't we account for the flow through the cut here as we did in the example (https://youtu.be/VYZGlgzr_As?t=3288)? I understand that the capacity of the middle region is considered infinite because it is limited by the incoming flow from the source, which will be adjusted accordingly."
47,8C_T4iTzPCU,"What are these pairs? As you can imagine, these pairs correspond to the games that each of these teams that are inside the circle play against each other.
So 3 plays 4 a certain number of times.
According to that table it's 4 times.
So I'm going to put a 4 in here.
This is 4 as well.
4, 5, 7, 2.
OK? And the edges in between here are going to have capacities of infinity and how are these edges structured? So far I've just explained how the left-hand side works.
These edges have a capacity of infinity.
1 2 goes to 1, 1 2 goes to 2.
1 3 goes to 1, 1 3 goes to 3.
And that's pretty much it.
So that's where these edges are.
That's how these edges are introduced.
And all of these edges have a capacity of infinity.
2 to 4, 3 to 4, and that.
So far, it's pretty straightforward.
There's one last thing that we need to do, which is add capacities to these edges.
This is actually crucial.
It turns out that we have to add capacities such that this max flow is going to represent elimination.",How is it that the capacities of edges are infinite?
48,8LEuyYXGQjU,"Okay, so what we've said here so far is that we have this approximation where what we do is we just take our policy, we run it out phi m times, for each of those m times we get a whole sequence of states and actions and rewards.
And then we average.
And this is an unbiased estimate of the policy gradient but it's very noisy.
So, this is gonna be unbiased and noisy.
If you think about what we saw before for things like Monte Carlo methods, it should look vaguely familiar, same sort of spirit, right? We have, um, we're just running out our policy.
We're gonna get some sum of rewards just like what we got in Monte Carlo, um, estimates.
But, [NOISE] so, it'll be unbiased estimate of the gradient.
So, it's unbiased estimate of the gradient, estimate of gradient.
But noisy.
So, what can make this actually practical? Um, there's a number of different techniques for doing that.
Um, but some of the things we'll start to talk about today are temporal structure and baselines.
[NOISE] Okay.","If you take a moment to work through the math on your own, you will find that the equation at the beginning (first line) does not equal the equation at the end (bottom line). To verify this, consider a simple case, such as an episode with three time steps and rewards r0, r1, and r2. Therefore, do not take her derivation at face value, as it omits several important details. Why is there such a discrepancy? The reason is that ""the policy's choice at a particular time step t only affects rewards received in subsequent steps of the episode and has no impact on rewards received in previous steps"" (https://web.stanford.edu/class/cs234/CS234Win2019/slides/lnotes8.pdf). Thus, the original equation at the beginning fails to consider this aspect. In my opinion, this is a crucial point that she neglected to mention."
49,8NYoQiRANpg,"As well as how you make predictions is, um-uh, is expressed only in terms of inner products, okay? So we're now ready to apply kernels and sometimes in machine learning people sometimes we call this a kernel trick and let me just the other recipe for what this means, uh, step 1 is write your whole algorithm, [NOISE] um.
[NOISE] In terms of X_i, X_j, in terms of inner products.
Uh, and instead of carrying the superscript, you know X_i, X_j, I'm sometimes gonna write inner product between X and Z, right? Where X and Z are supposed to be proxies for two different training examples X_i and X_j but it simplifies the notation, uh, right a little bit.","Discuss why the assumption \(\theta := \sum_{i=1}^n \beta_i \phi(x^{(i)})\) is reasonable in the context of kernel tricks, despite the limitations highlighted by the ""No Free Lunch"" theorem."
50,8NYoQiRANpg,"Right? So um, and it turns out that when X I is you know, 100 trillion dimensional, doing this will let us derive algorithms that work even in these  100 trillion or these infinite-dimensional feature spaces.
Now, I'm just deriving this uh, just as an assumption.
It turns out that there's a theorem called the representer theorem that shows that you can make this assumption without losing any performance.
Uh, the proof that represents the theorem is quite complicated.
I don't wanna do this in this class, uh, it is actually written out, the proof for why you can make this assumption is also written in the lecture notes, it's a pretty long and involved proof involving primal dual optimization.
Um, I don't wanna present the whole proof here but let me give you a flavor for why this is a reasonable assumption to make.
Okay? And when- just to- just to make things complicated later on uh, we actually do this.
Right? So Y I is always plus minus 1.","Discuss why the assumption \(\theta := \sum_{i=1}^n \beta_i \phi(x^{(i)})\) is reasonable. Consider the kernel tricks and the limitations of the ""No Free Lunch"" theorem."
51,8sOtXbQIOuE,"In general, the filtering distribution is the probability of the variables that you are considering conditioned on the evidence so far.
And suppose we have just two particles here, 0, 1, and 1, 2.
So now the propose step is going to take each of these particles, and I'm just going to sample a value for H3 the next variable-- given the transmission distribution.
Remember it was up and down with probability one quarter and the same with probability 1/2.
So that is going to produce these extended articles conditioned on the same evidence.
So for example, I'm going to take 0, 1.
I'm going to-- now that will produce this particle with probability 1/2 because I'm just keeping the value of the same year.
And I'm going to take this particle, and I'm going to extend it to 2.
And that's also going to happen with probability 1/2.
Now, this is a random algorithm.
So I could have sampled from the distribution.
I could have got 1 here.
I could have gotten 3 here, but let's just go with 1, 2.
So in the next step, I'm going to-- wait, so you should think about these particles as a guess as to what H3 is going to be.
But we need to fact-check this guess with evidence.
And so the weighting step is going to assign a weight to each article.
And that weight is going to be the probability of the new evidence I got conditioned on H3 here.
So this is going to produce a set of new particles, which are weighted representing the distribution-- the h1, h2, h3 conditioned on all the evidence so far.","Please explain why the probabilities, for example, do not add up to 1."
52,9TNI2wHmaeI,"And I'd like to talk a little bit about the relationship between NP-completeness and crypto.
Because we've made these assumptions about hardness.
Now, what's interesting here is that N composite is clearly in NP, but unknown if NP-complete.
So this is very interesting.
The tried and trusted algorithm for public key encryption relies on a computational assumption where the problem associated with that assumption is not even known to be NPC.
All right.
So that's kind of wild.
So how does this work? Or why does this work? Now, if you take other problems, like, is a graph 3-colorable? And so what does that mean? Well, you have three colors.
And you're not allowed to reuse the same color on two ends of an edge.
So if you put red over here, you can put red here, but you can't put red here and there.
And so that graph is 3-colorable.
But if you had a click, then this would not be 3-colorable.
Because you have all these edges.
You have three edges coming out.
And so clearly, the degree from a vertex is going to tell you what you have.
So if you have a 4-click over there, immediately it's not 3-colorable.
But checking whether a graphic is 3-colorable is NPC.
You can use a three set as a way of showing that.
So you can say, oh, wow, maybe I shouldn't be worried about RSA.","I believe that ""Is N composite?"" is not the appropriate decision problem for integer factorization. Since the AKS primality test has shown that ""PRIMES is in P,"" we can use AKS to determine whether an integer is composite. To do this, we simply run AKS and take the opposite of its output. A more suitable decision problem for integer factorization might be: ""Does a given number N have a factor d such that a < d <= b?"" This problem belongs to the NP class because a factor can be verified in polynomial time using the division algorithm. If we had a polynomial-time oracle to solve this decision problem, we could factor an integer in polynomial time by employing a binary search for factors within the interval (1, N-1], consulting the oracle. We could limit our search to prime factors by using AKS and then divide by each prime factor as often as possible once it is found. If the initial oracle query returns 'No,' we would conclude that N is the only factor. This algorithm would require O(log N) oracle calls per factor, and since a number N has O(log N / log log N) unique factors, according to a theorem in number theory, the overall process would result in a polynomial-time algorithm. However, such an algorithm would necessitate a polynomial-time oracle, the construction of which I will defer to the experts."
53,9g32v7bK3Co,"And again if you remember search problems, the solution to search problems was just a sequence of actions, said that's all I had, like a sequence of actions, a path that was a solution.
And the reason that was a good solution was like everything was deterministic, so I could just give you the path and then that was what you would follow.
But in the case of MDPs, the way we are defining a solution is by using this notion of a policy.
So a policy- let me actually write that here.
So we have defined an MDP but now I want to say well, what is a solution of an MDP? A solution of an Markov decision pro- process is a policy pi of S.","In this context, a policy is characterized as a direct mapping from the state space to the action space; for instance, the prescribed policy at station-4 is to walk. This interpretation diverges from the one presented in the seminal RL text by Sutton and Barto, where a policy is described as ""a mapping from states to the probabilities of selecting each possible action."" To illustrate, at station-4, the policy might entail a 40% probability of walking and a 60% probability of taking the train. Consequently, the policy evaluation algorithm introduced in this lecture deviates slightly by omitting the iteration over potential actions. It is commendable that the instructor emphasizes this distinction."
54,9g32v7bK3Co,"Those are like, the only things I'm storing, because that allows me to compute and if I've converged then that kind of allows me to keep going because I only need my previous values to update my new values, right.
In terms of complexity, well this is going to take order of T times S times S prime.
Well, why is that? Because I'm iterating over T times step, and I'm iterating over all my states and I'm summing over all S primes, right.
So because of that- that's a complex idea yet, and one thing to notice here, is it- it doesn't depend on actions, right.
It doesn't depend on the size of actions.
And the reason it doesn't depend on the size of actions as you have given me the policy, you are telling me follow this policy.
So if you've given me the policy then I don't really need to worry about, like, the number of actions I have.
Okay.
All right.
Um, here is just another like the same example that we have seen.
So at iteration T equal to 1, in, is going to get 4, end is going to get 0, at iteration 2 it gets a slightly better value.","In this context, a policy is characterized as a direct mapping from the state space to the action space; for instance, the prescribed policy at station-4 is to walk. This interpretation diverges from the one presented in the classic reinforcement learning text by Sutton and Barto, where a policy is described as ""a mapping from states to the probabilities of selecting each possible action."" To illustrate, at station-4, the policy might entail a 40% probability of choosing to walk and a 60% probability of opting to take the train. The policy evaluation algorithm introduced in this lecture is also distinct, as it does not iterate over all possible actions. The instructor's decision to emphasize this distinction is appreciated."
55,A6Ud6oUCRak,"So let's go from here over to here and say that the probability of a whole bunch of things-- x1 through x10-- is equal to some product of probabilities.
We'll let the index i run from n to 1.
Probability of x to the last one in the series, conditioned on all the other ones-- sorry, that's probability of i, i minus 1 down to x1 like so.
And for the first one in this product, i will be equal to n.
For the second one, i will be equal to n minus 1.
But you'll notice that as I go from n toward 1, these conditionals get smaller-- the number of things on condition get smaller, and none of these things are on the left.","I have a question regarding the explanation of the general formula for conditional probabilities. However, when we apply it to the case P(a, b, c), where x1 = a, x2 = b, and x3 = c, we do not obtain the same result as the professor; it seems the probabilities are shifted in the opposite direction. In other words, we end up with P(a) as the last term instead of P(c). Can someone explain to me where my mistake lies? Thank you."
56,A6Ud6oUCRak,"What's that mean? That means that if you know that we're dealing with z, then the probability of a doesn't depend on b.
b doesn't matter anymore once you're restricted to being in z.
So you can look at that this way.
Here's a, and here's b, and here is z.
So what we're saying is that we're restricting the world to being in this part of the universe where z is.
So the probability of a given b and z is this piece in here.
a given b and z is that part there.
And the probability of a given z is this part here divided by all of z.",Is the region for z correct? I think it should cover both a and b.
57,AbhV49lfaWw,"There are three parts.
And we also covered regularization before we go into regularization, so we've just discussed the three components.
We still haven't spoken of any trade-offs between them.
But this- this is the mental model to have in, you know, in your mind to decompose the- the test error into three parts, right? Part due to noise in the test data, that's irreducible error, part due to noise in the training data, that's variance and bias.
Now, we also spoke about regularization and we will see why regularization, er, plays a role in, um, um, shortly, soon.
So regularization is a way in which we want to penalize our estimated parameters from having very large values.","At the given timestamp, we discuss the ""noise"" in the training data that leads to variance when testing various models on the same test data point. These models are constructed from different samples. Is the noise we refer to solely attributable to these differing samples, or is it a combination of the varying samples and the irreducible error associated with each y_train (since each y has its own distribution, which, in the case of regression, is Gaussian) from the training data?"
58,Amd_bNYzgUw,"And this follows as a trivial consequence of the inclusion-exclusion rule for two sets, because the probability of A union B is equal to this plus this minus some probability, namely, the probability of the intersection.
So you're taking away something non-negative from these two in order to equal that.
In particular, then, this must be less than or equal to that.
And the closely related phenomenon is [? basi-- ?] [AUDIO OUT] The probability that A or B happens is greater than or equal to the probability that A happens.","Should its caption be ""Union Bound"" or ""Boole's Inequality""?"
59,B5y47gWt3co,"So the way we write this in terms of, um, uh, GIN operator is to say, aha, we are taking the messages from the children, we aggregate- we transform them using an MLP, this is our function f, and we summed them up.
Um, and then we also add 1 plus epsilon, where epsilon is some small, uh, learnable scalar, our own message transformed by f and then add the two together and pass through another function, uh, phi.","""What is the purpose of using the term (1 + epsilon) on slide 70?"""
60,BZTWXl9QNK8,"So they're on a scale of minutes.
So if you connect within the same minute, then you're in good shape.
And if you connect on the minute boundary, well, too bad.
Yet another problem with the scheme-- it's imperfect in many ways.
But most operating systems, including Linux, actually have ways of detecting if there's too many entries building up in this table that aren't being completed.
It switches to this other scheme instead to make sure it doesn't overflow this table.
Yeah.
AUDIENCE: So if the attacker has control of a lot of IP addresses, and they do this, and even if you switch it the same-- PROFESSOR: Yeah, so then actually there's not much you can do.
The reason that we were so worried about this scheme in the first place is because we wanted to filter out or somehow distinguish between the attacker and the good guys.
And if the attacker has more IP addresses and just controls more machines than the good guys, then he can just connect to our server and request lots of web pages or maintain connections.
And it's very hard then for the server to distinguish whether these are legitimate clients or just the attacker tying up resources of the server.
So you're absolutely right.
This only addresses the case where the attacker has a small number of IP addresses and wants to amplify his effect.","""If the timestamp is included in the message, then why does it matter whether you connect at the exact minute boundary?"""
61,C1lhuz6pZC0,"And if we know that, what's the order? AUDIENCE: [INAUDIBLE].
JOHN GUTTAG: N log n plus n-- I guess is order n log n, right? So it's pretty efficient.
And we can do this for big numbers like a million.
Log of a million times a million is not a very big number.
So it's very efficient.
Here's some code that uses greedy.
Takes in the items, the constraint, in this case will be the weight, and just calls greedy, but with the keyfunction and prints what we have.
So we're going to test greedy.
I actually think I used 750 in the code, but we can use 800.
It doesn't matter.
And here's something we haven't seen before.
So used greedy by value to allocate and calls testGreedy with food, maxUnits and Food.getValue.
Notice it's passing the function.
That's why it's not-- no closed parentheses after it.
Used greedy to allocate.","What is the purpose of the ""item""?"
62,C1lhuz6pZC0,"So let's go look at the code that does this.
So here you have it or maybe you don't, because every time I switch applications Windows decides I don't want to show you the screen anyway.
This really shouldn't be necessary.
Keep changes.
Why it keeps forgetting, I don't know.
Anyway, so here's the code.
It's all the code we just looked at.
Now let's run it.
Well, what we see here is that we use greedy by value to allocate 750 calories, and it chooses a burger, the pizza, and the wine for a total of-- a value of 284 happiness points, if you will.","I don't understand why we obtain different answers from the greedy algorithms when we use the same items and the same key function. Although it performs local optimization, it doesn't imply that the local optimization should vary with each execution of the program, provided that the parameters remain constant."
63,C6EWVBNCxsc,"So each level, in fact, is going to be exactly n over b cost.
We should be a little careful about the bottom because the base case-- I mean, it happens that the base case matches this.
But it's always good practice to think about the leaf level separately.
But the leaf level is just m over b times n over m The m's cancel, so m over b times n over m.
This is n over b.
So every level is n over b.
The number of levels is log of n over m.
Cool.
So the number of memory transfers is just the product of those two things.","I am curious as to why the height is lgN - lgM rather than lg(N/B) - lg(M/B). Although the result is the same, it is a bit confusing."
64,CAKSh3M0y8k,"Now, I can also cancel k if it's relatively prime to n.
And the reason is that if I have ak equivalent to bk mod n and the gcd of k and n is 1, then I have this k prime that's an inverse of k.
So, I just multiply both sides by the inverse of k, namely k prime.
And I get that the left hand side is a times k, k inverse.
And the right hand side is b times k, k inverse.
And of course, that's a times 1 is equivalent to b times 1.","""Where, then, is the proof for the associativity of modular multiplication? ;>"""
65,CG4ihzTaGdM,"So y of minus 1 is x of minus 1 minus x of minus 2.
Since both of those are 0, it says that the output at time minus 1 is 0.
Trivial, right? Trivial.
And similarly, we can just iterate through the solution to the whole signal.
So y of 0 is x of 0 minus x of minus 1.
x of 0 is that special one, that is 1.
So now we get 1 minus 0, which is 1.
y of 1 is x of 1 minus x of 0.
Now the special one is on the other side of the minus sign, so the answer is minus 1.
y of 2 is x of 2 minus x of 1 -- they're both 0.
And in fact, all the answers from now on are going to be 0.
So what I just did is a trivial example of -- I use a difference equation to represent a system, and I figured out the output signal from the input signal.
That's the method that we call-- that's the representation for discrete time systems that we refer to as difference equations.","""Isn't x[2] equal to 2, given that on the x-axis the values are -1 for x[-1], 0 for x[0], and so on? If that's not the case, where do the x values originate from? I'm confused; this isn't trivial."""
66,CHhwJjR0mZA,"This is a bit circular.
I'm going to define an array in terms of the word RAM, which is defined in terms of arrays.
But I think you know the idea.
So we have a big memory which goes off to infinity, maybe.
It's divided into words.
Each word here is w bits long.
This is word 0, word 1, word 2.
And you can access this array randomly-- random access memory.
So I can give you the number 5 and get 0 1, 2, 3, 4, 5, the fifth word in this RAM.
That's how actual memories work.
You can access any of them equally quickly.
OK, so that's memory.
And so what we want to do is, when we say an array, we want this to be a consecutive chunk of memory.
Let me get color.
Let's say I have an array of size 4 and it lives here.
Jason can't spell, but I can't count.
So I think that's four.
We've got-- so the array starts here and it ends over here.
It's of size 4.
And it's consecutive, which means, if I want to access the array at position-- at index i, then this is the same thing as accessing my memory array at position-- wherever the array starts, which I'll call the address of the array-- in Python, this is ID of array-- plus i.
OK.
This is just simple offset arithmetic.
If I want to know the 0th item of the array, it's right here, where it starts.
The first item is one after that.
The second item is one after that.
So as long as I store my array consecutively in memory, I can access the array in constant time.
I can do get_at and set_at as quickly as I can randomly access the memory and get value-- or set a value-- which we're assuming is constant time.","""If you cannot explain arrays without referring to their physical structure, words, and bits, then you have failed. Even he gives the impression that array elements are generic words, when in reality, an array can have elements of any size. My instructors from 35 years ago did a better job at explaining this."""
67,E-_ecpD5PkE,"So they're both important, they're doing slightly different things.
All right.
So let's talk about, ah, could we move this up, please? Thank you.
Okay.
So like we sort of started talking about before, um, well, let's- let's talk about first the baseline.
So how should we choose the baseline? Um, one thing that we can do for the baseline, is just to- like what that what we're seeing there, which is an empirical estimate of V_Pi i.
So we could say, in general, we wanna just have- use V_Pi i as a baseline.
That means we have to compute it somehow.
And the way we estimate that could be from Monte Carlo or it could be from TD methods.
All right.",Why has the definition of the advantage function now become the subtraction of two Q-value functions? I thought the advantage function was previously defined as the subtraction of the Q (state-action) function and the V (value) function.
68,E3f2Camj0Is,"They don't have to be- This has- doesn't have to be anything to do with value iteration.
These are just two different value functions.
One could be, you know, 1,3,7,2 and the other one could be 5,6,9,8.
Okay.
So we just have two different vectors of value functions and then we re-express what they are after we apply the Bellman backup operator.
So there's that max a, the immediate reward plus the discounted sum of future rewards where we've plugged in our two different value functions.
And then what we say there is, well, if you get to pick that max a separately for those two, the distance between those is lower bounded than if you kind of try to maximize that difference there by putting that max a in.
And then you can cancel the rewards.
So that's what happens in the third line.
And then the next thing we can do is we can bound and say the difference between these two value functions is diff- is, um, bounded by the maximum of the distance between those two.
So you can pick the places at which those value functions most differ.
And then you can move it out of the sum.",Does anybody understand how she got to the 2nd step of the equation?
69,E3f2Camj0Is,"Um, I in this case because we're thinking about processes that are infinite horizon, the value function is stationary, um, and it's fine if you have include self loops.
So, it's fine if some of the states that you might transition back to the same state there's no problem.
You do need that this matrix is well-defined.
That you can take that you can take the inverse of it.
Um, but for most processes that is.
Um, so, if we wanna solve this directly, um, this is nice it's analytic, um, but it requires taking a matrix inverse.
And if you have N states so let's say you have N states there's generally on the order of somewhere between N squared and N cubed depending on which matrix inversion you're using.
Yeah.
Is it ever actually possible for, uh, that matrix not to have an inverse or does like the property that like column sum to one or something make it not possible? Question was is it ever possible for this not to have an inverse? Um, it's a it's a good question.
Um, I think it's basically never possible for this not to have an inverse.
I'm trying to think whether or not that can be violated in some cases.
Um, if yeah sorry go ahead.
Okay.
[NOISE] Yeah.
So, I think there's a couple, um, if there's a- if this ends up being the zero matrix, um depending on how things are defined.
Um, but I'll double-check then send a note on a Piazza.
Yeah.
Well, actually I think the biggest side about the transition matrix [inaudible] Let me just double check so I don't say anything that's incorrect and then I'll just send a note on- on Piazza.","Conjecture: An inverse exists if gamma is in the range [0,1), and it fails to exist if gamma equals 1. This is easy to check for systems with one or two states."
70,EC6bf8JCpDQ,"Ah.
But first of all, none of those expressions condition any of the variables on anything other than non-descendants, all right? That's just because of the way I've arranged the variables.
And I can always do that because are no loops.
I can always chew away at the bottom.
That ensures that whenever I write a variable, it's going to be conditioned on stuff other than its descendants.
So all of these variables in any of these conditional probabilities are non-descendants.
Oh wait.
When I drew this diagram, I asserted that no variable depends on any non-descendant given its parents.
So if I know the parents of a variable I know that the variable is independent of all other non-descendants.
All right? Now I can start scratching stuff out.
Well, let's see.
I know that C, from my diagram, has only one parent, D.
So given its parent, it's independent of all other non-descendants.
So I can scratch them out.
D he has two parents, B and R.
But given that, I can scratch out any other non-descendant.
B is conditional on T and R.
Ah, but B has no parent.
So it actually is independent of those two guys.
The trashcan, yeah, that's dependent on R.
And R over here, the final thing in the chain, that's just a probability.
So now I have a way of calculating any entry in that table because any entry in that table is going to be some combination of values for all those variables.","Thank you for the lecture! During the segment where you explained the bottom-to-top approach, starting at the minute mark where you discussed ""chewing variables from the bottom,"" I noticed that you excluded the R variable from P(B | T, R), given that B has no parents and is thus independent of both T and R. Intuitively, this makes sense to me; however, the ""explaining away"" principle suggests a connection between B and R since they both can cause D. If we assume D is true, there is a relationship between B and R. For instance, if the dog barked and there is a burglar, this could ""explain away"" the possibility of a raccoon's presence, and the reverse is also true. My question is: when and how should the ""explaining away"" principle be applied in modeling a system with belief networks? Additionally, if it is to be used, how would the relationship between B and R be represented? I would greatly appreciate your insights on this matter.
The Markov blanket is employed for a more robust purpose: it is utilized when we seek to establish the conditional independence of a specific node (A) from all other nodes. To achieve this, we must condition on A's Markov blanket. This blanket encompasses the nodes discussed in the lecture, which include A's parents and descendants, as well as the parents of A's descendants, should they exist. Intuitively, this is due to the fact that a descendant of A is likely influenced by all its parents. Therefore, understanding the state of those parents is essential if we are to make assertions about the behavior of that descendant in relation to A."
71,EK0sgHPLou8,"So here's one example for the Mixup.
We can generate some virtual examples between two classes.
So the first image in the left-hand side is for the cat.
And then next one is for dog.
And then we can combine them to generate some images in between.
So from cat and dog.
So this image has around the 70% probability to be classified as a cat and the 30% to be classified as a dog.
So this is a very common and a useful ways to do data augmentation.
Any questions about the process for the Mixup? So in domain generation, Mixup itself can improve the performance of domain generations.
So let's see.
We here, we want to do the tissue classification from Camelyon and also to do some learned type prediction.","How do we ensure that the final interpolated example is coherent? Should we simply use a low weight for the off class, or does that not significantly affect the outcome?"
72,EmSmaW-ud6A,"There's only one negative edge weight here.
What if I just added a large number, or in particular, the negative of the smallest edge in my graph to every edge in my graph? Then I'll have a graph with non-negative weights.
Fantastic.
Why is that not a good idea? Well, in particular, if I did that to this graph, if I added 2 to every edge, the weight of this path, which was the shortest path, changed from weight 3 to weight 9, because I added 2 for every edge.
But this path, which wasn't a shortest path in the original graph-- it had weight 4-- increased only by 2.
Now that is a shortest path.
Or it's a shorter path than this one, so this one can't be a shortest path.
So that transformation, sure, would make all the weights non-negative, but would not preserve shortest paths.","The telescope sum ensures that any path between a pair of vertices can be adjusted by adding the same value. However, it does not preserve this uniform adjustment across the entire graph. In other words, the equality of edge weights in a reweighted graph may be altered. For instance, if the weight of edge A~D is 500 and B~Z is 300, they could be reweighted to A~D = 600 and B~Z = 700, respectively. Consequently, we cannot depend on the edge relationships in telescope-sum-reweighted graphs. This contrasts with the intuitive method of adding weight to every edge until all are non-negative, which, although not meeting the reweighting requirement, does maintain the equality of edge weights. This has been a blind spot for me; I was initially trying to understand how the reweighting process could preserve global relationships."
73,EzeYI7p9MjU,"I mean, you could have defined it differently.
We're going to go with less than or equal to.
So in general, the rank, of course, is something that could be used very easily to find the median.
So if you want to find the element of rank n plus 1 divided by 2 floor, that's what we call the lower median.
And n plus 1 divided by 2 ceiling is the upper median.
And they may be the same if n is odd.
But that's what we want.","Has anyone perhaps found the definition of 'rank' at timestamp 501:070 to 880? Generally, we will define the rank of x, from 502:880 to 510, as the count of numbers in the set that are less than or equal to x. I apologize for any confusion; the definition could have been different. We will adhere to the definition of rank as the count of numbers in the set that are less than or equal to x, from 504:270 to 750. Is there a typo? Upon reviewing the written notes, I found that ""the number of numbers in the set that are smaller than x"" is a more accurate description compared to the definition of rank presented on the blackboard in the video, which states ""numbers in the set that are smaller than x."" In summary, the discrepancy is between ""the number of numbers in the set"" and ""numbers in the set."""
74,EzeYI7p9MjU,"Let's say n is odd.
And it's floor of n over 2.
You can find that median.
Right, so it's pretty easy if you can do sorting.
But we're never satisfied with using a standard algorithm.
If we think that we can do better than that.
So the whole game here is going to be I'm going to find the median.
And I want to do it in better than theta n log n time.
OK, so that's what median finding is all about.
You're going to use divide and conquer for this.
And so in general, we're going to define, given a set of n numbers, define rank of x as the numbers in the set that are greater than-- I'm sorry, less than or equal to x.","Has anyone perhaps found the definition of 'rank' within the range of 501,070 to 880? Generally speaking, we will define the rank of x, within the range of 502,880 to 510, as the count of numbers in the set that are less than or equal to x. I apologize for any confusion; there might be a discrepancy. Upon reviewing the written notes, I found that ""the number of numbers in the set that are smaller than x"" seems to be a more accurate definition compared to what was defined on the blackboard in the video as ""numbers in the set that are smaller than x."" To clarify, the distinction is between ""the number of numbers in the set"" and ""numbers in the set."""
75,EzeYI7p9MjU,"PROFESSOR: O n-- exactly right.
So on test complexity-- and so we got over theta n cubed complexity, OK? So it makes sense to do divide and conquer if you can do better than this.
Because this is a really simple algorithm.
The good news is we will be able to do better than that.
And now that we have a particular algorithm-- I'm not quite ready to show you that yet.
Now that we have a particular algorithm, we can think about how we can improve things.
And of course we're going to use divide and conquer.
So let's go ahead and do that.
And so generally, the divide and conquer, as I mentioned before, in most cases, the division is pretty straightforward.
And that's the case here as well.
All the fun is going to be in the merge step.
Right, so what we're going to do, as you can imagine, is we're going to take these points.
And we're going to break them up.
And the way we're going to break them up is by dividing them into half lengths.
We're going to just draw a line.
And we're going to say everything to the left of the line is one sub problem, everything to the right of the line is another sub problem, go off and find the convex hull for each of the sub problems.
If you have two points, you're done, obviously.
It's trivial.
And at some point, you can say I'm just going to deal with brute force.
If we can go down to order n cubed, if n is small, I can just apply that algorithm.
So it doesn't even have to be the base case of n equals 1 or n equals 2.","It seems like there might be a typo or some context missing in the sentence you've provided. If ""n3"" refers to a specific place or term that is understood in the original context, the revised sentence could be:

""Why is it at n3?""

However, if ""n3"" is a placeholder for something else or a mistake, please provide the correct term or additional context for a more accurate revision."
76,EzeYI7p9MjU,"There's always a little bit of convenience thrown in here.
We will assume that the a has unique elements.
So there's nothing that's x, OK? Good.
So the recurrence, once you do that, is t of n equals-- we're going to just say it's order one for n less than or equal to 140.
Where did that come from? Well, like 140.
It's just a large number.
It came from the fact that you're going to see 10 minus 3, which is 7.
And then you want to multiply that by 2.
So some reasonably large number-- we're going to go off and we're going to assume that's a constant.
So you could sort those 140 numbers and find the median or whatever rank.
It's all constant time once you get down to the base case.
So you just want it to be large enough such that you could break it up and you have something interesting going on with respect to the number of columns.
So don't worry much about that number.
The key thing here is the recurrence, all right? And this is what we have spent the rest of our time on.
And I'll just write this out and explain where these numbers came from.
So that's our recurrence for n less than or equal to 140.
And else, you're going to do this.
So what is going on here? What are all of these components corresponding to this recurrence? Really quickly, this is simply something that says I'm finding the median of medians.",Can somebody explain why it is T(n/5) and not 5T(n/5)? Aren't we performing the recursion five times at each step?
77,FgzM3zpZ55o,"So, what does planning involve? Involves optimization, often generalization and delayed consequences.
You might take a move and go early and it might not be immediately obvious if that was a good move until many steps later but it doesn't involve exploration.
The idea and planning is that you're given a model of how the world works.
So, your given the rules of the game, for example, and you know what the reward is.
Um, and the hard part is computing what you should do given the model of the world.
So, it doesn't require exploration.
And supervised machine learning versus reinforcement learning.
It often involves optimization and generalization but frequently it doesn't invo-, involve either exploration or delayed consequences.
So, it doesn't tend to involve exploration because typically in supervised learning you're given a data set.","I wonder why the game of Go doesn't require exploration. In fact, even human players would compute several moves ahead to explore the possibilities."
78,FkfsmwAtDdY,"So now, I can start using these sets to make assertions about my database that can be useful to know.
So for example, if I want to say that every student is registered for some subject-- which, of course, they are-- what I would say is that D, the set of all students, is a subset of R inverse of J.
So this concise set theoretic containment statement-- d is a subset of R inverse of J-- is a slick way of writing the precise statement that says that all the students are registered for some subject.
Now, happens not to be true by the way.
Because if you look back at that example, Adam was not registered for a subject.","Why not use D = R^(-1)(J) to indicate that every student is registered for at least one subject? Using the ""is a subset of"" symbol contradicts my intuition, as it suggests that the domain of discourse could have fewer elements than the set of students registered for at least one subject, which is impossible."
79,FkfsmwAtDdY,"Which means that R of Jason is that set of two courses that he's associated with or that are associated with him-- that he's registered 6.042 and 6.012.
So at this point, we've applied R to one domain element-- one student Jason.
But the interesting case is when you apply R to a bunch of students.
So the general setup is that if x is a set of students-- a subset of the domain, which we've been showing in green-- then if I apply R to X, it gives me all the subjects that they're taking among them-- all the subjects that any one of them is taking.
Let's take a look at an example.
Well, another way to say it I guess is that R of X is everything in R that relates to things in X.
So if I look at Jason and Yihui and I want to know what do they connect to under R-- these are the subjects that Jason or Yihui is registered for.
The way I'd find that is by looking at the arrow diagram, and I'd find that Jason is taking 042 and 012.
And Yihui is taking 012 and 004.
So between them, they're taking three courses.
So R of Jason, Yihui is in fact 042, 012, and 004.
So another way to understand this idea of the image of a set R of X is that X is a set of points in the set that you're starting with called the domain.
And R of X is going to be all of the endpoints in the other set, the codomain, that start at X.
If I said that as a statement in formal logic or in set theory with logical notation, I would say that R of X is the set of j in subjects such that there is a d in X such that dRj.
So what that's exactly saying that dRj says that d is the starting point in the domain.
d is a student.
j is a subject.
dRj means there's an arrow that goes from student d to subject j.
And we're collecting the set of those j's that started some d.
So an arrow from X goes to j is what exists at d an X.
dRj means-- written in logic notation-- it's really talking about the endpoints of arrows, and that's a nice way to think about it.","I am troubled by something: the teacher uses the pipe symbol ""|"" and the dot symbol ""."", yet both seem to be translated as ""such as."" Is there a difference between these two symbols?"
80,FlGjISF3l78,"So you can have multiple levels of inheritance.
What happens when you create an object that is of type something that's been-- of a type that's the child class of a child class of a child class, right? What happens when you call a method on that object? Well, Python's are going to say, does a method with that name exist in my current class definition? And if so, use that.
But if not, then, look to my parents.
Do my parents know how to do that, right? Do my parents have a method for whatever I want to do? If so, use that.
If not, look to their parents, and so on and so on.
So you're sort of tracing back up your ancestry to figure out if you can do this method or not.
So let's look at a slightly more complicated example.
We have a class named Person.
It's going to inherit from Animal.
Inside this person, I'm going to create my own-- I'm going to create an __init__ method.
And the __init__ method is going to do something different than what the animal's __init__ method is doing.
It's going to take in self, as usual.
And it's going to take in two parameters as opposed to one, a name and an age.
First thing the __init__ method's doing is it's calling the animal's __init__ method.
Why am I doing that? Well, I could theoretically initialize the name and the age data attributes that Animal initializes in this method.","Why write Animal__init__(self, age) at all in the code?"
81,G7mqtB6npfE,"We can use B to compute A, so then A must be easy.
But since we know that A is hard, that there's something wrong in our logic, so B must be hard.
Does that makes sense? Yes? Sort of? So let's move onto an actual problem.
So the first problem we're going to reduce is the Hamiltonian path.
So a well-known NP hard problem is the Hamiltonian cycle.
So here our A is-- so it's a Hamiltonian cycle.
So what's a Hamiltonian cycle? So what's a Hamiltonian cycle? So a Hamiltonian cycle-- so let's say you have a graph.
So we have this graph.
Let me draw this out.
That's it.
So a Hamiltonian cycle is a cycle in the graph which starts at some vertex, visits all the other vertices, and comes back to the starting vertex.
So in this case, we could do something like go here, and then take this vertex, take this vertex, take this vertex, and come back here.
So that is a valid Hamiltonian cycle.
So this graph is a Hamiltonian cycle.
So the decision problem is here that given the graph, does it have a Hamiltonian cycle? And that problem is NP-hard, so you can [INAUDIBLE] polynomial [INAUDIBLE].
So now the new polynomial shows NP-hard, which is B is Hamiltonian path.
So the Hamiltonian path is a very similar problem.
Instead of a cycle, you remove the requirement that you have to come back to the starting point.
You can just start anywhere and [INAUDIBLE] all the vertices and stop.
So for example, if you remove this edge, this graph no longer has Hamiltonian cycle, but it has a Hamiltonian path, which is just this line.
Simple.
So this is a simple reduction because the problems are very similar.
So the first step is, of course, showing that Hamiltonian path is an NP.
So that should be pretty clear because-- so what is our certificate here? So if someone says, OK, I have solved the Hamiltonian path and this is my Hamiltonian path.",How is that possible? Aren't we required to avoid repeating any vertices in a Hamiltonian cycle?
82,GqmQg-cszw4,"In particular, lab one is going to rely on a lot of subtle details of C and Assembly code that we don't really teach in other classes here in as much detail.
So it's probably a good idea to start early.
And we'll try to get the TAs to hold office hours next week where we'll do some sort of a tutorial session where we can help you get started with understanding what a binary program looks like, how to disassemble it, how to figure out what's on the stack, and so on.
All right.
And I guess the one other thing, we're actually videotaping lectures this year.
So you might be able to watch these online.","Where can I learn about the appearance of a binary program, the process of disassembling it, and the method for determining the contents of the stack, as mentioned earlier? Additionally, I would like to know whether it is possible for one program to access the data stored in the memory of another program."
83,GqmQg-cszw4,"The system works.
But if I want to say that no one other than the TAs can access the grades file, this is a much harder problem to solve, because now I have to figure out what could all these non TA people in the world to try to get my grades file, right? They could try to just open it and read it.
Maybe my file system will disallow it.
But they might try all kinds of other attacks, like guessing the password for the TAs or stealing the TAs laptops or breaking into the room or who knows, right? This is all stuff that we have to really put into our threat model.
Probably for this class, I'm not that concerned about the grades file to worry about these guys' laptops being stolen from their dorm room.
Although maybe I should be.
I don't know.
It's hard to tell, right? And as a result, this security game is often not so clear cut as to what the right set of assumptions to make is.
And it's only after the fact that you often realize, well should have thought of that.
All right.","How do threat models go wrong?
Is there a more detailed explanation for the atoi conversion that results in writing 0?"
84,IM9ANAbufYM,"There are other methods that you should be able to figure out right now.
Even if you don't know class activation maps.
[NOISE] So to sum it up.
[NOISE] We have an image, [NOISE] input image, [NOISE] put it in your new network that is a binary classifier.
[NOISE] And the network says one.
You wanna figure out why the network says one, based on which pixels, what do you do? Visualize the weights.
[NOISE] Visualize the weights.
Uh, what do you visualize in the weights? The edges.
So I think visualizing the weights, uh, is not related to the input.
The weights are not gonna change based on the input.
So here you wanna know why this input led to one.","He says to visualize the weights, which is indeed true. This is because dY/dX actually yields the transposed weights (having the same shape as the input image), so we are essentially visualizing the weights. What am I missing here?"
85,IPSaG9RRc-k,"Can someone tell me what this is, asymptotically? Yeah? STUDENT: n cubed-- JASON KU: n cubed-- why is that? Well, if we plug this stuff into that definition here, we have n factorial over 3 factorial n minus 3 factorial.
n factorial over n minus 3 factorial just leaves us with an n, an n minus 1, and an n minus 2 over 6.
And if you multiply all that out, the leading term is an n cubed, so this thing is asymptotically n cubed.
I skipped some steps, but hopefully you could follow that.
And then the last thing to remain is this one right there.
That one's a little tricky.
Anyone want to help me out here? What we can do is we can stick it into this formula, and then apply Sterling's approximation to replace the factorials.
That makes sense? OK, so what I'm going to do is-- let's do this in two steps.
This is going to be n factorial over-- what is this? n/2 factorial-- and then what is n minus n over 2? That's also n/2.
So this is going to be n/2 factorial squared.
Is that OK? Yeah? Now let's replace this stuff with Sterling's approximation and see if we can simplify.
So on the top, we have 2 pi n n/e to the n over-- and then we've got a square here, pi n.
I cancelled the 2-- n/2 over e to the n/2.
Did I do that right? OK.
I can't spell, and a lot of times, I make arithmetic errors, so catch me if I am doing one.
OK, so let's simplify this bottom here.
I'm not going to rewrite the top.
The bottom here-- we square this guy.
It's the pi times n.
And then this guy, n/2 squared-- that just stays is an n.
Then we have n/2 to the n/e-- something like that.
n/2 over e to the n-- that makes more-- me happier.
OK, so now we have this over this.
How do we simplify? Well, we can cancel out one of the root n's.
So we've got square root of pi n down here and square root of 2 up top.
And then what have we got? We've got n to the n down here and n to the n down-- up there, so those cancel.","Why does (n 3) equal (n^3)? Can someone please explain this to me?
""How does he obtain (n+1), n, and (n-2) over 6? I understand that it will yield n cubed, but I don't follow how he derived that."""
86,IPSaG9RRc-k,"I'm remembering that.
Now, I don't care about what's stored in x.next, because I've stored it locally.
That makes sense.
All right, so now I am free to relink that next pointer to my previous guy.
And now I can essentially shift my perspective over, so the thing that I'm going to relink now is the next one.
So x previous and x now equals x, x_next.
Does that make sense? Just relinked things over-- so that's the end of step 2.
Now, as I got down this at the end of this for loop, where is x? What is x_p, x, and x_next-- or x_n? Really, I'm only keeping track of x and x_p here.
So what are x_p and x at the end of this loop? I've done this n times.
I started with b at x.
So what is x? Yeah? So we have a vote that x is c.
STUDENT: [INAUDIBLE] JASON KU: So this is a little interesting.
All right.
I will tell you that c is either x_p, x, or x_n.
So we have one vote for x.
Who says something else? Eric doesn't like x.
There are only two other choices.","Could someone please explain line 11, a1? Thank you."
87,IPSaG9RRc-k,"OK, so that's the first thing.
Otherwise, what do we do? We shift one thing over and then we make a recursive call.
Does that make sense? OK.
So we'll delete the first thing as a temporary variable-- delete first.
And then we'll insert last, x.
And then we need to do the recursive call.
So what's a recursive call look like? Yeah? STUDENT: Shift_left D, K minus 1-- JASON KU: Yeah.
So shift_left D, K minus 1-- OK? And then we can return.
This thing doesn't need to return anything.
It's just doing stuff to the thing.
Right? And whenever we get this K, we make a call, that gets down to 0, we will terminate because we will return.
We're in this range somewhere between we-- have an input after this line.
We know that K is somewhere between 1 and n minus 1.
And what we'll do is, every time through this recursion, we will subtract 1 from K.
So this is a nice, well-ordered sequence.
We do the correct thing obviously in the base case, and as long as this thing was correct for a smaller value of K, this thing also does the correct thing, because we're shifting over one, as we are asked, and we're letting this do the work of the rest.
I don't have to think about that.
I just have to think about this one loop, this one part of the thing that I'm doing.
Constant amount of work is done in this section.
And how many times do I call a function? STUDENT: [INAUDIBLE] JASON KU: Yeah.",Can anyone explain to me how the first element of a data structure becomes the last element after the algorithm is applied?
88,Ih0cPR745fM,"Clearly, by the pigeonhole principle if nobody goes to 0, there must be two guys that collide.
So this problem is also total by the pigeonhole principal.
So it always has a solution, no matter what the circuit is.
And the class PPP is all problems in NP that are reducible to this problem.
Finally, the hierarchy of problems I defined is this.
P, FNP, there is total FNP somewhere here, which I don't show.
And these are the relationships of these problems.
I haven't shown that these arrows true, that this is a subclass, PPD's a subclass of these two subclasses.
This is easy.
This is a simple exercise we can think about.
This is basically my introduction to PPA, PPAD and related classes.
The final thing that I want to point out is answering a question that was asked after the previous lecture, which was why did you define these classes, and not just a TFNP complete problem.
Why did you have to pay special attention to precise existence argument that gives rise to the guarantee that your problems are total? And the reason for that is that actually TFNP is not what's called a syntactic class.
In other words, if I give you a problem with TFNP-- if I give you a Turing machine, you cannot decide whether that is computing a total problem, that no matter what the input to that machine is, there's always an output.
So I had to pay attention to the specialized existence arguments because for specialized existence arguments, I know a priori that the problem is total.
So in particular, no matter what circuit I give you here, I don't even have to check anything.
No matter what input you give me, I know there is a solution.
No matter what pairs of circuits you give me here, I don't need to check anything.
I know the answer to this problem-- there's always an answer to this problem, and so on, so forth.
No matter what you give to me as input, it is important to define complexity classes for which you can show hardness results to find complete problems.
Otherwise, you would have what is called promise classes, which are not amenable to showing the completeness results.","Does the hierarchy imply that P is a subclass of PPAD, or should P actually be FP?"
89,IiD3YZkkCmE,"But they're also codified things, like if you see that the rheumatoid factor in a lab test was negative, then-- actually, I don't know why that's-- oh, no, that counts against-- OK.
And then various exclusions.
So these were the things selected by our regularized logistic regression algorithm.
And I showed you the results before.
So we were able to get a positive predictive value of about 0.94.
Yeah? AUDIENCE: In a the previous slide, you said standardized regression coefficients.
So why did you standardize? Maybe I got the words wrong.
Just on the previous slide, the-- PETER SZOLOVITS: I think-- so the regression coefficients in a logistic regression are typically just odds ratios, right? So they tell you whether something makes a diagnosis more or less likely.
And where does it say standardized? AUDIENCE: [INAUDIBLE].
PETER SZOLOVITS: Oh, regression standardized.
I don't know why it says standardized.
Do you know why it says standardized? KATHERINE LIAO: Couple of things.
One is, when you run an algorithm right on your data set, you can't port it using the same coefficients because it's going to be different for each one.
So we didn't want people to feel like they can just add it on.
The other thing, when you standardize it, is you can see the relative weight of each coefficient.","The correct answer to the question, ""Why did you standardize the predictors?"" is that you should always standardize your data before feeding it into a regularized regression model. This is because a regularized model penalizes large coefficients, and standardization ensures that the magnitude of the coefficients does not depend on the scale of measurement of the predictors."
90,J8Eh7RqggsU,"Uh, this should be outside of the recurse object.
Yeah.
Glad you guys are paying attention.
Um, otherwise, yeah, it would do basically nothing.
Any other mistakes? [LAUGHTER] Yeah.
Um, there is also function decorators that like implement memoizing for you.
In this class, are you okay if we use that or would you rather us like make our own in this case? Um, you can use the deco- you can be fancy if you want.
Okay.
Um, yeah.
But- but I think this is, you know, pretty transparent.
Easy for learning purposes.
Okay.
So let's run this.
So now it runs instantaneously as opposed to- I actually don't know how long it would have taken otherwise.
Okay.
And sanity check for t is probably the right answer because there's four was the original answer and multiply by 10.","""I don't understand how the cache works. Can someone please explain?"""
91,JDW82csukhE,"Uh, and also, um, another interesting side note is that, uh, we fix the pre-training strategy and the pre-training different GNNs models, uh, use different GNNs models for pre-training.
And what we found out is that the most expressive GNN model, namely GIN, that we've learned in the lecture, um, um, benefits most from pre-training.
Uh, as you can see here, the gain of pre-trained model versus non-pre-trained model is the- is the largest, um, in terms of accuracy.
And- and the intuition here is that the expressive GNN model can learn to capture more domain knowledge than less expressive model, especially learned from large amounts of, uh, data during pre-training.
So to summarize of our GNNs, we've, uh, said- learned that the GNNs have important application in scientific domains like molecular property prediction or, um, protein function prediction, but, uh, those application domains present the challenges of label scarcity and out-of-distribution prediction, and we argue that pre-training is a promising, uh, framework to tackle both of the challenges.
However, we found that naive pre-training strategy of this, uh, supervised graph level pre-training gives sub-optimal performance and even leads to negative transfer.
And our strategy is to, um, effective strategy is to pre-train both node and graph embeddings, and we found this strategy leads to a significant performance gain on diverse downstream tasks.
Um, yeah, thank you for listening.
","""It seems odd that the GAT performs the worst and benefits the least from pretraining, doesn't it? Theoretically, it should be more expressive than both GCN and GraphSAGE."""
92,KlQiwkhLBg0,"But in the problem, in the homework problem, they were sneaky.
And they only said that f of T is big O of something.
So remember, what's the difference between big O and big theta? Well, intuitively, big theta says that my function really does look like this guy.
Somehow, it's bounded above and below as I go far enough out.
In big O, there's just a bound above, right? So this is somehow looser.
And so the way to apply master theorem in this case is say, well, at least f of n is upper bounded.
It looks like this.
So the best that I can do is to replace this guy also with an upper bound.
Yeah.","""I think it should be 2^m - 1 < k < 2^m. If k were less than or equal to 2^m - 1, then it wouldn't reach 2^m. Please correct me if I am wrong.""
This question pertains to locating planets using the index K. Assuming that the key indices are unique, can't we just set the left to 1 and the right to K, given that the K key will never be at an index greater than K?
Wouldn't the last house also be special because it is a house that has no easterly neighbor? Therefore, the sequence actually has two special houses, not all but one. Perhaps my understanding of ""all but one"" is incorrect."
93,KsHOdr5UYZ0,"If you have tasks that are complex or require a lot of thinking and processing data, those are problems where AI can help.
If you have a problem with drug overdoses or with suicides, AI is probably not going to help.
And in fact, I've heard of cases where people collect data, and they do all of this quantitative analysis, but this is fundamentally a human problem.
And so it can be actually quite distracting.
So my point is simply that, at its best, AI can make the Air Force more efficient, more effective.
At its worst, it can distract us from actually solving difficult, challenging human and cultural problems.
So it's really important to be discerning about what kind of problem you're trying to solve and where AI can help.
But at this point, I hope, and I think that you have the tools to prevent that from happening, and to harness this very powerful new technology in support of building a more effective Air Force and pursuing America's interests.
So thank you all so much.
I'm going to stick around for questions, and I will leave you with this conclusion slide.
","""I strongly disagree with the statement, 'If you have a problem with drug overdoses or with suicides, AI is probably not going to help.' Can someone explain why he believes AI is incapable of addressing human issues? There is already evidence demonstrating that AI can assist with such problems. I truly don't understand his reasoning."""
94,KzH1ovd4Ots,"So you take two vectors and construct a matrix out of them.
All right, by- um, pick the- pick the ith, um, element from this vector, jth element from this vector, multiply them, and that becomes the ij element of- of- of the matrix, right? So this- this, uh, for the outer product, you don't need the vectors to have the same dimension.
And a matrix that you construct from one row vector and one column vector is also called a rank one matrix, right? Why is it called rank one? Because one way to think of it is it is made of one pair of a row and a column vector, right? So that makes it, um, um, what's, uh, what's also called as a- a rank one matrix.","I believe the explanation for a rank 1 matrix should be that the columns of the resulting matrix are simply scalar multiples of the first column, which means the dimension of the column space is 1."
95,KzH1ovd4Ots,"Uh, now, next we are going to limit ourselves to only square matrices, which means the input and output spaces have the same dimensions, and for the purpose of visualization, we're gonna only consider, let's say, a three-dimensional space.
Right? Now, in this diagram we have two different, um, um, um, two different pictures for the input space and the output space.
But now, because A is symmetric, I'm sorry, A is- is, um, a square matrix, we're gonna overlay the input and output space onto the same space, right? So here, the input space and output space are- are being overlaid here.
Now let's ask the question- let's, you know, A, you know, let's ask the question.
We saw what happened, uh, for, you know, pick, you know, choose some points, you know, run it through A, you get an output, uh, uh, output point.
Now, what happens if we take the unit sphere around the origin? By unit sphere, I mean, just to the points on the surface of the unit sphere.
Think of it as a soccer ball at the center on the origin.
And you take every point on the surface, run it through A, you get a corresponding output point for every, you know, input point of- of- of the soccer ball, right? How would the resulting shape look? Right? So that's gonna look as an ellipsoid.
It's- it's, you know, almost like an ellipse.
So that's gonna be- right? So what- what- what exactly happened here? We- it's- it's a three-dimensional input and output space.
We started with the input as- we didn't have one input, but we had a collection of points as inputs.
And that collection was precisely those points that live on the surface of a unit sphere.
And let's say we- we took this point in the input space, run it through A, we got a corresponding out point, and that point, say, this one.
Right? So every point on the input surface maps to some point on the output surface of that shape, right? We could have done this with any input shape, but, you know, sphere is easy to kind of analyze, right? Now, similarly, um, you do it for another point, um, let's say this point, and let's say that maps here, and let's say we pick- what color do we have? Green- green, and let's say we pick this point and that maps here.
Okay? Now, we saw what- what- what- what happens when you- when we, uh, take some shape, for example, a sphere and run it through A, instead of thinking of um, um, running a point through A.",Great lecture. I have one question about the last part: why do we get only an ellipsoid output for a sphere-shaped input and not any other shape? Could this be related to the assumption that matrix A is symmetric?
96,L5uBeAGJV1k,"And if you want to skip the short discussion of the meta theorems, that's fine, because it's never going to come up again in this class.
So let's look at this phrase in English, where the poet says, ""all that glitters is not gold."" Well, a literal translation of that would be that, if we let G be glitters, and I can't use G again, so we'll say Au is gold, then this translated literally would say for every x, G of x, if x is gold implies that not gold of x.
So is that a sensible translation? Well, it's clearly false, because gold glitters like gold.
And you can't say that gold is not gold.
So this is not what's meant.
It's not a good translation.
It doesn't make sense.
Well, what is meant, well, when the poet says, ""all that glitters is not gold,"" he's really leaving out a key word to be understood from context.
All that glitters is not necessarily gold.
He was using poetic license.
You're supposed to fill in and understand its meaning.
And the proper translation would be that it is not true that everything that glitters is gold.
It is not the case that for all x, if x glitters, then x is gold.
So it's just an example where a literal translation without thinking about what the sentence means and what the poet who articulated this sentence intended will get you something that's nonsense.
It's one of the problems with machine translation from natural language into precise formal language.","Is it equivalent to say, ""There exists an x such that G(x) and not Au(x)""? I believe the logic is essentially the same, but I wonder if there's a subtle difference that might preclude the possibility of an empty set."
97,MEz1J9wY2iM,"And so we're going to have two phases here in this particular approximation scheme.
The first phase is find an optimal partition, A prime, B prime, of S1 through Sm.
And we're just going to assume that this exhaustive search, which looks at all possible subsets, and picks the best one.
OK? And how many subsets are there for a set of size m? It's 2 raised to m.
So this is going to be an exponential order, 2 raised to m algorithm.
OK? I'm just going to find the optimum partition through exhaustive search for m.
Right? m is less than n.
So I'm picking something that's a smaller problem.
I'm going to seed this.
So, the way this scheme works is, I'm seeding my actual algorithm-- my actual heuristic-- with an initial partial solution.","It seems that the professor made an error while explaining the approximate partition. Initially, during the algorithm's construction, he stated that m < n, but later, during the proof, he suggested imagining that the second part never executed due to m being large. However, the only scenario where m would not execute is if m equals n, because if m does not equal n, there would still be remaining elements that have not been allocated to any partition."
98,MEz1J9wY2iM,"I'm constantly taking stuff away.
When xk equals 0, I'm going to be done.
OK? And I'm going to move a little bit between discrete and continuous here.
It's all going to be fine.
But what I have is, if I just take that, I can turn this.
This is a recurrence.
I want to turn that into a series.
So I can say something like 1 minus 1 over t, raised to k, times n.
And this is the cardinality of x, which is the cardinality of x0.
So that's what I have up there.
And that's essentially what happens.
I constantly shrink as I go along.
And I have a constant rate of shrinkage here.
Which is the conservative part of it.
So keep that in mind.
But it doesn't matter from an analysis standpoint.
OK? So if you look at that, and you say, what happens here? Well, I can just say that this is less than or equal to e raised to minus-- you knew you were going to get an e, because you saw a natural algorithm here, right? And so, that's what we got.
And basically, that's it.
You can do a little bit of algebra.
I'll just write this out for you.
But I won't really explain it.
You're going to have Xk equals 0.
You're done.
The cost, of course, is k.
Right? The cost is k, because you've selected k subsets.
All right, so that's your cost, all right? So, when you get to the point, you're done.
And the cost is k.
So what you need is, you need to say that e raised to minus kt divided by n is strictly less than 1.
Because that is effectively when you have strictly less than 1 element.
It's discrete.
So that means you have zero elements left to cover.
That means you're done OK? So that's your condition for stopping.
So this done means that e raised to minus kt times n is strictly less than 1.",How do you derive that (1 - 1/t)^k is less than or equal to e^(-k/t)?
99,MH4yvtgAR-4,"Basically here is the prediction of the label for a- for a- uh, for a given color whether it's toxic or not, this is whether it is truly toxic or not, um, and then the way you can think of this is y takes value one if it's toxic, and zero if it's not.
If the true value is zero, then this term is going to survive and it's basically one minus the lock predicted- prob- uh, one minus the predicted probability.
So here we want this to be the predicted probability- to be as small as possible so that one minus it becomes close to one because log of 1 is 0, so that this discrepancy is small.
And if the- if the, uh, class value is one, then this term is going to survive because 1 plus 1- 1 minus 1 is 0 and this- this goes away.
So here we want this term to be as close to one as possible.
Which again would say, if it's- uh, if it's toxic, we want the probability to be high.
If it's not toxic, we want the probability [NOISE] to be low- uh, the predicted probability, uh, of it being toxic.
And this is the cross, uh, entropy loss.
So this is the encoded input coming from node embeddings.
Uh, these are the classification rates, uh, for the final classification.
Uh and these are the, uh, node labels, is it basically toxic, uh, or not.","""Shouldn't the loss function have a minus sign?"""
100,Mi8wnYc1m04,"Yeah, this over here should be small y.
Thank you.
Yeah, so you, um, um, so now we have, uh, something called the law of total expectation, right? So the law of total expectation tells us that, expectation of X can be written as the expectation of- expectation of X given Y, right? Now, this holds true for any X and any Y, right? Y could be completely independent of X, it could be dependent on X.
But the expectation of X can always be decomposed into this nested form where, um, you condition on Y, you get a new random variable, right? And you take the expectation of- of this random variable and you get back the expectation of X.",Is it true that E[X|Y] might not follow a normal distribution?
101,MjbuarJ7SE0,"So I'm going to just temporarily hop out of the scope and see is there variable x outside of me? And it'll find this variable x here, and it's going to print out its values.
So that's OK.
This last example here is actually not allowed in Python-- similar to this one-- except that I'm trying to increment a value of x, but then I'm also trying to reassign it to the same value of x.
The problem with that is I never actually initialized x inside h.
So if I said-- if inside h, I said x is equal to 1, and then I did x plus equals to 1, then it would be this example here-- f of y.
But I didn't do that.
I just tried to access x and then incremented and then tried to reassign it.
And that's actually not allowed in Python.
There is a way around it using global variables.
But it's actually frowned upon to use global variables, though global variables are part of the readings for this lecture.
And the reason why it's not a great idea to use global variables is because global variables sort of give you this loophole around scopes, so it allows you to write code that can become very messy.","In the last example, where x is not defined within function g(x), it should result in an error (UnboundLocalError: local variable 'x' referenced before assignment), shouldn't it? This is similar to the example shown previously."
102,MjbuarJ7SE0,"So h returns None.
Back to whoever called it, which was this code inside g.
So that gets replaced with None-- the thing that I've-- this circled red h here.
As soon as h returns, we're going to get rid of that scope-- all the variables created within it-- and we're done with h.
So now we're back into g.
And we just finished executing this and this got replaced with None.
We're not printing it out, so this doesn't show up anywhere; it's just there.
So we're finished with that line.
And the next line is return x.
So x inside g is 4, so 4 gets returned back to whoever called it, which was in the global scope here.
So this gets replaced with 4.
So once we've returned x, we've completely exited out of the scope of g, and we've come back to whoever called us, which was global scope and we've replaced z is equal to g of x and that completely got replaced with 4-- the returned value.
So that's sort of showing nested functions.
All right just circling back to decomposition-abstraction.
This is the last slide.
You can see if you look at the code associated with today's lecture, there are some other examples where you can see just how powerful it is to use functions.
And you can write really clean and simple code if you define your own functions and then just use them later.","If the function h() had included a return statement at the end, would the outer function have returned 'abc' for x instead of 4?"
103,MjbuarJ7SE0,"So using global variables, you can be inside a function and then modify a variable that's defined outside of your function.
And that sort of defeats the purpose of functions and using them in writing these coherent modules that are separate.
That said, it might sometimes be useful to use global variables, as you'll see in a couple lectures from now.
OK cool.
So let's go on to the last scope example.
OK this slide is here, and notice I've bolded, underlined, and italicized the Python Tutor, because I find it extremely helpful.
So the Python Tutor-- as I've mentioned in one of the assignments-- it was actually developed by a grad student here, or post-grad student slash post-doc here.",What is the difference between incrementing x and printing x if they are both defined in the same place relative to g() and h()?
104,N2lwsB1qfJw,"And I ask the question, how close is y hat_ i to y_i? And the performance metric is going to be the measure of how close y hat_i is to y_i.
And normally it's designed so that the smaller the metric, the better the prediction performance.
It's an error measure.
Some people call it prediction performance metric, the prediction error as a result.
So there are a few which are very commonly used.
The first one is the mean square error.
So this here, uh, is the two-norm.
Let me just mark it here.","Hello everyone, in the presentation, various prediction metrics were demonstrated. However, I'm confused by a particular notation (this might be a simple question): when the professor mentioned y_i or y_i, what does the subscript 'i' represent? It appears to be an index symbol, which I thought should be placed at the lower right of the 'y'. However, it was shown in the upper right position. I would greatly appreciate it if anyone could provide an answer. Thank you."
105,Nsc0Yluf2yc,"And the way we do that as I said is simply by taking the dark green representations of the output layer here and using them as inputs to subsequent blocks so they get attended to, and we proceed with the subsequent regularization and feed-forward steps just as before.
And when you work with these models in Hugging Face, if you ask for all of the hidden states, what you're getting is a grid of representations corresponding to these output blocks in green here.
And of course, just as a reminder I'm not indicating it here but there is actually multi-headed attention of each one of these blocks through each one of the layers.
So there are a lot of learned parameters in this model, especially if you have 12 or 24 attention heads.
At this point, I'm hoping that you can now fruitfully return to the original Vaswani, et al paper and look at their model diagram and get more out of it.
For me, it's kind of hyper compressed but now that we've done a deep dive into all the pieces, I think this serves as a kind of useful shorthand for how all the pieces fit together.","Is there a mistake in the statement ""You have 12 or 24 attention heads""? Shouldn't it be 12 or 24 layers, each with a number of attention heads equal to the length of tokens in the input/output sequence? Also, this lecture series is exceptionally well done! We are likely to develop our own NLU course at our university using these materials. This is an invaluable contribution to the upcoming generation of data scientists specializing in natural language!"
106,Nu8YGneFCWE,"I want to generalize this to be a family of hash functions, which are this habk for some random choice of a, b in this larger range.
All right, this is a lot of notation here.
Essentially what this is saying is, I have a has family.
It's parameterized by the length of my hash function and some fixed large random prime that's bigger than u.
I'm going to pick some large prime number, and that's going to be fixed when I make the hash table.
And then, when I instantiate the hash table, I'm going to choose randomly one of these things by choosing a random a and a random b from this range.
Does that makes sense? AUDIENCE: [INAUDIBLE] JASON KU: This is a not equal to 0.
If I had 0 here, I lose the key information, and that's no good.",How do we determine the key value (k) for the universal hash function hash(k) = (((ak + b) mod p) mod m)? Can someone provide an explanation?
107,Nu8YGneFCWE,"So basically, if I fix this location i, this is where this key goes.
Sorry.
This is the size of chain at h of Ki.
Sorry.
So I look at wherever Ki goes is hashed, and I see how many things collide with it.
I'm just summing over all of these things, because this is 1 if there's a collision and 0 if there's not.
Does that make sense? So this is the size of the chain at the index location mapped to by Ki.
So here's where your probability comes in.
What's the expected value of this chain length over my random choice? Expected value of choosing a hash function from this universal hash family of this chain length-- I can put in my definition here.
That's the expected value of the summation over j of xij.
What do I know about expectations and summations? If these variables are independent from each other-- AUDIENCE: [INAUDIBLE] JASON KU: Say what? AUDIENCE: [INAUDIBLE] JASON KU: Linearity of expectation-- basically, the expectation sum of these independent random variables is the same as the summation of their expectations.","""As I learned from Prof. Leighton in 6.042J in 2010, the Linearity of Expectation holds regardless of the property of mutual independence. Please correct me if I have misunderstood."""
108,Nu8YGneFCWE,"So this is equal to the summation over j of the expectations of these individual ones.
One of these j's is the same as i.
j loops over all of the things from 0 to u minus 1.
One of them is i, so when xhi is hj, what is the expected value that they collide? 1-- so I'm going to refactor this as being this, where j does not equal i, plus 1.
Are people OK with that? Because if i equals-- if j and i are equal, they definitely collide.
They're the same key.
So I'm expected to have one guy there, which was the original key, xi.
But otherwise, we can use this universal property that says, if they're not equal and they collide-- which is exactly this case-- the probability that that happens is 1/m.
And since it's an indicator random variable, the expectation is there are outcomes times their probabilities-- so 1 times that probability plus 0 times 1 minus that probability, which is just 1/m.
So now we get the summation of 1/m for j not equal to i plus 1.
Oh, and this-- sorry.
I did this wrong.","Revised Sentence:
""Jason asserts that the chain length remains constant if m is of an order at least as large as n. However, wouldn't 1+((n-1)/m) vary between 1 and 2, which implies it is not constant? For instance, if both m and n are very large, then 1+((n-1)/m) would approximate 2. Conversely, if m is very large and n is very small, such as m=10,000,000 and n=1, then 1+((n-1)/m) would be approximately 1. My limited understanding of probability might be the reason why this is confusing to me. Any clarification would be appreciated.""
""Shouldn't that be 'less than or equal to' since the probability of 1/m is an upper bound for the likelihood of two keys having the same hash value?"""
109,Nu8YGneFCWE,"So what I would need to do-- and if I was storing your keys as MIT IDs, I would need an array that has indices that span the tire space of nine-digit numbers.
That's like 10 to the-- 10 to the 9.
Thank you.
10 to the 9 is the size of a direct access road off to build to be able to use this technique to create a direct access array to search on your MIT IDs, when there's only really 300 of you in here.
So 300 or 400 is an n that's much smaller than the size of the numbers that I'm trying to store.
What I'm going to use as a variable to talk about the size of the numbers I'm storing-- I'm going to say u is the maximum size of any number that I'm storing.","Great video! Thank you, MIT OCW :) How is it 10^9? Can someone please explain?"
110,Nu8YGneFCWE,"This isn't u.
This is n.
We're storing n keys.
OK, so now I'm looping over j-- this over all of those things.
How many things are there? n minus 1 things, right? So this should equal 1 plus n minus 1 over m.
So that's what universality gives us.
So as long as we choose m to be larger than n, or at least linear in n, then we're expected to have our chain lengths be constant, because this thing becomes a constant if m is at least order n.","Shouldn't it be n-2 instead of n-1, since one of the n-1 elements is not counted when j=i? Can anyone explain this? Thank you!"
111,OgO1gpXSUzU,"A pretty small number.
But the probability of 26 consecutive reds when the previous 25 rolls were red is what? No, that.
AUDIENCE: Oh, I thought you meant it had been 26 times again.
JOHN GUTTAG: No, if you had 25 reds and then you spun the wheel once more, the probability of it having 26 reds is now 0.5, because these are independent events.
Unless of course the wheel is rigged, and we're assuming it's not.
People have a hard time accepting this, and I know it seems funny.
But I guarantee there will be some point in the next month or so when you will find yourself thinking this way, that something has to even out.","The slide presents a good opportunity to introduce probability notation because the way the second sentence is phrased in English is quite misleading. The first sentence represents the probability of 26 consecutive reds, denoted as P(26 consecutive reds). The second sentence should represent the probability of 26 consecutive reds given that the first 25 are red, denoted as P(26 consecutive reds | the first 25 are red). However, the second sentence, as it is currently written, is grammatically incorrect. What the professor intends to convey is the ""Probability of the 26th roll being red, given that the preceding 25 rolls were red."" This clarification significantly aids in understanding that the rolls are independent and not correlated. The slide's wording, as it stands, erroneously suggests that there might be a total of 26 plus 25 rolls.
I understand the concept of independence (I think, I hope), but it hinges on the assumption that the wheel is not rigged. However, is this assumption still justified after 25 consecutive reds? Probably, since such events do occur (and they remain in our memories for decades), but it raises the question: at what point, after how many consecutive reds, should we start to become suspicious?"
112,OgO1gpXSUzU,"All right, so we've got one flip, and it came up heads.
And now I can ask you the question-- if I were to flip the same coin an infinite number of times, how confident would you be about answering that all infinite flips would be heads? Or even if I were to flip it once more, how confident would you be that the next flip would be heads? And the answer is not very.
Well, suppose I flip the coin twice, and both times it came up heads.
And I'll ask you the same question-- do you think that the next flip is likely to be heads? Well, maybe you would be more inclined to say yes and having only seen one flip, but you wouldn't really jump to say, sure.
On the other hand, if I flipped it 100 times and all 100 flips came up heads, well, you might be suspicious that my coin only has a head on both sides, for example.
Or is weighted in some funny way that it mostly comes up heads.
And so a lot of people, maybe even me, if you said, I flipped it 100 times and it came up heads.
What do you think the next one will be? My best guess would be probably heads.
How about this one? So here I've simulated 100 flips, and we have 50 heads here, two heads here, And 48 tails.
And now if I said, do you think that the probability of the next flip coming up heads-- is it 52 out of 100? Well, if you had to guess, that should be the guess you make.
Based upon the available evidence, that's the best guess you should probably make.
You have no reason to believe it's a fair coin.","He misses the implications of Bayes' theorem: observing 52 heads from 100 flips still makes it much more likely that the coin is fair rather than biased. As he mentions, there are significantly more fair coins and dice out there than weighted ones. The probability you have to assess is P(52 heads | coin is fair) * P(coin is fair) versus P(52 heads | coin is biased) * P(coin is biased). It is far more likely that the coin is fair."
113,OgO1gpXSUzU,"And that gets us to what's in some sense the fundamental question of all computational statistics, is how many samples do we need to look at before we can have real, justifiable confidence in our answer? As we've just seen-- not just, a few minutes ago-- with the coins, our intuition tells us that it depends upon the variability in the underlying possibilities.
So let's look at that more carefully.
We have to look at the variation in the data.
So let's look at first something called variance.
So this is variance of x.
Think of x as just a list of data examples, data items.
And the variance is we first compute the average of value, that's mu.
So mu is for the mean.
For each little x and big X, we compare the difference of that and the mean.","What does the symbol |X| represent, and is it the same as the symbol n?"
114,OgO1gpXSUzU,"But you wouldn't bet that it would have fewer than 5.
Because of this, if you now look at the average of the 20 spins, it will be closer to the mean of 50% reds than you got from the extreme first spins.
So that's why it's called regression to the mean.
The more samples you take, the more likely you'll get to the mean.
Yes? AUDIENCE: So, roulette wheel spins are supposed to be independent.
JOHN GUTTAG: Yes.
AUDIENCE: So it seems like the second 10-- JOHN GUTTAG: Pardon? AUDIENCE: It seems like the second 10 times that you spin it.
Like that shouldn't have to [INAUDIBLE]..
JOHN GUTTAG: Has nothing to do with the first one.
AUDIENCE: But you said it's likely [INAUDIBLE]..
JOHN GUTTAG: Right, because you have an extreme event, which was unlikely.
And now if you have another event, it's likely to be closer to the average than the extreme was to the average.
Precisely because it is independent.
That makes sense to everybody? Yeah? AUDIENCE: Isn't that the same as the gambler's fallacy, then? By saying that, because this was super unlikely, the next one [INAUDIBLE].
JOHN GUTTAG: No, the gambler's fallacy here-- and it's a good question, and indeed people often do get these things confused.
The gambler's fallacy would say that the second 10 spins would-- we would expect to have fewer than 5 reds, because you're trying to even out the unusual number of reds in the first Spin Whereas here we're not saying we would have fewer than 5.","What is he throwing, and why? I already noticed this in the previous video. What is it about?"
115,OgO1gpXSUzU,"The next time they come to the plate, the idiot announcer says, well he struck out six times in a row.
He's due for a hit this time, because he's usually a pretty good hitter.
Well that's nonsense.
It says, people somehow believe that if deviations from expected occur, they'll be evened out in the future.
And we'll see something similar to this that is true, but this is not true.
And there is a great story about it.
This is told in a book by Huff and Geis.
And this truly happened in Monte Carlo, with Roulette.
And you could either bet on black or red.
Black came up 26 times in a row.
Highly unlikely, right? 2 to the 26th is a giant number.
And what happened is, word got out on the casino floor that black had kept coming up way too often.
And people more or less panicked to rush to the table to bet on red, saying, well it can't keep coming up black.
Surely the next one will be red.
And as it happened when the casino totaled up its winnings, it was a record night for the casino.
Millions of francs got bet, because people were sure it would have to even out.
Well if we think about it, probability of 26 consecutive reds is that.","I don't understand why this isn't the case: isn't the primary purpose of the ""Law of Large Numbers"" to ensure a ""regression to the mean""? Take the casino example where ""black"" came up 26 times in a row; it seems likely that the ""regression to the mean"" principle would start to address this ""anomaly,"" doesn't it? Therefore, in the next 26 spins, we might expect a slight biasnot an even 13/13 split between red and black, but perhaps something like 15/11, slightly correcting the anomaly, right? Or maybe even 14/12, correct? Otherwise, there would be no regression to the mean. If that's the case, then switching to betting on red would be the right move, wouldn't it? Or have I completely misunderstood the ""regression to the mean"" concept?"
116,OgO1gpXSUzU,"Today we would think of a few microseconds, but those machines were slow.
Hence was born Monte Carlo simulation, and then they actually used it in the design of the hydrogen bomb.
So it turned out to be not just useful for cards.
So what is Monte Carlo simulation? It's a method of estimating the values of an unknown quantity using what is called inferential statistics.
And we've been using inferential statistics for the last several lectures.
The key concepts-- and I want to be careful about these things will be coming back to them-- are the population.
So think of the population as the universe of possible examples.
So in the case of solitaire, it's a universe of all possible games of solitaire that you could possibly play.","This individual committed numerous errors, one of which is evident early on when he attempts to define the Monte Carlo simulation. He incorrectly refers to an ""unknown quantity,"" but consider the scenario where I aim to simulate a known quantity, perhaps to verify my programming skills. I could have a mathematically adept person calculate the exact answer using traditional methods, and then use a computer to simulate it for an approximation. By his definition, this wouldn't constitute a Monte Carlo simulation because the value is known. Extending this idea, imagine I simulate a quantity I believe to be unknown, yet, unknown to me, someone has already determined the precise answer. His definition would suggest this isn't Monte Carlo simulation either. The definition fails to clarify to whom the quantity must be unknown, rendering it ambiguous. A quick internet search for Monte Carlo simulation yields far superior definitions than the inadequate one provided in this lecture.

Another significant issue with his definition is the claim that a random sample will typically exhibit the same properties as the population from which it is drawn. This is not necessarily true, especially if the population includes an extremely rare event, such as one in one quintillion. For instance, if two individuals each use a computer to simulate one trillion random samples, they are still six orders of magnitude short of the full population size. It is highly probable that both will record zero occurrences of the rare event, leading to the incorrect conclusion that no such events exist. Even if they repeated the simulation ten times, they would still be five orders of magnitude short of the population size, which equates to a sampling rate of two in one hundred thousand, insufficient for reliable results. While it's true that zero is close to the expected value, the issue is that recording zero occurrences provides no information about the existence of the rare event we seek to precisely quantify."
117,OgO1gpXSUzU,"We're saying we'd probably have fewer than 10.
That it'll be closer to the mean, not that it would be below the mean.
Whereas the gambler's fallacy would say it should be below that mean to quote, even out, the first 10.
Does that makes sense? OK, great questions.
Thank you.
All right, now you may not know this, but casinos are not in the business of being fair.
And the way they don't do that is in Europe, they're not all red and black.
They sneak in one green.
And so now if you bet red, well sometimes it isn't always red or black.
And furthermore, there is this 0.","However, if we begin counting from the start of the series, once we encounter five consecutive blacks, the subsequent black would extend the sequence from five to six, which is more significant. Is it not possible to consider it in this manner?"
118,OgO1gpXSUzU,"What does this mean? If I were to conduct an infinite number of trials of 10,000 bets each, my expected average return would indeed be minus 3.3%, and it would be between these values 95% of the time.
I've just subtracted and added this 3.5, saying nothing about what would happen in the other 5% of the time.
How far away I might be from this, this is totally silent on that subject.
Yes? AUDIENCE: I think you want 0.2 not 9.2.
JOHN GUTTAG: Oh, let's see.
Yep, I do.
Thank you.
We'll fix it on the spot.
This is why you have to come to lecture rather than just reading the slides, because I make mistakes.
Thank you, Eric.
All right, so it's telling me that, and that's all it means.
And it's amazing how often people don't quite know what this means.
For example, when they look at a political pole and they see how many votes somebody is expected to get.","Thank you for the insightful lecture. I have one question regarding a point you mentioned: ""The return on betting a pocket 10,000 times in European roulette is -3.3%."" Was this figure derived from a Monte Carlo simulation? I'm curious because a European roulette wheel has 37 pockets. Winning yields a 35 to 1 payout, plus the return of the original bet, totaling 36 units for a win. The probability of winning is 1/37, which is approximately 0.0270, leading to an expected return of -2.7%, or a 97.3% payout rate for European roulette, depending on the perspective. Thank you once again for the valuable information."
119,P3YoIxiz6to,"So in general, we have some problem A-- decision problem A and parameter k.
And we want to convert into some decision problem B with parameter k prime.
And of course, as usual, the set up is we're given an instance x.
This is going to look almost identical to NP Karp-style reductions, but then we're going to have one extra condition.
So instance x of A gets mapped to by a function f to an instance x prime of B.
X prime is f o x as usual.
This needs to be a polynomial time function just like for NP reductions, which means, in particular, x prime has polynomials size reduced back to x.
It should be answer preserving.
So x is yes instance for A if and only x prime is a yes instance for B.
So, so far, exactly NP reductions.
And then when we need one extra thing which is parameter preserving.
This is there's some function g, which I'll call the parameter blow up-- or, I guess you call it parameter growth for g-- such that the new parameter value for the converted instance is, at most, that function of the original parameter value of the original instance.
Question? AUDIENCE: Are there any limits on the amount of time that g can take to compute? PROFESSOR: g should be a computable function.
I think that's all we need.
Probably polynomial time is also a fine, but usually this is going to be like linear or polynomial or exponential.
It's rarely some insanely large thing, but computable would be nice.
Cool.
So that is our notion of parameterized reduction.
And the consequence, if this exists and B is fixed parameter tractable, then A is because we can take an instance of A converting it to B.
If the original parameter was founded by some k, this new parameter will be bounded by g of k.
New instance will be bounded of g of k.
So we run the FPT algorithm for B, and that gives us the answer to the original instance of A.
So if we don't care about what this function is, we are basically composing functions.
So there's some f dependence on k in this algorithm, and we're taking that function of g of k is our new function.
And we get a new dependence on k and the running time over FPT.
So what that means is if we believe it A does not have an FPT, then B does not have an FPT if we can do these reductions.","""At around the time you provide the definition of a parameterized reduction, the classic definition typically asserts that a reduction is computable by an FPT algorithm (as seen, for example, in Flum & Grohe). Why do you choose to offer this more liberal definition of an FPT reduction?"""
120,PNKj529yY5c,"A small detail, not a particularly important one.
Now where are we.
We've got that guy there.
We've got our complete architecture.
We've got our solved problem.
And now we can start reflecting on what we've done.
We can say, for example, how good an integration program is this? And the answer is, it was pretty good.
This machine that Slagle was using was a machine that was over in building 26.
And we were so proud of it, that it was behind glass, and you could go there and watch the tape spin, it was really a delight.
32k of memory, that's 32k of memory.
It's amazing that he was able to do anything with a machine of that size.",How did he determine the depth of functional composition?
121,PNKj529yY5c,"All good, I see some confused, worried, concerned looks.
Maybe I've made a mistake, perhaps I should use notes.
Well no, wait a minute.
For those of you who have a concerned look, remember that if x equals a sine y, then dx is equal to cosine y dy.
That's why it's cosine to the fourth not cosine to the fifth, as you were perhaps thinking it might be.
So now we've made some progress.
We look at this, we say, are there any safe transformations that apply? And the answer is, no.
Now we look for a heuristic transformation that might apply, and I say, what do you see? Which one? What's that? AUDIENCE: [INAUDIBLE].
SPEAKER 1: She said something unintelligible, but what she probably said is, that this looks like a pattern that might match with the heuristic transformation A, right? Because we have a function in which the variable is buried, universally in sines, or cosines, or tangents, or cotangents, or secants, or cosecants.
And we know we can rewrite that in one of three ways.
It's already written as a function of sine and cosine.
But we can also rewrite that in terms of tangent and cosecant.
Or cotangent and secant.
So when we do that, we can go this way, and we can get the integral of 1 over the cotangent of x dx.","""Hey, could someone explain to me why dx equals cos y? I would appreciate it greatly. I can't find an explanation online. A helpful link would also be appreciated. Thank you."""
122,PNKj529yY5c,"All right so that's progress, maybe.
But don't see this in any of the heuristic transformations, what do I do now? I didn't have to look in the heuristic transformations, because one of the safe transformations applies.
Because this thing is a rational function and the degree of the numerator is greater that the degree of the denominator, so I have to divide.
And when I divide, and that by the way is number four, I get what? Is anybody good high school algebra that can help me out with that? AUDIENCE: Y squared minus 2 plus negative 2 over 1 plus y squared SPEAKER 1: Exactly, y squared minus 1 plus 1 over 1 plus y squared, I think.",Can anyone tell me how he got that division result?
123,PNKj529yY5c,"But once we get a name for it, we'll get power over it.
And then we'll be able to deploy it, and it will become a skill.
We'll not just witness it, we'll not just understand it, we'll use it instinctively, as a skill.
So there you are, you've got that problem, there's your problem, and what do you do to solve it? I don't know, look it up in a table? You'll never find it in a table because of that minus sign and that 5.
So you're going to have to do something better than that.
So what you're going to do, is what you always do when you see a problem like that.
You try to apply a transform, and make it into a different problem that's easier to solve.
And eventually, what you hope is that you'll simplify it sufficiently, that the pieces that you've simplified to will be found in some small table of integrals.
So how long is this table? It's not the case that we're going to look at a table with 388 elements, because this is not a big table of integrals.
This is what a freshman might have in a freshman's head, after taking a course in integral calculus.
One of the interesting questions is, how many elements have to be in that table to get an A in the course? We're interested in how much knowledge is involved, that's one of the elements of catechism that I've listed over there, that will be part of the gold star ideas suite of the day.
So we'd like to take that problem, and find a way to make it into another problem that's more likely, or closer to being found in the table.
So what we're going to do is very simple, graphically.
We're going to take the problem we're given, and convert it into another problem that's simpler.
And we're going to give that process and name, and we're going to call it problem reduction.
And so, in the world of integral calculus, there are all sorts of simple methods, simple transformations, we can try that will take a hard problem and make it into an easier problem.
And some of these transformations are extremely simple and always safe.
Some of them are just, well let's try it and see what happens.
But some of them are safe, and I'd like to make a short list of safe transformations right now.
Now I'm going to be going into some detail.","""What is problem reduction?"""
124,PNKj529yY5c,"Now what? Now we're really getting close to getting through this, because that is a sum.
And by virtue of the fact that it's a sum, that divides into three pieces, and the top piece is the integral of y squared, the middle piece is the integral of minus 1, and the bottom piece is the integral of 1 over 1 plus y squared dy in all cases.
Gosh, if I look this up, I've found it.
That's up there, that's letter B.
So I'm done with that.
This one I can transform again, by virtue of 1, and now I get the integral dy.
That's in there, that's B as well.
As this one, I don't know.
But I'd better keep track of what I'm doing here.
This is in the and node, so I've got to do all of those.
I can't give up on that last thing.
And that and transformation is transformation number 3.
So this is in the table, this is in the table, we still have this to do, but that's C, heuristic transformation C.
We have 1, plus y squared, then with the transformation C, with y-- this is y squared-- y equals tangent of z And then we get to the integral of dz and that's in the table and, we're done.","""How did he arrive at dz from the integral of 1/(1+y^2) dy?"""
125,PNKj529yY5c,"Or we can rewrite that as a function of tangent of x, and cosecant of x.
Or we can rewrite that as function of cotangent of x, and the secant of x.
So that's a transmission from trigonometric form, into another trigonometric form.
It's not always a good idea, sometimes it helps.
Well that's just part one of our suite of heuristic transformations.
Stop.
There are others that we need to have in our repertoire, in order to solve the problem.
One of them is a family of transformations, which I'll show you only one.
It goes like this, if you have the integral of a function, of the tangent of x, then you can rewrite that as the integral of a function of y over 1 plus y squared dy.
So that's a transformation from a trigonometric form into a polynomial form.
So it gets rid of all that trigonometric garbage we don't want to deal with.
And there's a whole family of things like that, just as there's a family of transformations like so, but this is enough to give you flavor.
Now there's a C that we need as well.
And that's going to be your proper knee-jerk reaction when you see something of the form 1 minus x squared.","""Where do the heuristic transformations that come after originate from? What is this?"""
126,PNKj529yY5c,"We have no transformation that can take us further, so we need something else.
And what we need by way of something else, is some transformations that we will describe as-- perhaps we'll call them, heuristic transformations.
A funny word, meaning a method that often works isn't guaranteed to work.
It's not an algorithm in the usual sense that we talk about algorithms.
But rather, it's an attempt.
So these things I'm going to talk about now, are sometimes useful, not always useful.
Sometimes take you into a blind alley, don' always work.
But you can't get an A in calculus without knowing some of them.
So you said, some kind of trig substitution.
So here is some kind of trig substitution.
We'll call this heuristic transformation A.
You have a function sine x, cosine x, tangent of x, cotangent of x, secant of x, and cosecant of x.
And we all know from high school trigonometry, that we can rewrite that as a function of sine x, and cosine x.","""Where can I find materials on the transformations mentioned earlier? I have taken calculus classes, but I have never encountered anything like that before and would like to learn more."""
127,QOtA76ga_fY,"One is that they assumed that encryption also provides authentication or integrity of messages.
So don't do that.
And Kerberos version 5 fixes this by explicitly authenticating messages.
Another thing they sort of had a problem with is the ability for arbitrary clients to guess people's passwords.
So any suggestions of how we could fix this? How do you prevent guessing attacks in a protocol like this? What could we try? Yeah.
STUDENT: Some sort of salting? I'm not sure.
PROFESSOR: Well, so salting would just means that the client has to hash the password in different ways, maybe.
But it still doesn't prevent them from trying lots of things.
So maybe it'll be more expensive to build a dictionary.","How can one discover the password using brute force, as the teacher inquired at the last minute? The password is hashed with Kc. However, the attacker is unaware of Kc and only has access to the encrypted message: {{Tc,s}Ks, Kc,s}Kc. Although the attacker can attempt to apply brute force to the encrypted message: {{Tc,s}Ks, Kc,s}Kc, they have no knowledge of the true value of {{Tc,s}Ks, Kc,s}Kc. They could try every possible Kc, including the correct key, but they would not be able to determine whether the result is accurate."
128,QUO-HQ44EDc,"So it means using the training data, we are going to train two classifiers.
For example, using a linear classifier, a neural network, or support vector machine or a decision tree that given a feature vector of a given node, it's going to predict its label.
And in phase 1, we are going to apply this classifier Phi_1, so that now every node in the network has some predicted, uh, label.
And then using the training data, we are also going to train this classifier Phi_2, that is going to use two inputs.
It's going to use the feature of the node of interest, as well as this summary vector z that captures or summarizes the labels of the nodes, um, in it's network neighborhood.
And this Phi_2 is going to predict the label of the node of interest v based on its features, as well as the summary z of its, uh- of the labels of its neighbors.
And then- right, so we have first applied Phi_1 then we also have trained Phi_2 so that now we will go into phase 2 where we are going to iterate and we are going to iterate until convergence, uh, the following- uh, the following, uh, iteration where on the, uh, test set, which means on the unlabeled nodes.
Uh, we are going to use this, uh, first, the classifier Phi_1 to assign initial, uh, labels.
We are going to compute the label summaries z, and then we are going to predict the labels with the second classifier, Phi_2.
And we are going now to repeat this process until it converges in a sense that we are going to update the vector z.","Thank you for the insightful lecture. I have a question regarding the Iterative classifier, specifically in Phase 1, the training phase. We train two classifiers, \(\phi_1\) and \(\phi_2\). For the label summary vector \(z_v\) that is required for classifier \(\phi_2\), does it originate from the output of \(\phi_1\)? Or are the classifiers trained independently, as an earlier comment seems to imply? My concern is that if they are trained separately, the error from \(\phi_1\) will compound with the error from \(\phi_2\). This suggests that \(\phi_2\) may not be trained to be sufficiently robust against the potential noise in \(z_v\) that is absent during training but could be present during testing due to errors from \(\phi_1\)."
129,RN8qpSs8ozY,"Um, in online settings if you don't know that, you can also constantly be updating it with a time step.
It's a good question.
Yeah.
How is delta decided? How is what? Pardon.
How is delta, is like what is delta? Okay.
Good question.
Um, so, question is what is delta.
What we're gonna, I did not tell you, uh, in this ca- well, in this case it's telling us, um, it's specifying what is the probability this is holding like what this inequality is holding.
Later we're gonna pro- provide a regret bound that is high probability.
So we're gonna say we're gonna have a regret bound which is something that like with probability is 1 minus a function of delta, um, your regret will be sub-linear.
So that's how.
You can get expected regret bounds too and the UCB paper which, um, one of the original UCB papers provides an expected bound but I thought it was a little, the, this bound was a little bit easier to do in class.
So I thought I would do the high probability bound.
Yeah.
So before we were talking about regret, I didn't exactly understand how you use regret to update your estimate of the action value.
Oh, good question.
Um, so question is, do we use or how would we use the regret bound, the regret to update our estimate action, we don't.
Regret is just a tool to analyze our algorithm.
Great clarification.
So regret is a way for us to analyze whether or not an algorithm is gonna be good or bad in terms of how fast the regret gro- grows but it's not used in the algorithm itself.
The algorithm doesn't compute regret.
And it's not used in terms of the updating.
Excuse me.
Okay.","Presumably, this is only true if the upper bound holds, right?
Are you referring to the initial definition of the upper bound, where U(at) > Q(at) with high probability? I realize it may sound vague and imprecise."
130,Rfkntma6ZUI,"So perhaps your GN- GCN, GNN may need to be a bit deeper, but you have reduced, uh, the number of parameters, uh, significantly, which will lead to, uh, faster training, perhaps more robust model, less overfitting, um, and so on.
So that's, uh, the technique of, uh, block diagonal matrices, uh, where basically you reduce the dimensionality of each W by assuming a block diagonal structure.
Uh, second idea how to, uh, make this, uh, RGCN more scalable and its learning more tractable is to- to build on the key insight which is we wanna share weights across different relations, right? We don't wanna consider relations as independent from each other, but we want relations to kind of share weights, uh, to also share some information.","In Basis Learning, it is unclear whether the learned scalars a_rb are relation-specific, which implies that the importance for each matrix V would be a vector rather than a scalar.
In Basis Learning, it is unclear how the basis matrices (V_b) are obtained. Are they set to be trainable, similar to the importance weight coefficients (a_rb)?"
131,RvRKT-jXvko,"So why would we want to use tuples? Tuples are actually useful in a couple of different scenarios.
So recall a few years ago, we looked at this code where we tried to swap the values of variables x and y.
And this first code actually didn't work, because you're overwriting the value for x.
So instead, what we ended up doing was creating this temporary variable where we stored the value of x, and then we overwrote it, and then we used the temporary variable.
Well, turns out this three liner code right here can actually be written in one line using tuples.
So you say x comma y is equal to y comma x.
And Python goes in and says, what's the value of y? And assigns it to x.
And then what's the value of x? And assigns it to y.
Extending on that, we can actually use tuples to return more than one value from a function.
So functions, you're only allowed to return one object.
So you're not allowed to return more than one object.",What is their use?
132,RvRKT-jXvko,"We haven't created a new object in memory.
We're just modifying the same object in memory.
And you're going to see why this is important as we look at a few side effects that can happen when you have this.
So I've said this a couple of times before, but it'll make your life a lot easier if you try to think of-- when you want to iterate through a list if you try to think about iterating through the elements directly.
It's a lot more Pythonic.
I've used that word before.
So this is sort of a common pattern that you're going to see where you're iterating over the list elements directly.
We've done it over tuples.
We've done it over strings.
So these are identical codes.
They do the exact same thing, except on the left, you're going from-- you're going through 0, 1, 2, 3, and so on.
And then you're indexing into each one of these numbers to get the element value.
Whereas on the right, this loop variable i is going to have the element value itself.
So this code on the right is a lot cleaner.
OK.
So now let's look at some operations that we can do on lists.
So there's a lot more operations that we can do on lists, because of their mutability aspect than we can do on tuples or strings, for example.",A list has not been created yet. Can someone explain how this works?
133,SE4P7IVCunE,"Because this is just like we did in the very first program, when I'm checking 0, 1, 2, 3, 4, 5, 6, 7, 8, when I'm trying to find the cube root of 8.
Once I've reached 8, I'm going to stop.
And it's the same thing here.
So I just added this little clause that says, well, while I'm greater than or equal to epsilon and I'm still less than the actual cube, just keep searching.
But once I've reached the cube, then stop searching.
And with 10,000, you can see that I failed to actually find-- so that's what this part, here, does.
It tells me I've failed to find the cube root with those particular parameters.
The last thing we're going to look at is bisection search.
And to illustrate this, I'm going to need one volunteer.
And you're going to play a game with me in front of the whole class.","Great tutorial and lesson. Just a minor point, in my humble opinion, Anna made a slight error when she included ""and guess <= cube"" in the test condition. Since epsilon represents an error margin, the accurate condition generally is ""cube - epsilon <= guess**3 <= cube + epsilon."" Thus, the test condition in the example should be ""and guess**3 <= cube + epsilon,"" or the test should be reformulated to reflect the range within ""<<"" and "">>."""
134,SE4P7IVCunE,"But it's an entirely different object that I've created here.
Again, it might not mean anything right now, but just keep this in the back of your mind, strings are immutable.
So the next thing I want to talk about is a little bit of recap on for loops.
And we're going to see how we can apply for loops, very easily, to write very nice, readable code when dealing with strings.
So remember that for loops had a loop variable.
My loop variable being this var, here, in this particular case.
It can be anything you want.
And this variable, in this particular case, iterates over this sequence of numbers, 0, 1, 2, 3, 4.
So the very first time through the loop, var has a value of 0.
It does the expressions in the loop.
As soon as they're done, var takes the value 1.
It does all the expressions in the loop.
And then var takes the value 2, and it does that all the way up until 0, 1, 2.",Is the loop variable iterating over a collection rather than a set?
135,SE4P7IVCunE,"So as long as the guess cubed minus the cube-- so how far away are we from the actual answer-- is greater than some epsilon, keep guessing, because the solution is not good enough.
But once this is less than epsilon, then we've reached a good enough solution.
So two things to note with approximate solutions.
So you can get more accurate answers if your step size is really, really small.
If you're incrementing by 0.0001, you're going to get a really good approximate solution, but your program will be a lot slower.
Same sort of idea with epsilon, you can change epsilon.
If you change epsilon to be a bigger epsilon, you're sacrificing accuracy, but you're going to reach a solution a lot faster.
So here's the code for the approximate solution of a cube root.
It might look intimidating, but, look, almost half this code is just initializing variables.
So we're initializing, this is the cube we want to find the cube root of.
We pick an epsilon of this.
We start with a guess of 0.
We start with an increment of 0.0001.
And just for fun, let's keep track of the number of guesses that it takes us to get to the answer.","Could someone assist me? It seems that the code provided is incorrect. The while loop should not execute since the condition is not met; that is, guess**3 - cube is not greater than or equal to 0.1. Instead, 0x0x0 - 27 is less than 0.1, right? Therefore, the while loop should not initiate at all, should it?"
136,SE4P7IVCunE,"So my cube is 8.
I'm going to have a for loop that says, I'm going to start from 0.
And I'm going to go all the way up to-- So I'm going to start from 0 and go all the way up to 8.
For every one of these numbers, I'm going to say, is my guess to the power of 3 equal to the cube 8? And if it is, I'm going to print out this message.
Pretty simple, however, this code is not very user friendly, right? If the user wants to find the cube root of 9, they're not going to get any output, because we never print anything in the case of the guess not being a perfect cube.
or the cube not being a perfect cube.
So we can modify the code a little bit to add two extra features.
The first is we're going to be able to deal with negative cubes, which is kind of cool.
And the second is we're going to tell the user, if the cube is not a perfect cube, hey, this cube is not a perfect cube.
So we're not going to silently just fail, because then the user has some sort of feedback on their input.
So let's step through this code.
We have, first of all, a for loop just like before.
And we're going to go through 0 to 8 in this case.
We're using the absolute value, because we might want to find the cube root of negative numbers.
First thing we're doing is doing this check here.
Instead of guessing whether the guess to the power of 3 is equal to the cube, we're going to check if it's greater or equal to, and we're going to do that for the following reason.",Why is there a +1 in the range of that for loop? Is it to make the loop continue until 8 instead of stopping at 7?
137,STjW3eH0Cik,"And there are b to d of those.
How many are there at this next level up? Well, that must be b to the d minus 1.
How many fewer nodes are there at the second to the last, the penultimate level, relative to the final level? Well, 1 over b, right? So, if I'm concerned about not getting all the way through these calculations at the d level, I can give myself an insurance policy by calculating out what the answer would be if I only went down to the d minus 1th level.
Do you get that insurance policy? Let's say the branching factor is 10, how much does that insurance policy cost me? 10% of my competition.","I have a question regarding a scenario where we might not have enough time and only manage to reach the (d-1)th level. The speaker suggests that we could maintain a temporary answer at each level as we progress downward, ensuring that we have some form of approximate answer at any given time. However, I'm puzzled about how we can arrive at any answer without reaching the leaf nodes, since it is only at these nodes that we can determine the winner of the game. Consider the game of tic-tac-toe: at the (d-1)th level, we lack sufficient information to ascertain whether the sequence of moves leading to this node will result in a win or a loss. At even higher levels, such as (d-3), the situation becomes increasingly unclear; as we delve deeper, anything seems possible. If an algorithm opts to compute only up to the (d-1)th level, then all paths appear equally viablenone guarantee a win, and none ensure a loss, because, as I understand it, wins and losses are only assessed at the leaf nodes. This is particularly evident in the pure MinMax algorithm. So, how can we possibly have an 'approximate answer' at the (d-1)th level, or even at the (d-5)th level?"
138,STjW3eH0Cik,"I'll need some help, especially from any of you who are studying cosmology.
So, we'll start with how many atoms are there in the universe? Volunteers? 10 to the-- SPEAKER 2: 10 to the 38th? SPEAKER 1: No, no, 10 to the 38th has been offered.
That's why it's way too low.
The last time I looked, it was about 10 to the 80th atoms in the universe.
The next thing I'd like to know is how many seconds are there in a year? It's a good number have memorized.
That number is approximately pi times 10 to the seventh.
So, how many nanoseconds in a second? That gives us 10 to the ninth.
At last, how many years are there in the history of the universe? SPEAKER 3: [INAUDIBLE].
14.7 billion.
SPEAKER 1: She offers something on the order of 10 billion, maybe 14 billion.
But we'll say 10 billion to make our calculation simple.
That's 10 to the 10th years.
If we will add that up, 80, 90, plus 16, that's 10 to the 106th nanoseconds in the history of the universe.
Multiply it times the number of atoms in the universe.
So, if all of the atoms in the universe were doing static evaluations at nanosecond speeds since the beginning of the Big Bang, we'd still be 14 orders of magnitudes short.","Question: Prof. Winston states that there are approximately  * 10^7 seconds in a year. Can anyone explain the origin of the  in this calculation? Does it account for leap years in some way? If not, shouldn't the calculation be 60 seconds/minute x 60 minutes/hour x 24 hours/day x 365 days/year, which equals 31,536,000, a figure that is close to  * 10^7 but not exactly the same? Thank you."
139,STjW3eH0Cik,"So, let's carry on and see if we can complete this equal to or less than 8, equal to 8, equal to 8-- because the other branch doesn't even exist-- equal to or less than 8.
And we compare these two numbers, do we keep going? Yes, we keep going.
Because maybe the maximizer can go to the right and actually get to that 8.
So, we have to go over here and keep working away.
There's a nine, equal to or less than 9, another 9 equal to 9.
Push that number up equal to or greater than 9.
The minimizer gets an 8 going this way.
The maximizer is insured of getting a 9 going that way.
So, once again, we've got a cut off situation.
It's as if this doesn't exist.
Those static evaluations are not made.","Did he compare two Max nodes for the deep cut, or did he compare the bottom Min node with the root Max node?"
140,STjW3eH0Cik,"So, you have to see that.
This is the important essence of the notion the alpha-beta algorithm, which is a layering on top of minimax that cuts off large sections of the search tree.
So, one more time.
We've developed a situation so we know that the maximizer gets a 2 going down to the left, and he sees that if he goes down to the right, he can't do better than 1.
So, he says to himself, it's as if that branch doesn't exist and the overall score is 2.
And it doesn't matter what that static value is.
It can be 8, as it was, it can be plus 1,000.
It doesn't matter.
It can be minus 1,000.
Or it could be plus infinity or minus infinity.
It doesn't matter, because the maximizer will always go the other way.
So, that's the alpha-beta algorithm.
Can you guess why it's called the alpha-beta algorithm? Well, because in the algorithm there are two parameters, alpha and beta.
So, it's important to understand that alpha-beta is not an alternative to minimax.
It's minimax with a flourish.
It's something layered on top like we layered things on top of branch and bound to make it more efficient.
We layer stuff on top of minimax to make it more efficient.
As you say to me, well, that's a pretty easy example.
And it is.
So, let's try a little bit more complex one.
This is just to see if I can do it without screwing up.
The reason I do one that's complex is not just to show how tough I am in front of a large audience.
But, rather, there's certain points of interest that only occur in a tree of depth four or greater.","In the alpha-beta pruning example, the branch that isn't actually generated is simply the rightmost one leading to the terminal node, which isn't computed due to the ""cut."" Is this correct? If so, the statement ""it's as if that branch doesn't exist"" should be understood to mean that the algorithm will never select the action leading to the right-hand node (the one with a value of 1 or less). Is this interpretation accurate?"
141,STjW3eH0Cik,"That's level 0.
That's level 1.
This is level d minus 1.
And this is level d.
So, down here you have a situation that looks like this.
And I left all the game tree out in between .
So, how many leaf nodes are there down here? b to the d, right? Oh, I'm going to forget about alpha alpha-beta for a moment.
As we did when we looked at some of those optimal searches, we're going to add these things one at a time.
So, forget about alpha-beta, assume we're just doing straight minimax.
In that case, we would have to calculate all the static values down here at the bottom.","""I am having trouble understanding the insurance policy being taught. Can anybody help?"""
142,STjW3eH0Cik,"This move generation is not made and computation is saved.
So, let's see if we can do better on this very example using this alpha-beta idea.
I'll slow it down a little bit and change the search type to minimax with alpha-beta.
We see two numbers on each of those nodes now, guess what they're called.
We already know.
They're alpha and beta.
So, what's going to happen is the algorithm proceeds through trees that those numbers are going to shrink wrap themselves around the situation.
So, we'll start that up.
Two static evaluations were not made.
Let's try a new tree.
Two different ones were not made.
A new tree, still again, two different ones not made.
Let's see what happens when we use the classroom example, the one I did up there.
Let's make sure that I didn't screw it up.
I'll slow that down to 1.
2, same answer.
So, you probably didn't realize it at the start.",Shouldn't the root node be updated to be greater than or equal to 8?
143,SUJCgisKW1Y,"So we'll be looking at what's the best way? Something like y hat test log y tests.
I should have written this down before.
I think the cross entropy loss is actually the opposite of this.
Is this correct for cross entropy loss? Something like this.
And so then you'll be minimizing the parameters of basically our comparator function with respect to how accurate your predictions are.
So more specifically, what this will look like is this corresponds to something called matching networks.
And what we're going to do is we're going to encode our training examples into some embedding space.
And then for each of those embeddings, we will compute this function that you can think of this as potentially taking the dot product between your embedding for x test and you're betting for xk.","""Does the cross entropy loss shown need to have a negative sign preceding it?"""
144,SXBG3RGr_Rc,"OK, it's L'Hopital.
L'Hopital's Rule.
You have to differentiate the-- I guess we differentiate this guy as a ratio or something, and see what happens when it goes to 0.
And what we get when we use L'Hopital's Rule is that, oh thank God, this is still zero.
So now we know that we have a point up there and a point down there.
So now we've got three points on the curve, and we can draw it.
It goes like that.
No, it doesn't go like that.
It's obviously a Gaussian, right? Because everything in a nature is a Gaussian.
Can you put that laptop away, please? Everything in nature is a Gaussian, so it looks like this.
That right? No, actually, not everything in nature is a Gaussian.
And in particular, this one isn't a Gaussian either.
It looks more like one of those metal things they used to call quonset huts.
That's what it looks like.
Boom, like so.
So that is the curve of interest.
Now, did God say that using this way of measuring disorder was the best way? No, Got has not indicated any choice here.
We use this because it's a convenient mechanism, it seems to make sense, but in contrast to the reason it's used information theory, it's not the result of some elegant mathematics.
It's just a borrowing of something that seems to work pretty well.
Any of those curves would work just about the same, because all we're doing with it is measuring how disordered a set is.","Why do we need to use L'Hospital's Rule if N/T equals 0 (not approaching 0)? If we multiply anything by 0, we get 0, correct?"
145,Sdw8_0RDZuw,"And at this point, I'm done.
I'm left with a set of edges called covering edges, which have the property that the only way to get from one vertex to another is going to have to be to use a covering edge to the target for vertex.
Or more precisely, the only way to get, say, from a to b is going to be to use that covering edge.
If there was any other path that went from a elsewhere and got back to b without using this edge, then it wouldn't be a covering edge anymore.
The fact that it's a covering edge means that if you broke it, there's no way anymore to get from a to b.
So that's the definition of covering edges and you'll do a class problem about them, more precisely, in a minute.
So the other edges are unneeded to define the walk relation.
And all we need to keep are the covering relations to get the minimum representation of the walk relation in terms of a DAG.
","""Shouldn't Edge(b,d), not being a covering edge, be deleted?"""
146,T1AtlGrCoU8,"And having proved both for all x Q of x and for all y P of y, clearly the AND holds.
And I've just proved that the right-hand side of this implication is true given that the left-hand side is true.
Now, having called this proving validity, let me immediately clarify that this is not fair to call a proof because the rules of the game are really murky here.
This theorem, you could read it as saying that universal quantification distributes over AND is one of these basic valid formulas that is so fundamental and intelligible that it's hard to see what more basic things you are allowed to assume when you're proving it.","I'm struggling to fully understand what you mean by this proposition. Are you suggesting that 'z' represents a set of all possible values of 'z'? And when you mention P(z) and Q(z), are you referring to some properties of those elements? So, does 'z' define a domain or a set of elements of type 'z', concerning the elements themselves? My question is, what leads you to believe that the elements 'x' and 'y' are part of the set 'z'? How does universal generalization logic apply here?

I'm confused by all the technical terms, although everything was clear to me until now. Could you explain the problem using some concrete, real-world examples? For instance, if 'z' is a set of male individuals, and P(z) or Q(z) represents some property of them, why do you assume that 'x' and 'y' are also male individuals, specifically from the set 'z'? For example, if P(z) and Q(z) indicate that a person is black and taller than 6 feet, how can we assume that a random 'x' is black or taller than 6 feet, even if he happens to be male by chance, or even a person at all? Nothing specifies what 'x' isit could be a goat, a cow, or even a rock."
147,TDo3r5M1LNo,"It's like interval arithmetic.
You know, interval arithmetic? I want to know, what are the extremes I can get on the output of a division if I'm given that a number is in some interval here and some interval here? If the answer is always, use one of the extreme endpoints here and use one of the extreme endpoints here, then this algorithm will work.
Otherwise, all bets are off.
Cool.
So if you negate-- if you put a minus here, that will work fine, because it's negating this range.
And then it's just like sum.
But-- AUDIENCE: [INAUDIBLE] ERIK DEMAINE: Oh, a divider-- if you're careful about 0, yeah.
Actually, it doesn't work, because we care about how close this can get to 0 for division.
It might be enough to consider those.
It's like, instead of minimizing and-- instead of computing this entire interval, if this interval spans 0, maybe I need to know-- if 0 is here, I need to know how close to 0 I can get on the left side and how close to 0 I can get on the right side.
Still just four quantities I need to know.
I would guess, for division, that's enough.
Yeah.
Nice.
Solved a little problem.
Then, we would be multiplying the subproblem space, instead of by 2, by 4.
Hey, maybe we should put this on the final.
No, just kidding.
Now it's in lecture, so we can't use it.
But it's a cool set of problems, right? You can do a lot with dynamic programming.
You don't need to be that clever, just brute force anything that seems hard.
And when it works, it works great.
And this class is all about understanding when it works and when it doesn't work.
Of course, we will only give you problems where it works.
But it's important to understand when it doesn't work.
For example, DAG shortest paths-- that algorithm on a non-DAG, very bad.
Infinite time.
OK.
Our last example is piano fingering.",Could someone explain to me how to use this algorithm for division?
148,TOb1tuEZ2X4,"And correspondingly, you can have either one or two keys in a node.
Make sense? AUDIENCE: Yeah.
PROFESSOR: Cool.
OK So coming back to this.
So the root does not have a lower bound.
The root can have one child in any tree.
So you have a B equal to 5 tree, the root can still have one child-- sorry.
Not one child, one key element, two children.
All right.
It's good.
Also it's completely balanced.
So all the leaves are the same depth.
So you can see it here, right? So you can't have a dangling node here.
This is not allowed.
You have to have a leaf.
You have to have something going down, and everything ends at the same level.","Is anyone in the class paying attention? It's not at all clear why the depth is logN. It was only after mentioning that all the leaves are at the same depth that we understood the depth to be actually logN, indicating the tree is balanced."
149,TOb1tuEZ2X4,"It's less than 30, it goes to the left.
It's between 10 and 17, it goes in the middle.
16.
And it's greater than 14, so we add 16 here.
All right.
That seems good.
All the properties fine.
This still has two elements, which is the maximum, but it's good.
It doesn't overflow.
Let's insert something else.
Let's insert 2.
So 2 goes to 30, goes down, goes down.
And we have a problem, because 2 has overflowed this node.
So we split.
And the way we split is we take the middle element.
So we split the node here.
And 3 goes up to the parent, so 3 goes here.
And all good, except for the parent has overflowed.
So what do we do with the parent? We split the parent again.
And this time, it's right down the middle, the 10 goes up.
So OK, let's get rid of this.
So now that we split the parent, the 10 goes up here.
And you're good.
It's a bit cluttered, so let me reposition the 17.
Did those two operations make sense? Questions? AUDIENCE: If your node size [INAUDIBLE] number of-- PROFESSOR: So just pick the-- first of all-- OK.","Guys, the B-tree is not valid, is it? We are having problems with the root node."
150,TOb1tuEZ2X4,"So that's the general node.
So before we go into more details of the properties and everything, the question is why use B-trees.
So if we do a quick depth analysis, we can see that the depth is to log n rate.
Is that clear to everyone sort of, why the depth is log n? Because you have branching just like in binary search trees.
In fact, you have more branching.
But in any case, depth is to log n.
But why use B-trees over binary search trees? Anyone have a reason why you would prefer to use B-trees or not? So all the operations are still log n.","Is anyone in the class paying attention? The reason why the depth is logN is not at all clear. It was only after it was mentioned that all the leaves are at the same depth that we understood the depth to be actually logN, indicating the tree is balanced."
151,TOb1tuEZ2X4,"You do a merge.
So what do you have? So here you have B minus 2, and here you have B minus 1.
And you get 2B minus 3.
Well, you've got another element.
You also take the parent.
So how do you do the merge.
I just want to show you the merge first.
So the way you do it is you move the parent down, and you merge these two.
Seems OK? So you move the parent node down and merge these two.
And, well, now this comes together, and this points into the new node.
Sort of clear what's going on? Questions? Yes? AUDIENCE: So now the parent is underfull? PROFESSOR: Well, so you have-- yeah, exactly.
So you have decreased the size of the parent, so it might be underfull.
So you propagate.
Anything else? AUDIENCE: So are these all different techniques for doing that? PROFESSOR: So there are two cases.
So either you have a sibling which has extra nodes to donate to you or you don't.
If you don't, then you have to do this.
AUDIENCE: But what about that case? Or is that just like-- PROFESSOR: No, that is moving it down to the leaf.
Once you move the deletion down to the leaf, so here we have something now.
And now you move it all the way back up.","I have a question: The nodes are not supposed to have more than b-1 keys, but in his example (m), there are 5 keys in one leaf. How is that correct?"
152,TU0ankRcHmo,"And notice what I've wrote here is basically, p of t equals M times p of t.
So now, because we have this correspondence, this means that r is a stationary distribution of the random walk.
So basically, it means that this flow based equations can be interpreted based on the flow, or can also be interpreted as this intuition that there is a random walker walking around infinitely long over the graph, so that after some time, basically, it doesn't matter where the walker- random walker started, but it's really all about this distribution of where the random walker is, is going to converge to this stationary, uh, distribution.
And in order for you to compute the stationary distribution, is the same problem as it was before to solve that system of equations or to solve this recursive equation of r equals m times r.","I enjoyed the lecture and appreciated learning about PageRank in such detail. I do have a question regarding the connection between random walks and the flow equation. We assumed that the random walk reaches a stationary state after a finite number of steps, which implies that the stationary state is simply r. However, this assumption wasn't revisited later in the lecture. What happens if the random walk doesn't converge to a fixed state within a finite number of steps but instead approaches it indefinitely? Will the limit still be r? The lecture also mentioned that the starting point of the random walk does not affect the final converged state, which will be r regardless. I believe I have a proof that addresses both of these questions, assuming that matrix M has n independent eigenvectors, where n is the number of nodes. I'm uncertain, though, whether this assumption is universally applicable and, consequently, whether my proof is of any significance. Nonetheless, I would be eager to discuss any related ideas."
153,TWVntUfXsKs,"It's 110 choose to times the variance of the M sub ij turns out to be about 16.37, which means that the standard deviation sigma is less than 4.
Now I can apply Chebyshev, because by the Chebyshev band the probability that 16.4 is within a 2 sigma, is further away than 2 sigma, is only one chance in four.
Which means the probability that it's within 2 sigma, that the actual number of measured pairs is within 2 sigma of the expected number 16.4 is greater than 1 minus 1/4, or 3/4.
There's a 3/4 chance that the number of pairs that we find is within 2 sigma of the expected number 16.4.
Sigma was about 4, so this is 8, which means that we're expecting, with 3/4 probability, somewhere between 8.4, meaning 9, and 24.4, meaning 25 pairs.
So 75% of the time, in a class of 110, we're going to find between 9 and 25 pairs of birthdays.
Did that actually happen? Well it did.
In our class of 110 for whom we had data, we actually found 21 pairs of matching birthdays.
Literally we found 12 pairs and three triples, but each triple counts as three matching pairs.
And there they are, the blues are triples.
And you can see whether your birthday is among those, and knowing that you have a classmate or two that have the same birthday that you do.
So there are 15 different birthdays, but they count as 21 pairs because it's 12 single pairs, and three triplets, each of which counts for three pairs.
","If anyone is confused by the formula, he intended to write Pr[16.4 - 2(sigma) <= |R-u| <= 16.4 + 2(sigma)], but instead, he simply used Pr[16.4  2 sigma]. For some reason, I thought he substituted 16.4 for |R-u| in Chebyshev's formula, which is nonsensical.
Why is the standard deviation less than 4?"
154,TjZBTDzGeGg,"And we're going to use some duct tape to help us think about just one piece of the wheel.
So I want you to just think about that piece of the wheel as the wheel comes flying over the top, and I blow on it like that.
What's going to happen to that one piece? It's going to go off that way, right? And the next piece is going to go off that way too.
So when it comes over, it has to go that way.
Let me do some ground f here just to be sure.
It's very powerful feeling.
Try it.
We need a demonstration.
I don't anybody think that I'm cheating, here.
So let's just twist it one way or the other.
So that's powerful pull, isn't it.
Alex is now never going to get the gyroscope wrong, because he's got the right representation.
So much of what you're going to accumulate in this subject is a suite of representations that will help you to build programs that are intelligent.
But I want to give you a second example, one a little bit more computational.
But one of which was very familiar to you by the time you went to first grade, in most cases.","""I can't grasp the point; could someone explain it to me? Thanks."""
155,TjZBTDzGeGg,"So I like to reinforce that by giving you a little puzzle.
Let's see, who's here? I don't see [? Kambe, ?] but I'll bet he's from Africa.
Is anyone from Africa? No one's from Africa? No? Well so much the better-- because they would know the answer to the puzzle.
Here's the puzzle.
How many countries in Africa does the Equator cross? Would anybody be willing to stake their life on their answer? Probably not.
Well, now let me repeat the question.
How many countries in Africa does the Equator cross? Yeah, six.
What happened is a miracle.
The miracle is that I have communicated with you through language, and your language system commanded your visual system to execute a program that involves scanning across that line, counting as you go.
And then your vision system came back to your language system and said, six.
And that is a miracle.
And without understanding that miracle, we'll never have a full understanding of the nature of intelligence.
But that kind of problem solving is the kind of problem solving I wish we could teach you a lot about it.
But we can't teach you about stuff we don't understand.
We [INAUDIBLE] for that.
That's a little bit about the definition and some examples.
What's it for? We can deal with that very quickly.
If we're engineers, it's for building smarter programs.
It's about building a tool kit of representations and methods that make it possible to build smarter programs.","Could someone please explain to me what he meant by ""Well, so much the better""?"
156,TjZBTDzGeGg,"So it would be two to the fourth she says aggressively and without hesitation.
Yes, two to the fourth, 16 possibilities.
So we could actually draw out the entire graph.
It's small enough.
There's another position over here with the farmer, fox, goose, and grain.
And in fact that's the one we want.
And if we draw out the entire graph, it looks like this.
This is a graph of the situations and the allowed connections between them.
Why are there not 16? Because the other-- how many have I got? Four? 10? The others are situations in which somebody gets eaten.
So we don't want to go to any of those places.
So having got the representation, something magical has happened.","Can someone explain why there are 2^4 scenarios in the farmer, fox, goose, and grain problem?"
157,Tw1k46ywN6E,"AUDIENCE: And you want to add all the weights.
PROFESSOR: And you want to add all the weights, right.
You're exactly right.
I have two more Frisbees, but I need to use them for something else.
So you get one next time.
Or do I have more than that? No, no.
These are precious Frisbees here.
Sorry, man.
And you corrected me, too.
Shoot.
This is sad.
So that needed to be a Wr.
But you also need to add up all of the nodes that are in here, because they're one more level.
And now you see why I made the mistake.
I don't usually make mistakes.
But what I really want-- actually it's more like-- I don't know, a few per lecture.
Constant order one mistakes.
I'm going to say this is Wi, j, where Wi, j is simply the sum of all of the weights from i to j.
And this makes perfect sense, because the nice thing is that I don't even need to put an r in there.
I could choose a particular r.
Wr will be in there.
But all of the other nodes are going to be in there anyway.
So it doesn't really matter what the r selection is corresponding to this term here.
I will put it inside the minimization.
This bracket here is closed with that, but you can pull that out, because that doesn't depend on r.","""Why do we need to add all of the nodes? I thought we only needed to add nodes where i <= r <= j."""
158,Tw1k46ywN6E,"And we'll talk about that, as to why the greedy algorithm doesn't quite work.
So it's kind of similar to the interval scheduling and the weighted interval scheduling problem that we had back in February, where the regular interval scheduling problem, greedy worked, earliest finish time worked.
But when it came to weights, we had to graduate to dynamic programming.
So here's our second problem, optimal BSTs.
So what is an optimal BST? We have a bunch of keys that we want to store in the BST, K1 through Kn.
And we'll assume that the way this is set up is that K1 is less than K2, da, da, da, less than Kn.","Revised sentence: ""First example question: The pseudocode presented only functions with contiguous palindromes, correct? In contrast, the brain teasers initially provided did not require contiguity, for example, 'rotator' extracted from 'turboventilator.' I fail to understand how the code could identify 'rotator' if it begins with L(0, end). Additionally, I'm unclear as to why the number of subproblems is n squared."""
159,U1JYwHcFfso,"It may sound difficult, but in fact, there's a pretty simple way, which are called AVL trees, that maintain balance in a particular way called height balance.
This is if we take the height of node.left-- actually, I'd prefer to-- node.right, minus height of node.left, so this thing is called skew of the node.
I want this to always be minus 1, 0, or plus 1.
So this is saying that if I have any node, and I look if its left subtree and its right subtree, I measure their heights-- remember, that's downward distance, maximum distance to a leaf-- I measure the height of this tree-- maximum height-- and I measure the maximum height of this subtree, I want these to be within 1 of each other.","I am somewhat confused about the definition of skew. I attempted to apply the formula he wrote on the blackboard, which is { skew(node) = height(node.right) - height(node.left) }. For instance, if I take this tree and use the formula to calculate the skew, I would get the following results: Z (height: 2, skew: 1 - 0 = 1), Y (height: 1, skew: 0 - 0 = 0), and X (height: 0, skew: 0 - 0 = 0). However, this doesn't seem right to me because it would suggest that the tree is balanced, with each node's skew falling within the range of {-1, 0, 1}. I would expect this tree to be unbalanced and skewed to the right by 2. Intuitively, I would guess the skews to be something like: Z (height: 2, skew: 2), Y (height: 1, skew: 1), and X (height: 0, skew: 0). Which interpretation is correct? If the second interpretation is accurate, then I am uncertain about the correct formula or what I am doing wrong."
160,UHBmv7qCey4,"And to get the weights for the next generation, all I have to do is scale them so that they equal half.
This was not noticed by the people who developed this stuff.
This was noticed by Luis Ortiz, who was a 6.034 instructor a few years ago.
The sum of those weights is going to be a scaled version of what they were before.
So you take all the weights for which this new classifier-- this one you selected to give you the minimum weight on the re-weighted stuff-- you take the ones that it gives a correct answer for, and you take all of those weights, and you just scale them so they add up to 1/2.
So do you have to compute any logarithms? No.
Do you have to compute any exponentials? No.
Do you have to calculate z? No.
Do you have to calculate alpha to get the new weights? No.
All you have to do is scale them.
And that's a pretty good thank-god hole.
So that's thank-god hole number one.
Now, for thank-god hole number two, we need to go back and think about the fact that were going to give you problems in probability that involve decision tree stumps.
And there are a lot of decision tree stumps that you might have to pick from.
So we need a thank-god hole for deciding how to deal with that.
Where can I find some room? How about right here.
Suppose you've got a space that looks like this.","While discussing the benefits of the ""Thank God hole number 1,"" the professor noted that there is no need to compute logarithms or alphas. However, I'm confused as to why alphas are deemed unnecessary, given that H(x) is a weighted sum of h(x), with alphas serving as the weights. It seems to me that we would still need to compute alphas to obtain the final answer. Could someone please clarify what I might be overlooking?"
161,UHBmv7qCey4,"It would happen to be wrong, but it's a valid test with a valid outcome.
So that's how we double the number of test that we have lines for.
And you know what? can even have a kind of test out here that says everything is plus, or everything is wrong.
So for each dimension, the number of decision tree stumps is the number of lines I can put in times 2.
And then I've got two dimensions here, that's how I got to twelve.
So there are three lines.
I can have the pluses on either the left or the right side.
So that's six.
And then I've got two dimensions, so that gives me 12.
So that's the decision tree stump idea.
And here are the other decision tree boundaries, obviously just like that.
So that's one way can generate a batch of tests to try out with this idea of using a lot of tests to help you get the job done.
STUDENT: Couldn't you also have a decision tree on the right side? PATRICK WINSTON: The question is, can you also have a test on the right side? See, this is just a stand-in for saying, everything's plus or everything's minus.
So it doesn't matter where you put the line.
It can be on the right side, or the left side, or the bottom, or the top.
Or you don't have to put the line anywhere.
It's just an extra test, an additional to the ones you put between the samples.
So this whole idea of boosting, the main idea of the day.
Does it depend on using decision tree stumps? The answer is no.
Do not be confused.
You can use boosting with any kind of classifier.
so why do I use decision tree stumps today? Because it makes my life easy.
We can look at it, we can see what it's doing.
But we could put bunch of neural nets in there.
We could put a bunch of real decision trees in there.","Doesn't the orange line at the bottom symbolize the exact same thing as the orange line on the very left? Both indicate ""Everything is +"" or ""Everything is -."" Therefore, we don't have 12 classifiers, but only 10."
162,UHBmv7qCey4,"So I want the alpha over here.
That'll be sort of a byproduct of picking that test.
And with all that stuff in hand, maybe that will be enough to calculate Wt plus 1.
So we're going to use that classifier that we just picked to get some revised weights, and then we're going to go around that loop until this classifier produces a perfect set of conclusions on all the sample data.
So that's going to be our overall strategy.
Maybe we've got, if we're going to number these things, that's the fourth big idea.
And this arrangement here is the fifth big idea.
Then we've got the sixth big idea.
And the sixth big idea says this.
Suppose that the weight on it ith sample at time t plus 1 is equal to the weight at time t on that same sample, divided by some normalizing factor, times e to the minus alpha at time t, times h at time t, times some function y which is a function of x, But not a function of time.","""Where did that formula come from?"""
163,UroprmQHTLc,"This defense is good against that virus.
All right, so each virus, I have a defense for.
So an example would be-- these are, by the way, dated viruses, but that's when the slides were made.
So against the Mydoom virus, you could use Defender, Microsoft Defender.
Against the ILOVEYOU virus, you could use Norton.
Against the Bablas virus, you could use ZoneAlarm.
Well, is that what we want? It's expensive.
It means that for every different virus, I need a different defense.
I have to spend a fortune on software.
This is not what we want.
So that's when for every virus, there's a defense, but the quantifiers are in the wrong order.
Let's reverse them.
Suppose I tell you that there's one defense that's good for all viruses.
There is a defense such that for every virus, d protects against v.
For example, if d is MITviruscan, then it would be wonderful if it was true that d protects against all viruses, there's one defense good against every attack.
That's what we want because it's a lot cheaper.
All right, let's start looking at a concrete mathematical example.","He stated that each virus requires a unique defense. However, why must the defense differ for each one? The statement implies that there is a defense capable of protecting against all viruses. Is it not possible for a single defense to be effective against multiple viruses? For example, antivirus software like Norton can protect against viruses such as MYDOOM, ILOVEYOU, and BABLAS."
164,UroprmQHTLc,"Well, let's shift again, the point being here both we're looking at alternate quantifiers, and we're understanding that the truth of a proposition with quantifies depends crucially on the domain of discourse.
If we let the domain of discourse be the negative reals, then what this is saying is that for every negative real, there's a bigger negative real.
And that, of course, is true.
Because if you give me a negative real r, then r over 2, because it's negative, is actually bigger than r.
And it will not be positive if r isn't positive.
So sure enough, G in this case is true.
All right, let's reverse the qualifiers and see what happens.
It's worth thinking about.
So let's call H the assertion that for every y-- sorry, that there exists a y such that for every x, x is less than y.",How can r over 2 be positive?
165,VJzv6WJTtNc,"And it's not clear with all that common basis for returning what value they have.
It's not immediately obvious that they're independent, but as a matter of fact they are.
And again this is absolutely something that you should check out.
If you don't stop the video now to work it out, you should definitely do it afterward.
It's an important little exercise and it's easy to check.
All you have to do is check that the probabilities of the event of odd number of heads in the event all match are independent as events.
Or you could use the random variable definition and check that these two random variables were independent by checking 4 equations because this can have values 0 and 1.","The probability of event O, Pr(O), is 1/2, whereas the conditional probability of event O given event M, Pr(O|M), is 1, which is not equal to Pr(O) because M signifies the outcome 111. Therefore, I deduce that these two events are not independent. Could I be mistaken? Is there an error?"
166,VYZGlgzr_As,"And then we've got flow conservation.
And the important thing here is that I don't have it for all V, but I do have it for vertices, V, that are not the source or the sink.
And I'm going to require f(u,v) equals 0, right? And the last one, which I haven't talked about, but becomes easy to talk about, given this constraint, is skew symmetry.
So if you take-- this doesn't have to be an edge between u and v.
Now, I'm talking about the flow, f, between u and v.
And so the u could be s, which is the source.
v could be t, which is the sink.
So in general, I'm not talking about a flow.","How does the sum even work? Does the sum of flow values outgoing from all vertices, except for the source and sink, equal zero?"
167,VYZGlgzr_As,"SRINIVAS DEVADAS: d, b is going out.
That's exactly right, d, b.
And the plus 2, it would be d, T, right? And so you have to do the enumeration.
It's worthwhile doing once.
And then it gets kind of boring.
We won't to do it again.
But you have to realize that you have to absolutely look at every pair of vertices.
And you have to use skew symmetry and ensure that, even though there's actually no edge going out, if there's an edge coming in, you've got to count that.
And that's going to get a negative.
Whatever is coming in, you've got to subtract, OK? So it's not that complicated.
Yeah, go ahead.
STUDENT: Do we not consider S, c? SRINIVAS DEVADAS: I'm sorry? STUDENT: Do we not consider S, c? SRINIVAS DEVADAS: So the beauty of this is that, when you don't have a particular edge from S to c, you can use skew symmetry to argue that S, c and c, S cancel out each other, all right? So that's the good part, right? And thanks for asking the question.
That's a good question.
All right.
Here you go.
So you can do that by just looking at the edges.
And you can add up the numbers, all right? And so I don't think this is going to be absolutely crucial to understand the rest of the lecture.
Keep this in mind, that there's a process by which you define the value of a cut.
And we're going to get back to this, when we prove the max-flow min-cut theorem next time.","During the lecture, one student inquired as to why we don't consider the path from s to c. Srinivas explained that when there is no specific edge from s to c, you can employ skew symmetry to argue that the paths from s to c and from c to s cancel each other out. I didn't understand his explanation. If we are focusing on f(S, T), why does the path from c to s become relevant and negate the path from s to c?"
168,VYZGlgzr_As,"So what I've done here is invoke, essentially, this, except it's not exactly that, in the sense that it's written a little bit differently.
But if you see what's going on here, what I've done is look at this s, and I've said, think of this s as being cap V minus s.
Right? So that gives you s.
And those are clearly disjoint, right? Those are clearly disjoint sets.
There is this one and this one are disjoint sets.
That's what I mean to say.
I mean, these two aren't disjoint, but this and that are disjoint.
And that's what you need, in order to invoke the little property that you have here.
And so that all make sense? You see why I did that? OK? What can I say about either of these two quantities? Can I say something about either of these two quantities? Yeah? STUDENT: f(V,V) is 0.","Why does f(s, V) equal f(V, V) minus f(V - s, V)?"
169,VYZGlgzr_As,"So you can see that, pretty quickly, it gets kind of confusing, if we end up having these cycles that are such simple cycles, especially the ones where you have Su and Us, OK? And so we're going to disallow cycles of two kinds, right? The first cycle we're going to disallow is this simple one where we say, if I have a, then no self-loop edges allowed.
So that would involve accumulation at a particular node.
And it's going to make things really confusing.
And CLRS disallows that.
And most flow network algorithms assume that you're just going to discard these cycles.
This particular transformation that I'm going to describe here is something that is going to be forced on you.
And this is for your benefit in over 6 lectures and sections.
But it's not actually something that CLRS follows.","""What was the purpose of prohibiting self-edges and cycles, such as (u,v) and (v,u)? I'm not entirely clear on the distinction between net flow and positive flow, nor do I understand the implications of the professor's assumption on the subsequent definitions, constraints, and the proof itself."""
170,VYZGlgzr_As,"You know, dump the source on the left, and dump the sink on the right.
And you compute the flow the way we've defined it.
That's the flow.
It doesn't matter how you partition these vertices, as long as you've got the source on the left and the sink on the right, OK? And so we can prove this using implicit summation notation.
We'll do that.
And that'll give you a really good sense of why this statement is true, because we know that, for any given cut, the flow that cut is bounded by the capacity of that cut, right? You know that.
But to show this, here's how we could show that, f(S, T) is f(S, V) minus f(S, S), OK? So I'm playing around, just like I did before.
I had taken the cap T-- I know that S union T is cap V, right? This is a partition.
So I know that S union T is cap V.
So I can put a V here and an S here, right? And that's a subtraction over there, of course, right? So put that up here and finish this, a couple more lines.","The proof appears somewhat odd; he has split the right-hand side (RHS) term in the parentheses into two parts, yet according to the rules he provided earlier, the left-hand side (LHS) term in the parentheses should be broken up."
171,VrMHA3yX_QI,"If we rerun that, we see that it trains itself up very fast.
So we seem to be still close enough to a solution we can do without one of the neurons in each column.
Let's do it again.
Now it goes up a little bit, but it quickly falls down to a solution.
Try again.
Quickly falls down to a solution.
Oh, my god, how much of this am I going to do? Each time I knock something out and retrain, it finds its solution very fast.
Whoa, I got it all the way down to two neurons in each column, and it still has a solution.
It's interesting, don't you think? But let's repeat the experiment, but this time we're going to do it a little differently.
We're going to take our five layers, and before we do any training I'm going to knock out all but two neurons in each column.
Now, I know that with two neurons in each column, I've got a solution.
I just showed it.
I just showed one.
But let's run it this way.
It looks like increasingly bad news.
What's happened is that this sucker's got itself into a local maximum.
So now you can see why there's been a breakthrough in this neural net learning stuff.
And it's because when you widen the net, you turn local maxima into saddle points.
So now it's got a way of crawling its way through this vast space without getting stuck on a local maximum, as suggested by this.
All right.
So those are some, I think, interesting things to look at by way of these demonstrations.
But now I'd like to go back to my slide set and show you some examples that will address the question of whether these things are seeing like we see.
So you can try these examples online.
There are a variety of websites that allow you to put in your own picture.
And there's a cottage industry of producing papers in journals that fool neural nets.
So in this case, a very small number of pixels have been changed.
You don't see the difference, but it's enough to take this particular neural net from a high confidence that it's looking at a school bus to thinking that it's not a school bus.","So I understand that the picture on the left is a school bus, but can someone explain to me what is in the picture on the right?"
172,VrMHA3yX_QI,"Let's see, it might look like this.
There's a [? summer.
?] There's a minus 1 up here.
No.
Let's see, there's a minus 1 up-- [INAUDIBLE].
There's a minus 1 up there.
There's a multiplier here.
And there's a threshold value there.
Now, likewise, there's some other input values here.
Let me call this one x, and it gets multiplied by some weight.
And then that goes into the [? summer ?] as well.
And that, in turn, goes into a sigmoid that looks like so.
And finally, you get an output, which we'll z.","Could someone clarify what he is doing with the -1 and the threshold value? I watched his previous lecture, 12a, but I still don't quite understand how he eliminates thresholds by doing that."
173,WO6vQJ6Rhm8,"You start saying, aha, that sentence sounds like recursion.
And that's right.
That's how we're going to solve this problem.
OK? So, in particular, here's going to be our thing.
I'm going to do a slightly different one in the solution-- so you guys should all be vigilant-- which is I'm going to write x ij to be the min work to convert.
I'm not a Python programmer, but hopefully I got this right.
i colon is going to be everything from i to the end of the file.
So in other words, this is the suffix version of our problem-- and into B j colon, like that.
OK.","If I had Document A and created Document B as a copy of A, but with the first line moved to the end, wouldn't continuous swaps and incrementing the line indices i and j by one result in all possible swaps? In this scenario, after each swap, the lines would not match, as indicated in condition 3. Edit: This issue was clarified later."
174,WPSeyjX1-4s,"And here's the way to think about it recursively.
I want to move a tower of size n, I'm going to assume I can move smaller towers and then it's really easy.
What do I do, I take a stack of size n minus 1, I move it onto the spare one, I move the bottom one over, and then I move a stack of size n minus 1 to there, beautiful, recursive solution.
And how do I move the smaller stack? Just the same way, I just unwind it, simple, and, in fact, the code follows exactly that.
OK, I do a little [INAUDIBLE] domain up here to try and get your attention, but notice by doing that what did I do? I asked you to think about it recursively, the recursive solution, when you see it, is in fact very straightforward, and there's the code.","""Is anyone else having trouble understanding the Hanoi code?"""
175,WPSeyjX1-4s,"And you can see now it says, if either x is equal to 0 or x is equal to 1 I'm going to return 1, otherwise, reduce it to two simpler versions of the problem but with different arguments, and I add them up.
OK, and if we go look at this, we can actually run this, if I can find my code.
Which is right there, and I'm just going to, so we can, for example, check it by saying fib of 0.
I just hit a bug which I don't see.
Let me try it again.
I'll try it one more time with fib of 0.
Darn, it's wrong, let me try it.
I've got two different versions of fib in here, that's what I've got going on.","If you act quickly and look at line 165, you'll notice he has a scenario where n equals 1. Although the subsequent code isn't visible, it behaves as depicted in his slide, calculating n minus 1 and n minus 2. When he inputs 0, the base case of n equaling 1 doesn't apply, leading him to continuously decrement n into negative values, resulting in infinite recursion. However, an interesting takeaway is that a module can contain two functions with identical signatures, and in such instances, Python opts to execute the second one."
176,WPSeyjX1-4s,"But notice, every other thing I do here, I've actually computed those values.
I'm wasting measures, or wasting time, it's not so bad with fib of 5, but if this is fib of 20, almost everything on the right hand side of this tree I've already computed once.
That means fibs very inefficient.
I can improve it by using a dictionary, very handy tool.
I'm going to call fib not only with a value of n, but a dictionary which initially I'm going to initialized to the base cases.
And notice what I do, I'm going to say if I've already computed this, just return the value in the dictionary.
If I haven't, go ahead and do the computation, store it in the dictionary at that point, and return the answer.","I appreciated that the recursive Fibonacci code (the inefficient version) had some optimization flaws, but I couldn't grasp how the modified dictionary became the argument for fibo_efficient(n, d). I mean, we modified 'd', but wasn't it already passed to fibo_efficient(n, d) as an argument?
""Thanks! The lecture on recursion was excellent. Its purpose was clearly defined and thoroughly explained. By the way, did anyone notice the 'code error' related to the base case?""
What would happen if you wanted to call fib_efficient(0, d)?"
177,WPSeyjX1-4s,"I say I need to prove that if it's true for an arbitrary value of n, I'm just going to prove that it's also then true for n plus 1.
And if I can do those two things I can then conclude for an infinite number of values of n it's always true.
Then we'll relate it back to programming in a second, but let me show you a simple example of this, one that you may have seen.
If I had the integers from 0 up to n, or even from 1 up to n, I claim that's the same as n times n plus 1 over 2.
So 1, 2, 3, that's 6, right.
And that's exactly right, 3 times 4, which is divided by 2, which gives me out 6.
How would I prove this? Well, by induction? I need to do the simple cases if n is equal to 0, well then this side is just 0.
And that's 0 times 1, which is 0 divided by true.
So 0 equals 0, it's true.
Now the inductive step.
I'm going to assume it's true for some k, I should have picked n, but for some k, and then what I need to show is it's true for k plus 1.
Well, there's the left hand side, and I want to show that this is equal to that.
And I'm going do it by using exactly this recursive idea, because what do I know, I know that this sum, in here, I'm assuming is true.
And so that says that the left hand side, the first portion of it, is just k times k plus 1 over 2, that's the definition of the thing I'm assuming is true.
To that I'm going to add k plus 1.
Well, you can do the algebra, right? That's k plus 1 all times k over 2 plus 1, which is k plus 2 over 2.
Oh cool, it's exactly that.
Having done that, I can now conclude this is true for all values of n.","""Why did Prof. Grimson add k+1 to k(k+1)/2?"""
178,WPSeyjX1-4s,"If it's either 0 or 1 long, it's a palindrome.
Otherwise you could think about having an index at each end of this thing and sort of counting into the middle, but it's much easier to say take the two at the end, if they're the same, then check to see what's left in the middle is a palindrome, and if those two properties are true, I'm done.
And notice what I just did I nicely reduced a bigger problem to a slightly smaller problem.
It's exactly what I want to do.
OK? So it says to check is this, I'm going to reduce it to just the string of characters, and then I'm going to check if that's a palindrome by pulling those two off and checking to see they're the same, and then checking to see if the middle is itself a palindrome.
How would I write it? I'm going to create a procedure up here, isPalindrome.
I'm going to have inside of it two internal procedures that do the work for me.
The first one is simply going to reduce this to all lowercase with no spaces.
And notice what I can do because s is a string of characters.
I can use the built in string method lower, so there's that dot notation, s.lower.
It says.
apply the method lower to a string.
I need an open and close per end to actually call that procedure, and that will mutate s to just be all lowercase.
And then I'm going to run a little loop, I'll set up answer or ans to be an empty string, and then, for everything inside that mutated string, I'll simply say, if it's inside this string, if it's a letter, add it into answer.
If it's a space or comma or something else I'll ignore it, and when I'm done just return answer, strips it down to lowercase.
And then I'm going to pass that into isPal which simply says, if this is either 0 or 1 long, it's a palindrome, returned true.
Otherwise, check to see that the first and last element of the string are the same, notice the indexing to get into the last element, and similarly just slice into the string, ignoring the first and last element, and ask is that a palindrome.
And then just call it, and that will do it.
And again there's a nice example of that in the code I'm not going to run it, I'll let you just go look at it, but it will actually pull out something that checks, is this a palindrome.
Notice again, what I'm doing here.
I'm doing divide-and-conquer.
I'm taking a problem reducing it, I keep saying this, to a simpler version of the same problem.","Shouldn't ""and isPal(s[1:-2])"" be at the penultimate line, with ""-2"" instead of ""-1""?"
179,WQHOImO0pX0,"And I'm going to say that s HALTS-- the string has this property called halting or HALTS-- if and only if this procedure P that s describes returns successfully when it's applied to s.
This is where we're really doing a diagonal argument.
We're taking the sth object-- the procedure that's described by s and applying it to s.
And that's kind of going down the diagonal of s applied to s, where the n-th element of the n-th row in a pictorial diagonal argument.
That's the idea that we're going here.
But let's to go back to the definition.
A string is said to HALT if when you interpret it as the description of a procedure that takes a string argument and you apply that string procedure to that very same thing, s, you successfully return.","The assumptions presented in this lecture, which are listed below, appear to be nonsensical. I would appreciate some assistance in understanding them:

1. An infinite string is computable. The sole definition provided for ""computable"" refers to segments of infinite strings. However, most computing deals with finite entities, and even when computations involve irrational numbers, there is no need to calculate all their digits. How is this definition of ""computable"" logical?

2. There is a countably infinite number of procedures. Given the limitations on size in the programming world we operate within, how can there be an infinite number of procedures? A large finite number is conceivable, but infinite?

3. There are uncountably many infinite binary strings that are non-computable. It is true that there are uncountably many irrational numbers, which would mostly be non-computable according to the previous definition. But what relevance does this have to actual computers halting in the real world?

4. HALT is defined as a program that terminates when given its own source code as input (refer to Slide 7). This definition of HALT is peculiar. Why is this specific input the only scenario considered?

5. A type-checking program, C, is said to exist based solely on the program text, s. Can a flawless type-checking procedure truly exist without any knowledge of the inputs to s? These explanations seem vague and forced. What crucial information am I overlooking?"
180,WQHOImO0pX0,"It's impossible to write a procedure that determines of strings whether they describe a procedure that HALTS when applied to itself.
OK.
That at least gives us some concrete problem that we can say is not something that a computer can do.
Even though it's a very well defined clear question, it's just not possible to get a computing procedure that will on an arbitrary string, figure out the right answer.
Any program that applied to strings is trying to do this job, either it will give the wrong answer.
Or if it never gives a wrong answer, it means it doesn't give an answer at all on some strings.
All right.","The assumptions presented in this lecture, which are listed below, appear to be nonsensical. I would appreciate some assistance in understanding them:

1. An infinite string is computable. The sole definition of ""computable"" provided relates to segments of infinite strings. However, most computing deals with finite entities, and even when computing involves irrational numbers, there is no need to compute all their digits. How is this definition of ""computable"" logical?

2. There is a countably infinite number of procedures. Given the limitations on size in the programming world, how can there be an infinite number of procedures? A large finite number is plausible, but infinite?

3. There are uncountably many infinite binary strings that are non-computable. While it's true that there are uncountably many irrational numbers, most would be ""non-computable"" according to the previous definition. But what relevance does this have to real-world computers and the halting problem?

4. HALT is defined as a program that terminates when given its own source code as input (refer to Slide 7). This definition of HALT is peculiar. Why is this specific input the only scenario considered?

5. A type-checking program, C, is said to exist based solely on the program text, s. Can a flawless type-checking procedure truly exist without any knowledge of the inputs to s? These explanations seem vague and forced. What crucial information am I overlooking?"
181,WQHOImO0pX0,"ROFESSOR: [? Diagonal ?] arguments are elegant, and infinite sets-- some people think-- are romantic.
But you could legitimately ask what is all this weird infinite stuff doing in a course that's math for computer science? And the reason is that diagonal arguments turn out to play a fundamental role in the theory of computing.
And what we're going to talk about now is the application of diagonal arguments to show that there are noncomputable sets and examine a particular one.
So let's look at the class of infinite binary strings.
Now, we've seen that there are an uncountable number of infinite binary strings, and that's because there was a simple bijection between the infinite binary strings and the subsets of the natural numbers-- that is the power set of n.
Let's look at the infinite binary strings that we might think of and call computable strings.
And what I mean by a computable string is that there's simply a procedure that will tell me what its digits are.","The assumptions presented in this lecture, which are listed below, appear to be nonsensical. I would appreciate some assistance in understanding them:

1. An infinite string is computable. The sole definition provided for ""computable"" relates to segments of infinite strings. However, most computing deals with finite entities, and even when computing involves irrational numbers, there is no need to compute all their digits. How is this definition of ""computable"" justified?

2. There is a countably infinite number of procedures. Given the limitations on size in the programming world, how can there be an infinite number of procedures? A large finite number is understandable, but infinite?

3. There are uncountably many infinite binary strings that are non-computable. It is true that there are uncountably many irrational numbers, which are mostly non-computable according to the previous definition. But what relevance does this have to the halting of real-world computers?

4. HALT is defined as a program that returns when given its own source code as input (refer to Slide 7). This definition of HALT is peculiar. Why is this specific input the only scenario considered?

5. A type-checking program C exists that operates solely based on the program text s. Is it possible for a perfect type-checking procedure to exist without any knowledge of the inputs to s? These explanations seem vague and forced. What crucial information am I overlooking?"
182,WQHOImO0pX0,"So what I mean is that the procedure applied to argument n will return the n-th digit of the string s.
That's the definition of what I mean by saying s is computable.
I can compute its digits, whichever digits are needed.
Now, we saw that there were only a countable number of finite binary sequences, and I mention that now because I want to think about sequences over the slightly larger alphabet instead of 0 1, the 256 ASCII characters.
And by the same argument, the set of finite ASCII strings is also countable.
You just list them in order of length-- same argument that we used for the binary strings.
Now, the point of looking at the ASCII strings-- the 256 keyboard characters-- is that every procedure that we enter into a computer, we type in an ASCII string.
Every procedure can be represented by an ASCII string.
And since they're only countably many finite ASCII strings, it follows that there are only countably many computable procedures.
Now, since in order to be a computable infinite string, there has to be a procedure which computes is digits, we can immediately conclude that there are only countably many infinite binary sequences that are computable-- only countably many computable infinite binary sequences.
But I already said there are an uncountable number of those infinite binary sequences.
So it has to be that there are noncomputable sequences, noncomputable infinite binary strings, that exist.
So we can conclude that as a matter of fact, since the set of infinite binary strings is uncountable and the computable ones are a countable subset, there have to be an uncountable number of noncomputable infinite binary sequences.
Most infinite binary sequences are actually noncomputable.
OK.
That's kind of abstract thing to know.","The assumptions presented in this lecture, which are listed below, appear to be nonsensical. I would appreciate some clarification. 

1. The concept of an infinite string being computable is introduced, yet the only definition of ""computable"" provided relates to segments of infinite strings. Typically, computation involves finite entities, and even when dealing with irrational numbers, there is no need to compute all their digits. How is this definition of ""computable"" justified?

2. The claim that there exists a countably infinite number of procedures is puzzling. Given the limitations on size in the programming world, how can there be an infinite number of procedures? A large finite number seems plausible, but infinite?

3. It is stated that there are uncountably many infinite binary strings that are non-computable. While it's true that there are uncountably many irrational numbers, most would be ""non-computable"" according to the previous definition. However, what relevance does this have to the actual halting of real-world computers?

4. The HALT operation is defined as a program that returns when given its own source code as input, as shown on slide 7. This definition is peculiar. Why is this specific input the only scenario considered?

5. The existence of a type-checking program C that operates solely based on the program text s is proposed. Is it possible for a perfect type-checking procedure to exist without any knowledge of the inputs to s? These explanations seem vague and forced. What crucial information am I overlooking?"
183,X0Jw4kgaFlg,"So if i doesn't equal j we're going to get a 0, and if i does equal j, well, then we're going to get the regular one variable derivative of the logistic function, which if I remember correctly, you were asked to compute-- now I can't remember whether it's assignment, 1 or assignment 2, but one of the two asked you to compute it.
So our Jacobian for this case looks like this.
We have a diagonal matrix with the derivatives of each element element along the diagonal and everything else is 0.
OK, so let's look at a couple of other Jacobians.
So if we are asking, if we've got this Wx plus b basic neural network layer and we're asking for the gradient with respect to x, then what are we going to have coming out is that that's actually going to be the matrix W.","The reasons why the non-diagonal elements of the Jacobian are equal to zero might be somewhat unclear. The derivatives at non-diagonal positions are zero because h1 is the output corresponding to the input z1 and is not affected by any other inputs. Therefore, h1 will only change with respect to dh1/dz1 when there is a change in z1, and it will remain unchanged (0) with respect to changes in other inputs, such as z2 or z3."
184,XROTP1RiNaA,"And we'll start with versions of SAT because we like SAT.
So I'm just going to tell you that sharp 3SAT is hard.
First sharp SAT is hard, and the usual proof of SAT hardness shows sharp P completeness for sharp SAT.
And if you're careful about the conversion from SAT to 3CNF you can get sharp three satisfied.
It's not terribly interesting and tedious.
So I will skip that one.
So what about Planar 3SAT? I stared at this diagram many times for a while.
This is lecture seven for replacing a crossing in a 3SAT thing with this picture.
And all of this argument and this table in particular, was concluding that the variables are forced in this scenario.","""Where is the lecture that contains the problems and reductions you began referring to earlier? Thank you."""
185,XhyOAX6oSX4,"And if you take a different sample, the hypothesis might look very different.
And it still doesn't fit your data at all.
It's just capturing the noise, but maybe hasn't captured the signal.
So it's weighs a lot depending on the sample that you get.
But still it doesn't go through it so that it- it is kind of overfitting and underfitting at the same time.
But these are just heuristics, right? So what- what you, um, what you want to do in general, um, what- what we've discussed so far is- is- is mostly for you to build a mental model of what's going on under the covers, okay? But what you actually do in practice, the actions that you take, is what we're going to discuss next, and that's called cross-validation.","Can we say that it is underfitting the signal and overfitting the noise? Also, is it considered a good model if it is underfitting the noise and overfitting the signal?"
186,XhyOAX6oSX4,"And that, that's called bootstrap.
However, bootstrap is generally used for um, bootstrap is- is generally used to obtain um- what -what we saw previously as the sampling distribution for getting an -an- uh- a distribution over your parameters, right? And uh, it's not commonly used in machine learning where the goal is to do prediction on unseen examples.
Uh- it's, it's more of a technique to get um- uncertainty estimates or confidence intervals on your model parameters.
And that's more commonly used in- in- in more classical statistics settings where you want to get confidence intervals on your parameters.
Um, it doesn't- it doesn't help us a lot in terms of getting generalization error estimates.
All right.
Regularization.
So first let's, uh- we gonna have a quick, um- a quick motivation for regularization.","You said that bootstrap isn't widely used in machine learning, but it forms the basis for random forests and other tree-based algorithms, correct?"
187,Xm5VrxZYhu4,"So it will be like two symmetric relations rather than a inverse relation.
So this is where, uh, DistMult fails [NOISE].
Um, and then the last place, uh, where, uh, DistMult fails is, uh, composition, uh, relations, right? And the problem is that DistMult defines hyperplane for, uh, each, uh, head, er, relation pair and the union of hyperplanes induced by this, er, by this composition of relations cannot be expressed by a single, uh, hyperplane.
So kind of, you know, a union or an intersection of hyperplanes is not a hyperplane, uh, anymore.
So that's why- th- that's the intuition why composition relations, uh, are also not possible.","Thank you for the lecture! I believe the proof that DistMul cannot model inverse relations is incorrect. We have <h, r2, t> = <t, r1, h> for specific h and t. Therefore, the equation suggests that r1 - r2 should belong to a specific hyperplane; however, this does not imply that r1 equals r2."
188,Xnpt8US31cQ,"And then again, you say, Aha, now I'm here, I wanna go deeper.
And you keep descending until you hit, uh, the, uh, individual cell, and this is when you, uh, when you stop and you put an edge.
Um, this means that you are going to land in a given, um, in a given cell exactly with the probability, um, uh, according to that cell.
The only difference is that you may get a couple of edges colliding, right? You may land to the same cell multiple times.
If that happens, just ignore it, uh, and try again.
And this gives you now a very fast way to generate a- a Kronecker graph, right? So, uh, basically, this, uh, edge-dropping or ball-dropping mechanism basically says, um, take the initia- initial, uh, matrix, whatever are the entries, uh, normalize them and then keep dropping in, um, in- keep descending in until you hit an individual cell and put an edge there, right? Put a value 1 there, and this would mean that you have connected nodes, uh, i and j that are in a column, uh, in the row i and column j.","Thank you for sharing. The Fast Kronecker Generator algorithm does not decrease the number of simulations required; instead, it conserves memory by avoiding the need to store a large probability matrix. This is achieved by recursively breaking down the levels from the initial probability matrix. Therefore, the primary benefit is memory savings rather than simulation speed, correct? The algorithm is considered 'fast' because we can halt at an intermediate level and use a single simulation as an approximation for the 'edge' or sub-matrix. Is my understanding accurate? Thank you once again for sharing this information."
189,Xv0wRy66Big,"And intuitively, the idea is that rather than summing here over all the nodes, uh, in the network, we are only going to sum over a subset of the nodes.
So essentially, we are going to sample a couple of negative examples and sum, uh, over them.
So the way the approximation works out is that, um, we, um, we can- we can view this as an approximation to the, um, to the- to the softmax function, where we can, uh, approximate it using, uh, the following, uh, expression.
We are going to take log sigmoid function of the dot product between u and v.
Uh, this is for the, uh, for the theorem here.
And then we are going to say minus sum of i going from one to k, so this is our k negative examples, logarithm, again, of the sigmoid function between the, uh, st- starting node u, and the negative, um, negative sample, uh, i, where this negative samples, this negative nodes will be, uh, sampled at random, but not at ra- at uniform random, but random in a biased way.
So the idea here is that, instead of normalizing with respect to all nodes in the network, we are going to normalize softmax against k random negative samples, so negative nodes, uh, from the network.
And this negative samples will be carefully chosen.
So how do we choose negative samples? We- we sample k negative nodes, each with probabil- probability proportional to its degrees.",Can you explain why a higher k in sampling corresponds to a higher bias towards negative events?
190,ZA-tUyM_y7s,"What does constant look like? Maybe this is 1,000 up here.
What does a constant look like? Looks like a line-- it looks like a line over here somewhere.
It could be as high as I want, but eventually, anything that's an increasing function will get bigger than this.
And on this scale, if I use log base 2 or some reasonable small constant, what does log look like? Well, let's do an easier one.
What does linear look like? Yeah, this-- that's what I saw what a lot of you doing.
That's linear.
That's the kind of base that we're comparing everything against.
What does log look like? Like this-- OK, but at this scale, really, it's much closer to constant than linear.",One of the axes on the graph represents n; what does the other one represent?
191,ZUZ8VbX1YNQ,"Now, the odds of finding an m that's not relatively prime to n are basically negligible because if you'd find such an m, it would enable you to factor them.
And we believe factoring is very hard.
But in fact, it actually works for all m, which is a nice theoretical results.
And you'll work this out in class problem.
OK.
That's how it works.
The receiver publishers e and n, keeps a secret key d.
The sender exponentiates the message to the power e.
The receiver simply decodes by raising the received message to the power d and reads off what the original was.
OK.
So we need to think about the feasibility of all of this because we believe that it's impossible to decrypt, but there's a lot of other stuff going on there that the players have to be able perform.
And let's examine what their responsibilities and abilities have to be.
So the receiver to begin with has to be able to find large primes.
And how on earth do they do that? Well, without going into too much detail, we can make the remark that there are lots of primes.","""Why can all m work if gcd(m, n) = m? What should we do?"""
192,ZUZ8VbX1YNQ,"That is to say by appealing to the prime number theorem, we know that among the n digit numbers, about log n of them are going to be primes so that you don't have to go too long before you stumble upon a random prime.
That is, if you're dealing with a 200 digit n and you're searching for a prime of around that size, you're not going to have to search more than a few hundred numbers before you're likely to stumble on a prime.
And of course, how do you know that you stumbled on a prime? Well, you need to be able to check whether a number is prime or not-- and efficientlY-- in order for this whole thing to be feasible.","Isn't it more correct to say that, for a range of integers x, x/ln(x) yields primes? I think there was a mistake there."
193,_3WDzxt5p8c,"You can check that P if and only if is true exactly when the complement of P XOR Q is true.
So now we come to two crucial properties of formulas called satisfiability and validity, and let's examine what those are.
So a formula is satisfiable if and only if it's true in some environment.
That is, it's satisfiable if there is some way to set the values of the variables P and Q to be truth values in such a way that the formula comes out to be true.
And a related idea is that a formula is valid-- it's also called a tautology-- if and only if it's true in all environments.
No matter what you set the variables to, it's going to come out to be true.
Let's look at some examples to solidify those two concepts.
So the formula P all by itself is satisfiable because it can be true if P is true, but it's not always true because P might be false.
Symmetrically, NOT P is also satisfiable because it could be true if P is false, but it's not always true.
It's not valid because P might be true, in which case not P would be false.
A formula that's not satisfiable, a formula that means that there is no truth value that makes it true, which is the same as saying that it's always false, is the formula P and NOT P.
It's probably the simplest not satisfiable formula or unsatisfiable formula.
So this is clearly false because either P or NOT P has got to be false and the [? and ?] will then will definitely come out to be false.
There is no value for P that makes this formula true.
It's unsatisfiable.
A valid formula, actually by De Morgan's law applied to the P and NOT P, is P or NOT P is going to be valid because no matter what truth value P has, it comes out to be true.","Is the term ""iff"" used here in the same way as it is in general mathematics, meaning that we need to prove ""if p is true, then q is true"" and also prove ""if q is true, then p is true""? Or are we at a stage where this is merely mentioned and we should use the ""definition""?"
194,_PwhiWxHK8o,"And now, we've got to have a summation over all the constraints.
And each or those constraints is going to have a multiplier, alpha sub i.
And then, we write down the constraint.
And when we write down a constraint, there it is up there.
And I've got to be hyper careful here, because, otherwise, I'll get lost in the algebra.
So the constraint is y sub i times vector, w, dotted with vector x sub i plus b, and now, I've got a closing parenthesis, a minus 1.
That's the end of my constraint, like so.
I sure hope I've got that right, because I'll be in deep trouble if that's wrong.
Anybody see any bugs in that? That looks right.
doesn't it? We've got the original thing we're trying to work with.
Now, we've got Lagrange multipliers all multiplied.
It's back to that constraint up there, where each constraint is constrained to be 0.
Well, there's a little bit of mathematical slight of hand here, because in the end, the ones that are going to be 0, the Lagrange multipliers here.
The ones that are going to be non 0 are going to be the ones connected with vectors that lie in the gutter.
The rest are going to be 0.
But in any event, we can pretend that this is what we're doing.
I don't care whether it's a maximum or minimum.","Why does Patrick Winston say that only the Lagrange Multipliers associated with vectors lying in the gutter will be non-zero, while the others will be zero?"
195,_PwhiWxHK8o,"And this is what the dot product is in the other space.
So that's one choice.
Another choice is a kernel that looks like this, e to the minus.
Let's take the dot product of the difference of those two guys.
Let's take the magnitude of that and divide it by some sigma.
That's a second kind of kernel that we can use.
So let's go back and see if we can solve this problem by transforming it into another space where we have another perspective.
So that's it.
That's another kernel.
And so sure, we can.
And that's the answer when transformed back into the original space.
We can also try doing that with a so-called radial basis kernel.
That's the one with the exponential in it.
We can learn on that one.
Boom.
No problem.
So we've got a general method that's convex and guaranteed to produce a global solution.","""Could someone please explain why the dot product is considered to be in another space? How does this occur? I would appreciate any advice on this matter. Thank you!""
The solution found doesn't seem to involve the concept of adding an extra dimension; rather, it appears to employ a non-linear approach, which, while simpler and visually appealing, leaves me confused."
196,_PwhiWxHK8o,"But if you're a positive sample, we're going to insist that this decision function gives the value of one or greater.
Likewise, if w thought it was some negative sample is provided to us, then we're going to say that has to be equal to or less than minus 1.
All right.
So if you're a minus sample, like one of these two guys or any minus sample that may lie down here, this function that gives us the decision rule must return minus 1 or less.
So there's a separation of distance here.
Minus 1 to plus 1 for all of the samples.
So that's cool.
But we're not quite done, because carrying around two equations like this, it's a pain.
So what we're going to do is we're going to introduce another variable to make like a little easier.
Like many things that we do, and when we develop this kind of stuff, introducing this variable is not something that God says has to be done.
What is it? We introduced this additional stuff to do what? To make the mathematics more convenient, so mathematical convenience.
So what we're going to do is we're going to introduce a variable, y sub i, such that y sub i is equal to plus 1 for plus samples and minus 1 for negative samples.
All right.
So for each sample, we're going to have a value for this new quantity we've introduced, y.
And the value of y is going to be determined by whether it's a positive sample or negative sample.
If it's a positive sample it's got to be plus 1 for this situation up here, and it's going to be minus 1 for this situation down here.","""Why only one, and why not two?""
""Why does it have to be 1 or greater? I mean, why specifically 1? Why must it be at least 1 to be considered beyond the margin?""
Could someone please explain to me why it is the way it is? I'm so lost on this step. Why did it change from ""greater than or equal to 0"" to ""greater than or equal to 1""?"
197,_PwhiWxHK8o,"I've lost track.
But what we're going to do is we're going to try to find an extremum of that.
So what do we do? What does 1801 teach us about? Finding the maximum-- well, we've got to find the derivatives and set them to 0.
And then, after we've done that, a little bit of that manipulation, we're going to see a wonderful song start to emerge.
So let's see if we can do it.
Let's take the partial of L, the Lagrangian, with respect to the vector, w.
Oh my God, how do you differentiate with respect to a vector? It turns out that it has a form that looks exactly like differentiating with respect to a scalar.
And the way you prove that to yourself is you just expand everything in terms of all of the vector's components.","Could someone clarify for me what he does with the Lagrange concept, or direct me to some accessible literature on the subject?"
198,_PwhiWxHK8o,"Is everybody relaxed, taking deep breath? Actually, this is the easiest part.
This is just doing a little bit of the algebra.
So the think we're trying to maximize or minimize is equal to 1/2.
And now, we've got to have this vector here in there twice.
Right? Because we're multiplying the two together.
So let's see.
We've got from that expression up there, one of those w's will just be the sum of the alpha i times y sub i times the vector x sub i.
And then, we've got the other one, too.
So that's just going to be the sum of alpha.
Now, I'm going to, actually, eventually, squish those two sums together into a double summation, so I have to keep the indexes straight.
So I'm just going to write that as alpha sub j, y sub j, x sub j.
So those are my two vectors and I'm going to take the dot product of those.","""Amazing lecture! Could anyone help me understand why the magnitude of w is the dot product of w1 and w2? That's not very clear to me (perhaps I'm missing something).""
Can someone explain what happens here? Where does the 'j' come from?
It seems that everyone understands everything, which is excellent. Therefore, if you are considering commenting to commend the lecturer, it implies you have a clear grasp of his explanations. If that's the case, please address these two questions:

1. The lecturer mentioned that x is projecting onto w. However, from my understanding, to project x onto w, the formula should be x.w/|w| (which excludes the direction of w), rather than simply w.x. Am I correct?

2. Around a certain minute, he states, ""and then we got the other one,"" and proceeds to add the sum(a_j y_j x_j). To which term is he referring? What does the index j represent? Also, why is w^2 expressed as wi.wj instead of wi.wi?

Thank you very much for your time. By the way, this lecture is superior compared to other tutorials.
Hello, he mentions ""and then we got the other one"" and subsequently adds the sum of (a_j y_j x_j). To which one is he referring? Also, what does the index j represent?
1. You are correct; however, the crux of the matter is that the decision rule solely relies on the vector w.x. The magnitude is merely a constant that can be incorporated into the constant b, correct?

2. In this context, the professor is discussing the vector w. The terms i and j serve merely as identifiers; both expressions are equivalent."
199,_PwhiWxHK8o,"Is it got something to do with Legendre? Has it got something to do with Laplace? Or does it have something to do with Lagrange? She says Lagrange.
Actually, all three were said to be on Fourier's Doctoral Defense Committee-- must have been quite an example.
But we want to talk about Lagrange, because we've got a situation here.
Is this 1801? 1802? 1802.
We learned in 1802 that if we going to find the extremum of a function with constraints, then we're going to have to use Lagrange multipliers.
That would give us a new expression, which we can maximize or minimize without thinking about the constraints anymore.
That's how Lagrange multipliers work.
So this brings us to miracle number four, developmental piece number four.
And it works like this.
We're going to say that L-- the thing we're going to try to maximize in order to maximize the width of the street-- is equal to 1/2 times the magnitude of that vector, w, squared minus.","I've researched Lagrange multipliers and such, but I'm still not quite certain about their application in this context. If anyone has a clear explanation, please share it.
Why did he subtract that term?"
200,_PwhiWxHK8o,"It's always got to be equal to or greater than 0.
But what I'm going to say is if we're for x sub i in a gutter.
So there's always going to be greater than 0, but we're going to add the additional constraint that it's going to be exactly 0 for all the samples that end up in the gutters here of the street.
So the value of that expression is going to be exactly 0 for that sample, 0 for this sample and this sample, not 0 for that sample.
It's got to be greater than 1.
All right? So that's step number two.
And this is step number one.","""Is 'zero in a gutter' referring to the bottom gutter or to both gutters? I didn't understand. Also, I didn't grasp the part where he takes half of 'w,' squares it, and then, instead of explaining the necessity, he simply says it's 'mathematically convenient' without providing any explanation."""
201,_PwhiWxHK8o,"Let's try this guy.
Oh.
What do you think? What happened here? Well, we're screwed, right? Because it's linearly inseparable-- bad news.
So in situations where it's linearly inseparable, the mechanism struggles, and eventually, it will just slow down and you truncate it, because it's not making any progress.
And you see the red dots there are ones that it got wrong.
So you say, well, too bad for our side-- doesn't look like it's all that good anyway.
But then, a powerful idea comes to the rescue, when stuck switch to another perspective.
So if we don't like the space that we're in, because it gives examples that are not linearly separable, then we can say, oh, shoot.
Here's our space.
Here are two points.
Here are two other points.
We can't separate them.
But if we could somehow get them into another space, maybe we can separate them, because they look like this in the other space, and they're easy to separate.
So what we need, then, is a transformation that will take us from the space we're in into a space where things are more convenient, so we're going to call that transformation phi with a vector, x.
That's the transformation.
And now, here's the reason for all the magic.
I said, that the maximization only depends on dot products.","The solution found doesn't seem to involve the concept of adding an extra dimension; rather, it appears to implement a non-linear ""road,"" which, although simpler and visually appealing, leaves me confused."
202,_PwhiWxHK8o,"OK? Well, we can do the same trick with x minus.
If we've got a negative sample, then y sub i is negative.
That gives us our negative w times dot over x sub i.
But now, we take this stuff back over to the right side, and we get 1 plus b.
So that all licenses to rewrite this thing as 2 over the magnitude of w.
How did I get there? Well, I decided I was going to enforce this constraint.
I noted that the width of the street has got to be this difference vector times a unit vector.
Then, I used the constraint to plug back some values here.
And I discovered to my delight and amazement that the width of the street is 2 over the magnitude of w.
Yes, Brett? STUDENT: So your first x plus is minus b, and x minus is 1 plus b.
PATRICK WINSTON: Yeah.
STUDENT: So you're subtracting it? PATRICK WINSTON: Let's see.
If I've got a minus here, then that makes that minus, and then, the b is minus, and when I take the b over to the other side it becomes plus.","""(1 - b) - (1 + b) equals -2b, not 2, correct?""
Hi, could anyone explain why the vector w disappeared? I think it should be 2*w/||w|| rather than 2/||w||.
Why is the w vector set to one, as expressed by the equation 2 / ||w vector||?
""If anyone else was confused, xPlus should actually be -1-b. Therefore, when you substitute, it becomes 1-b-(-1-b) = 2. However, I don't understand where the w vector disappears to; if someone could explain that, I would appreciate it.""
1. Not exactly. The term |w| will influence the equation. If we maintain it in the form y_i(x_i . w/|w| + b) - 1 = 0, then substituting those values into the width equation will yield a result of 2, rather than 2/|w|. I'm still uncertain about why it's acceptable to exclude |w|.
Hoenir Bhullar, are you suggesting that in the first equation, 'w' is a unit vector (thus, w/||w||)? That's the only expansion that comes to mind. However, if that's the case, how can one substitute 'w.x' in the equation with '.x'?
The original sentence appears to be a series of mathematical expressions rather than a coherent sentence. However, I can attempt to clarify and correct the notation used in the expressions while maintaining the original meaning. Here is a revised version:

""Consider the equation y_i (w  x_i - b) - 1 = 0. Subtracting 1 from both sides gives (w  x_i - b) - 1 = 0. Further simplifying, we have -w  x_i - b - 1 = 0, which leads to -w  x_i = 1 + b. Multiplying both sides by -1 gives w  x_i = -1 - b. Then, calculating (1 - b) - (-1 - b), we get 1 - b + 1 + b, which simplifies to 2.""

Please note that the original sentence seems to be a sequence of steps in an algebraic manipulation. The revised version above attempts to present these steps in a clearer and more grammatically correct format."
203,_PwhiWxHK8o,"STUDENT: Yeah, so if you subtract the left with the right [INAUDIBLE].
PATRICK WINSTON: No.
No, sorry.
This expression here is 1 plus b.
Trust me it works.
I haven't got my legs all tangled up like last Friday, well, not yet, anyway.
It's possible.
There's going to be a lot of algebra here eventually.
So this quantity here, this is miracle number three.
This quantity here is the width of the street.
And what we're trying to do is we're trying to maximize that, right? So we want to maximize 2 over the magnitude of w if we're to get the widest street under the constraints that we've decided that we're going to work with.","""Sorry, why does negative x become 1 plus b?"""
204,_PwhiWxHK8o,"Should we take a break? Should we get coffee? Too bad, we can't do that in this kind of situation.
But we would if we could.
And I'm sure when Vapnik got to this point, he went out for coffee.
So now, we back up, and we say, well, let's let these expressions start developing into a song.
Not like that, that's vapid, speaking of Vapnik.
What song is it going to sing? We've got an expression here that we'd like to find the minimum of, the extremum of.
And we've got some constraints here that we would like to honor.
What are we going to do? Let me put what we're going to do to you in the form of a puzzle.","I became confused when the lecturer insisted that the equation y_i(x_i * w + b) - 1 = 0 represents ""the constraints."" Such an equation cannot serve as constraints because it implies that ""the solution (w) must ensure all training samples lie within the gutter,"" a condition no solution can fulfill. I believe the actual constraints should be the inequality y_i(x_i * w + b) - 1 >= 0, meaning ""the solution (w) must allow all training samples to be on or beyond the gutter's edge."" Our goal is then to find a vector w that creates the widest possible margin while adhering to these constraints. The equation y_i(x_i * w + b) - 1 = 0 is only satisfied by the training samples that lie exactly in the gutter, which demonstrates that the formula for calculating the street width is 2/||w||. However, that is the extent of its applicationit does not define the constraints."
205,_PwhiWxHK8o,"So now, if I only had a unit normal that's normal to the median line of the street, if it's a unit normal, then I could just take the dot product or that unit normal and this difference vector, and that would be the width of the street, right? So in other words, if I had a unit vector in that direction, then I could just dot the two together, and that would be the width of the street.
So let me write that down before I forget.
So the width is equal to x plus minus x minus.
OK.
That's the difference vector.
And now, I've got to multiple it by unit vector.
But wait a minute.
I said that that w is a normal, right? The w is a normal.
So what I can do is I can multiply this times w, and then, we'll divide by the magnitude of w, and that will make it a unit vector.",Can someone clarify how the calculation of the unit normal and the difference vector determines the width?
206,_PwhiWxHK8o,"So now, my decision rule with this expression for w is going to be w plugged into that thing.
So the decision rule is going to look like the sum of alpha i times y sub i times x sub i dotted with the unknown vector, like so.
And we're going to, I guess, add b.
And we're going to say, if that's greater than or equal to 0, then plus.
So you see why the math is beginning to sing to us now.
Because now, we discover that the decision rule, also, depends only on the dot product of those sample vectors and the unknown.
So the total of dependence of all of the math on the dot products.
All right.
And now, I hear a whisper.
Someone is saying, I don't believe that mathematicians can do it.
I don't think those numerical analysts can find the optimization.
I want to be sure of it.
Give me ocular proof.
So I'd like to run a demonstration of it.
OK.
There's our sample problem.
The one I started the hour out with.
Now, if the optimization algorithm doesn't get stuck in a local maximum or something, it should find a nice, straight line separating those two guys to finding the widest street between the minuses and the pluses.
So in just a couple of steps, you can see down there in step 11.
It's decided that it's done as much as it can on the optimization.
And it's got three alphas.
And you can see that the two negative samples both figure into the solution, the weights on the Lagrangian multipliers are given by those little yellow bars.
So the two negatives participate in the solution as one of the positives, but the other positive doesn't.","I am unclear about something. How can you determine which samples are support vectors? In other words, how do you know which alpha values are zero and which are not? Perhaps I need to understand Lagrange multipliers better to resolve this issue.
""The guy seated on the right side: how did he come up with this?""
""Does anyone know how to derive 'alpha' and 'b' in the Decision Function()?"""
207,_PwhiWxHK8o,"So that dot product, not a product, that dot product is, in fact, a scalar, and it's the width of the street.
It doesn't do as much good, because it doesn't look like we get much out of it.
Oh, but I don't know.
Let's see, what can we get out of it? Oh gee, we've got this equation over here, this equation that constrains the samples that lie in the gutter.
So if we have a positive sample, for example, then this is plus 1, and we have this equation.
So it says that x plus times w is equal to, oh, 1 minus b.
See, I'm just taking this part here, this vector here, and I'm dotting it with x plus.
So that's this piece right here.
y is 1 for this kind of sample.
So I'll just take the 1 and the b back over to the other side, and I've got 1 minus b.","I'm having trouble understanding; what is the purpose of the unit vector W, and why do we use it as such? I thought that simply projecting (X+ - X-) onto the normal vector W, as previously seen, would suffice without the need for normalization.
I became confused when the lecturer insisted that the equation y_i(x_i * w + b) - 1 = 0 represents ""the constraints."" Such an equation cannot serve as constraints because it implies that ""the solution (w) must ensure all training samples lie within the gutter,"" a condition no solution can fulfill. I believe the actual constraints should be the inequality y_i(x_i * w + b) - 1 >= 0, meaning ""the solution (w) must allow all training samples to be on or beyond the gutter's edge."" Our goal is then to find a vector w that maximizes the street's width while adhering to these constraints. The equation y_i(x_i * w + b) - 1 = 0 is only satisfied by the training samples that lie precisely in the gutter, demonstrating that the street's width is 2/||w||. However, this is the extent of its utilityit does not represent the constraints."
208,_PwhiWxHK8o,"So that's minus sub of alpha i times y sub i times b.
And then, to finish it off, we have plus the sum of alpha sub i minus 1 up there, minus 1 in front of the summation, such as the sum of the alphas.
Are you with me so far? Just a little algebra.
It looks good.
I think I haven't mucked it, yet.
Let's see.
alpha i times y sub i times b.
b is a constant.
So pull that out there, and then, I just got the sum of alpha sub i times y sub i.
Oh, that's good.
That's 0.
Now, so for every one of these terms, we dot it with this whole expression.
So that's just like taking this thing here and dotting those two things together, right? Oh, but that's just the same thing we've got here.
So now, what we can do is we can say that we can rewrite this Lagrangian as-- we've got that sum of alpha i.
That's the positive element.
And then, we've got one of these and half of these.
So that's minus 1/2.","The revised sentence could be: ""The entire equation was associated with subscript i, by far the most significant part, so why do we assume that some summations are different with subscript j?"""
209,_PwhiWxHK8o,"So what we're going to do with this first equation is we're going to multiply it by y sub i, and that is now x of i, plus b is equal to or greater than 1.
And then, you know what we're going to do? We're going to multiply the left side of this equation by y sub i, as well.
So the second equation becomes y sub i times x sub i plus b.
And now, what does that do over here? We multiplied this guy times minus 1.
So it used to be the case that that was less than minus 1.
So if we multiply it by minus 1, then it has to be greater than plus 1.
The two equations are the same, because that introduces this little mathematical convenience.
So now, we can say that y sub i times x sub i plus b.
Well, what we're going to do-- Brett? STUDENT: What happened to the w? PATRICK WINSTON: Oh, did I leave out a w? I'm sorry.
Thank you.
Yeah, I wouldn't have gotten very far with that.
So that's dot it with w, dot it with w.
Thank you, Brett.
Those are all vectors.
I'll pretty soon forget to put the little vector marks on there, but you know what I mean.
So that's w plus b.
And now, let me bring that 1 over to the left side, and that's equal to or greater than 0.
All right.
With Brett's correction, I think everything's OK.
But we're going to take one more step, and we're going to say that y sub i times x sub i times w plus b minus 1.","How does multiplying by yi yield that equation? Can anyone explain?
The professor equated the two equations, but he didn't multiply the left side of the equation by -1. I'm really confused.
""Hello, I have a question regarding the step he takes at : He transitions from [...]  0 to [...] = 0 and then states that the second equation confirms the points that lie within the gutter. However, the multiple potential projections of \vec{x} onto \vec{w} could yield different scalar values depending on \vec{x} and its position relative to the road's center. Assuming b is constant, how can all projections of vectors originating from within the gutter equal 0? Shouldn't the equation only hold true for \vec{x} located at the gutter's edge?"""
210,_PwhiWxHK8o,"So what we're going to say is this, that if we look at this quantity that we're checking out to be greater than or less than 0 to make our decision, then, what we're going to do is we're going to say that if we take that vector w, and we take the dot product of that with some x plus, some positive sample, now.
This is not an unknown.
This is a positive sample.
If we take the dot product of those two vectors, and we had b just like in our decision rule, we're going to want that to be equal to or greater than 1.
So in other words, you can be an unknown anywhere in this street and be just a little bit greater or just a little bit less than 0.","Could someone please explain why, for positive samples, wx+b is greater than or equal to 1?
Could someone please explain to me why it is the way it is? I'm so lost on this step. Why did it change from ""greater than or equal to 0"" to ""greater than or equal to 1""?
Why is it set to be >=1 and <= -1, rather than 2, -2, or 10, -10? What is special about the number 1?
Hello everyone. I have a question: Why is the value assigned as +1 or -1 when the sample is positive or negative, respectively? Shouldn't it be a small positive real number, such as +delta? (By ""delta,"" I am referring to a small positive real number.)
Why do we choose the quantity to be greater than 1? Is there a motivation for this, or could it be any number and the derivation would still remain valid?"
211,_PwhiWxHK8o,"Some of the vectors in the sample set, and I say some, because for some alpha will be 0.
All right.
So this is something that we want to take note of as something important.
Now, of course, we've got to differentiate L with respect to anything else it might vary, so we've got to differentiate L with respect to b, as well.
So what's that going to be equal to? Well, there's no b in here, so that makes no contribution.
This part here doesn't have a b in it, so that makes no contribution.
There's no b over here, so that makes no contribution.
So we've got alpha i times y sub i times b.","""Oh, is 'b' a variable? How is that possible? Isn't 'b' a constant value beyond which samples are positive? Why would he take the derivative with respect to 'b'?""
The perpendicular vector w appears to be the ""average"" of all the data point ""vectors."" Could that be the eigenvector? Just a speculative guess."
212,_PwhiWxHK8o,"We don't know anything about it's length, yet.
Then, we also have some unknown, say, right here.
And we have a vector that points to it by excel.
So now, what we're really interested in is whether or not that unknown is on the right side of the street or on the left side of the street.
So what we'd what to do is want to project that vector, u, down on to one that's perpendicular to the street.
Because then, we'll have the distance in this direction or a number that's proportional to this in this direction.
And the further out we go, the closer we'll get to being on the right side of the street, where the right side of the street is not the correct side but actually the right side of the street.
So what we can do is we can say, let's take w and dot it with u and measure whether or not that number is equal to or greater than some constant, c.
So remember that the dot product has taken the projection onto w.
And the bigger that projection is, the further out along this line the projection will lie.","""Sir, is that what is called an eigenvector?"""
213,_PwhiWxHK8o,"You differentiate those with respect to what you're differentiating with respect to, and everything turns out the same.
So what you get when you differentiate this with respect to the vector, w, is 2 comes down, and we have just magnitude of w.
Was it the magnitude of w? Yeah, like so.
Was it the magnitude of w? Oh, it's not the magnitude of w.
It's just w, like so, no magnitude involved.
Then, we've got a w over here, so we've got to differentiate this part with respect to w, as well.
But that part's a lot easier, because all we have there is a w.
There's no magnitude.
It's not raised to any power.",Why was ||w|| transformed into the vector w?
214,a1RaIqkdG0c,"And then, I just say the best time to end the party is at this time.
Time, which was discovered here, and the number of celebrities who are going to be attending is max count.
OK.
So this is essentially the code for the algorithm.
And this thing here is doing the slightly more relaxed check that I alluded to, which was like when you have 6:00, you're not just looking for 6:00 here, you're looking for 6:00 within this interval that looks like this.
So 6:00 is definitely within this, but it would not be-- 6:00 is not inside 5:00 and 6:00 as I described to you before, right? So that's simply the check.
That's why you have a less than equal to here and a strictly greater than over here, just to take care of that closed and open part of it.
So again, if you don't understand every nuance in this code, it's not that big a deal.
But hopefully you have the overall picture, right? Makes sense, right? So as you can imagine, there's a much better way.
There's a much better way that isn't as exhaustive as this one.
And do people have a sense of what a better way would be, or would you like a hint? And anyone want to conjecture a different way of solving this problem that's-- yeah, go ahead.
AUDIENCE: The exhaustiveness of this algorithm? SRINI DEVADAS: Ah, so the exhaustiveness of this algorithm is as follows.
So basically what I'm saying is-- what I'm saying is I'm going to go look at the range of times.
And all of these are per hour, right? Everything is per hour.
I'm going to look at the range of times, and I'm going to go ahead and include 6:00 through 12:00 here.
And I'm going to go 6:00, and I'm going to go figure out at 6:00 how many celebrities exist.
And then at 7:00, how many celebrities exist, and then at 8:00 all the way to 12:00, right? And I'm going to get in count, which is that list there, I'm going to get for each-- for count 6:00, I'm going to say two.","No matter how I think about it, the code is incorrect; there should be an ""or"" instead of an ""and"" in the loop where it checks if the celebrity is within the time range. This is because if she is already inside, she is counted, and if she is going to leave within that time frame, she is also counted. Therefore, I can't understand why there is an ""and"" instead of an ""or."""
215,aDmFyu0Yt7s,"The decision question is does white win? Now we end up with a tie in the case white doesn't win.
You can also change that by having a long path of blacks.
And black is going to sit there flipping, trying to get to flipping their edge.
And the length of the wire is exactly how long it takes to fill in all these things.
Yeah.
AUDIENCE: The reason why they have that gadget there at the end is you need to set exactly a certain number of them true? Before black can, and so-- PROFESSOR: Oh.
I see.
Right.
Yeah.
That's a little more subtle.
OK.
Zero minutes remaining.
The same crossover works.
And you can also build a protected OR.
Protected OR is where you never have both of these coming in.
If you allow me the notion of a free edge, which is just floating there and can be reversed whenever you want, this is a white edge, then this will act as a OR, because this choice gadget can only go one way or the other.
It's only one of the inputs can actually flip one of these edges, and then the OR will just take it.
So we can build predicted ORs.
And then I have a bunch of PSPACE hardness proofs based on bounded constraint logic, but I suggest we wait til Thursday to see this.
Because they're kind of fun.
And I'd rather spend a little time going through the proof details.
So you'll have to wait in suspense before we get to Amazons, which is a very fun-- you should play Amazons meanwhile, if you get a chance.
It's very fun if you have a chessboard around.
And Konane, which is an ancient, at least 300-year-old game from Hawaii, and Cross Purposes, which is a modern game.","Could someone clarify that comment? It appears to be useful, but I'm not sure what purpose the gadget serves."
216,aIsgJJYrlXk,"It's either odd or even, that's two.
And then my current city.
And I have n possible options for my current city.
It could be city one, city two, city three, so that's n.
So I have n options here.
I have two options here.
That's why I'm saying my whole state space is two times n, okay? All right.
Okay.
So let's try out this example.
Let's not put it in.
Uh, just talk to your neighbors about this, and then maybe, if you have ideas just let me know in a minute.
So- okay.
So what is the difference here? So we're traveling from city one to city n, and then the constraint is changed.
Now, we want to visit at least three odd cities.
So that's what we wanna do.
And then the question is, what is the minimal state? Talk to your neighbors.
[NOISE] All right.
Any ideas? Any ideas? [BACKGROUND] What is a possible state? Like it- don't worry about the minimal even, like for now.
Like what do I need to keep track of? Number of odd cities.
Number of, number of odd cities? Yeah.
Okay.
So- and is that it? Do I need to just know the number of odds cities? Um, or number of odd is about your, uh, [OVERLAPPING] So number- so, so what I meant is I also need to have current city, right? So, okay.",Thank you! How do you get 2N options if the graph is acyclic?
217,aIsgJJYrlXk,"Yes? Do you not also need an option for zero odd cities specific to [inaudible] Zero.
We're starting from city one, so we're already counting that in, but yeah, like, if you have zero odd cities, that is a good point too.
All right.
So I've gotta move.
Okay, so, um, that was that.
This is how it looks like.
Like you can think of your state space like this again as a tuple of I visited one, two, three, and- and then the cities.
I have another example here, you can think about this later and yeah, like, work, work it at home.
But, uh, basically the question is, again, you're going from city one to N, and you want to visit more odd cities than even cities.
What would be the minimal state space? But we can talk about it offline.
So the summary so far, is- is that state is going to be a summary of past actions sufficient to choose future actions optimally.
And then dynamic programming, it's not doing any magic, right, it's using this notion of state to bring down this exponential time algorithm to a polynomial time algorithm, and then, with the trick of using memoization, and with a trick of choosing the right state, okay? And we have talked about dynamic programming and how it doesn't work for acyclic graphs.
And now, we want to spend a little bit of time talking about uniform cost search, uh, and how that can help with the- with the cycles.
So if you guys have seen Dijkstra's algorithm, this is very similar to Dijkstra's, like, yeah.
So- so it's basically Dijkstra's.
But- all right.
So let's- let's actually talk about this.
So- so the observation here is that when we- when we think about the cost of getting from start state to some s prime, well, that is going to be equal to cost of going from s to s prime and then some past cost of s, okay.",Is there a better state for the second dynamic programming question than \( 2 \times n^2 \)?
218,auK3PSZoidc,"OK, that make sense? Good.
So here's the core routine that corresponds to the search.
And ignore this global variable here.
I'll explain that in a minute.
That's going to be our metric.
Backtracks is going to be our metric for computing performance.
And it's going to be quite interesting.
It's going to produce some interesting results for us when we run this on various different examples.
But this core procedure looks a lot like the n-queens search in the sense that you have a for loop and a recursive call.
And in this case the for loop is going to be something that ranges through the different values, that you find a location that you want to put something into, which is the next empty location in your current configuration.","In the solveSudoku function, it instructs to return true within the for loop, as well as to return false. I don't understand who is receiving the true value. Additionally, I'm confused about the process when a mistake is made. For example, if you initially fill in a 3 in the first cell, but the correct solution should be a 9, and the error is only detected after filling in 10 cells, how does the function determine where to backtrack? Does it start over at 1? If so, wouldn't you encounter the same issue again? This part is unclear to me."
219,b0HvwszmqcQ,"And we're gonna see just one example of the product rule.
So what does the, uh, gradient with respect to x of x transpose Ax, right? We're gonna apply the product rule as we know it.
So this is gonna be the gradient with respect to, um, let me use two colors to- to, um, right? Or precisely, right? Treat it as a product of two things and, you know, um, ah, ah, by them, uh, uh, differently.
And this is going to, um, come out to be, um, um, so here the- the gradient here is, um, you can think of this as gradient of x with, sorry, I forgot a transpose here, right? So, yeah, so this is gonna be, um, just Ax d transpose- Ax, and this is going to be, um, A transpose x, right? So, um, the- this is gonna be x times A plus A transpose.
And if A is- is, uh, uh, symmetric, this is going to be just 2Ax, right? A few more, um, matrix derivates which are gonna be very useful.
Yeah.
[BACKGROUND].
Uh, I'm sorry.
What's the- what's the question.
[BACKGROUND].
Oh, this is the product rule, so for example, um, um, when you are taking, um, so d by dx of f of x, g of x is equal to d by dx of f of x times g of x plus f of x times d by dx of g of x, right? This is the product rule, and this is the multivariate version of that.","A very basic question arises regarding the product rule at a certain timestamp: why does the second term result in A^T x and not x^T A? Is this because both are equivalent and it is written this way to maintain symmetry with the first term, which suggests that x is a row vector?"
220,b0HvwszmqcQ,"It- it- goes down along one axis and go- and goes up on another axis, right? So the- the- the connection between multi-variable calculus and linear algebra is- are- are very deep and- and you're going to- you're going to be using an analyzing Hessians of different loss functions to kind of characterize the convex, which means if they are convex, you know, that is good news.
It means the- when you minimize the loss function, you reach a unique global minimum, because if, you know, any bowl-shaped function or, you know, take any bowl, there's always like a unique, uh, global minimum, whereas if- if- if a function is- is not convex or if it has, you know, uh, uh, saddle points, then, you know, optimization is- is- is a little harder, trying to minimize the loss function is going to be harder, right? Now, some examples of, uh, how we actually go about calculating, uh, gradients.
Uh, for example, if you have a function x, the gradient of the function with respect to x is written like this.
This- this is the terminology that we generally use, the inverted, um, uh, triangle, um, is- is- denotes the gradient symbol.
And if you- if you are writing your homeworks in LaTeX, then this, you know, to get the symbol in LaTeX, you use backslash nabla, um, and, um, the- the- the, uh, uh, uh, subscript for nabla indicates what is the variable with respect to which you are, um, um, taking the, um, um, gradient and, you know, for gradients, this is going to be vector-valued, obviously, and the definition is just this.","""Hello, I have a brief question regarding the timestamp provided: Why do we use the Hessian matrix to determine the shape of a function? I understand that a positive semi-definite matrix corresponds to a bowl shape and is always positive. However, why is it specifically the Hessian that determines these properties of a function? Couldn't there be other matrices associated with a function that serve a similar purpose?"""
221,bQI0OmJPby4,"And we can even go to a level of specificity for pianos by saying we've got a Bosendorfer.
Why is a Bosendorfer special? I mean, is it like a Baldwin? Something's [INAUDIBLE], Yoka-- Lots of piano types, what's special? You see, you don't know because unless you play the piano, and probably unless you're a serious piano player you don't know that a Bosendorfer-- Ariel, you know.
STUDENT: I think its supposed to have an extra octave at the bottom, black keys.
Pretty cool.
PATRICK WINSTON: It's got some extra keys at the bottom.
And most people don't know that unless they're serious about the piano.
Some professional piano players, when they're confronted with a Bosendorfer have to have someone cover those keys because it screws up their peripheral vision, and hit the wrong key.
Because they're not used to having those extra keys at the bottom.
So that's a little detail but the Bosendorfer.
So you can make a kind of graph, and you can say, let's go from low, very general, to a basic level, to a specific level.
So it is the case in human knowledge that that graph has a tendency to look sort of like this.
So here's tool, here's hammer, here's ball peen.
So that level, where you have a big jump, that's the general to basic level of transition.
So that basic level is probably there because that's the level on which we hang a huge amount of our knowledge.
We know a lot about pianos, and it all seems to be hanging on that word piano, which gives us power with the concept.",What does the vertical axis represent on the graph?
222,bVjCjdq06R4,"You can possibly even just pretrain on the target data set itself.
So this is kind of breaking the common knowledge, or the common wisdom, here.
If you have a supervised pretraining task, then this won't work out, because you-- like running supervised learning on your fine tuning data set and then running fine tuning on your fine tuning data set, those are going to be the same exact thing.
So this is-- I would expect to only hold in the unsupervised pretraining case.
But it's something that breaks the common wisdom and suggests that we don't have everything figured out, even when it comes to fine tuning.
Yeah.
[INAUDIBLE] This is only in stages versus can you-- with this-- you're asking if this would also hold in the multi task setting? [INAUDIBLE] I think that they only ran experiments in the pretraining and fine tuning phase.
But you could read the paper to check.
And because it came out last week, I don't think anyone has built on it yet.
I should mention that this is averaged over a number of different-- they ran this on a number of different target tasks.
This was all in the NLP domain.
But there's actually another paper that came out actually a little bit before this that actually showed a similar result in computer vision tasks, as well.
Now the second paper is actually a paper that was co-authored by Yoonho and others-- Yoonho is a TA in the class.","Hello, thank you for the great content! I was wondering about the other paper you mentioned that involves pre-training on the downstream dataset, but within the vision domaincould you please specify which one it was?"
223,buzsHTa4Hgs,"So 4,345, uh, 4, um, er, where is, uh, er, uh, 34- uh, 345- um, is, um, layer hashes into color 10s, so we replace a color 10, uh, here.
And we could keep iterating this and we would come up, uh, with, uh, more and more, uh, refinement, uh, of the, uh, uh, colors of the graph.
So now that we have run this color refinement for a, uh, a fixed number of steps, let say k iterations, the Weisfeiler-Lehman, uh, kernel counts number of nodes with a given color.
So in our case, we run- we run this three times, so we have 13 different colors..
And now the feature description for a given graph is simply the count, the number of nodes of a given color, right? In the first iteration, uh, all the nodes were colored, um- all six nodes were colored the same one- uh, the same way.
Um, so there is six instances of color 1.
Then, um, after we iter- um, agg- aggregated the colors and refined them, you know, there were two nodes of color 2, one node of color 3, two nodes of color 4, um, and so on.
So here is now the feature description in terms of color counts, uh, for, uh, for, uh, different colors for the first graph and different colors, uh, for the second graph.
So now that we have the feature descriptions, the Weisfeiler-Lehman graph kernel would simply take the dot product between these two, uh, uh, feature descriptors and return a value.","Is there an obvious error in the color counts on the graph? It appears that the number 11 occurs twice, 12 occurs once, and 13 is absent in the first graph. Consequently, I have calculated the WL kernel value to be 50."
224,cjTs7lH5eUs,"So these are maybe yeah, two.
This will be our d train for task i, maybe we sampled task i.
And this is going to be d test for task i.
Now one thing that we're missing here is we're also missing labels.
We need labels in our training set and test set.
And so what we're going to do is we're also going to assign labels to each one of these.
So we can say labels zero, one, two.
It's important to have consistent labels across our characters.
OK.
And now we get to the fun part.
So what we're going to do is we are going to pass our training data set into a neural network that will implement the learning process.
And so in terms of passing this into a neural network, these data sets may have a number of different examples.
And so things like a recurrent neural network are typically a good choice or a transformer, if you want to be a little more into the times.
And so you'll pass this into-- your training data points into a recurrent neural network.
And at this point, we're going to get to the two different versions.
So one version of black box meta learning will take as input these training data points and output a set of parameters.","Thank you very much for this! What remains unclear to me is what she means when she says ""we pass our training dataset to the network."" Does this include passing the labels along with the inputs, perhaps concatenated? The labels must be utilized in some manner, correct?"
225,cjTs7lH5eUs,"So this would be like our training set and our test data set from all of the examples that we have for a particular task.
I should mention maybe here that it's worthwhile to kind of mix and match these.
So you shouldn't always use this as your test examples and always use this as your training examples.
You can get a little bit more data for meta-training by randomly sampling what you use as a training example and what you use as a test example.
And so kind of visually what this looks like is, maybe these are all the examples you have for one of your tasks.
You want to allocate this into training examples and test examples.
And so you'll kind of just randomly assign them to a training set and do a test set.","In the original context, I couldn't understand how the training data were being utilized for the model's training. The x-test and y-test are used for calculating the log-likelihood, but how is the training data actually employed?"
226,dARl_gGrS4o,"So let's see if we can do it with nine.
Let's see if we can do it with eight.
Let's see if we can do it with seven.
These take almost zero time, right? Because they're under constraint.
Wow, that's good, seven.
Let's try six.
Actually let's try two.
It loses fast.
Let's try three.
I don't know.
Maybe if you let it run one long enough three will work.
I doubt it.
While we're at it though, we might as well go back here and try it with six.
Remember seven worked real fast.
Gees, six, that was six, right? Yeah.
So let's try it with five.
OK.
So it runs real fast with five.
It terminates real quick with two, so we got three and four left.
So we could tell our boss, a la any time algorithm, that you're not real sure, but you know it's going to be either three or four.
And then, you got two computers.
You can let both run and see if either one terminate.
So you have three and four.
My guess is that three will eventually give up.
But of course, there's another little problem here.","The sentence can be revised for clarity and grammatical correctness while preserving the original meaning as follows:

""It is stated that the value could be 3 or 4, but I don't believe we can be certain of that; we don't know if 4 will terminate. Therefore, I think the most accurate we can say is that it could be 3, 4, or 5. Have I overlooked anything?"""
227,dRIhrn8cc9w,"Here we can think of there as just being a value function and you're just sort of updating one entry of that value function depending on which state you just reached.
So there's not kind of this nice notion of the whole previous value function of any value function.
I'll keep that there just for that reason.
Now, people often talk about the TD error, the temporal difference error.
What that is is it's comparing what is your estimate here.
So, your new estimate, which is your immediate reward plus gamma times your value of the state you actually reached minus your current estimate of your value.
Now, notice this one should have been sort of essentially approximating the expectation over S prime.
Because for that one you're going to be averaging.
And so this looks at the temporal difference.
So this is saying how different is your immediate reward plus gamma times your value of your next state, versus your sort of current estimate of the value of your current state.
Now note that that doesn't have to go to zero because that first thing is always ever just a sample, it's one future.
The only time this would be defined to go to zero is if this is deterministic, so there's only one next state.
So, you know, if half the time when I try to drive to the airport I hit traffic and half the time I don't, then that's sort of two different next states that I could go to for my current start state, either hit traffic or don't hit traffic.
Um, and so I'm either going to be getting that v pi of hitting traffic or v pi of not hitting traffic.
So this TD error will not necessarily go to zero even with infinite data because one is an expected thing from the current state and the other is which actual next state did you reach.",Should V^{pi}(s_{t}) be approximated over s instead of s'?
228,eg8DJYwdMyg,"I'm creating a function a new function called KNN.
This will be a function of two arguments, the training set and the test set, and it will be K nearest classifier with training set and test set as variables, and two constants, survived-- so I'm going to predict who survived-- and 3, the K.
I've been able to turn a function of four arguments, K nearest classify, into a function of two arguments KNN by using lambda abstraction.
This is something that people do fairly frequently, because it lets you build much more general programs when you don't have to worry about the number of arguments.
So it's a good trick to keeping your bag of tricks.
Again, it's a trick we've used before.
Then I've just chosen 10 for the number of splits, and we'll try it, and we'll try it for both methods of testing.
Any questions before I run this code? So here it is.
We'll run it.
Well, I should learn how to spell finished, shouldn't I? But that's OK.
Here we have the results, and they're-- well, what can we say about them? They're not much different to start with, so it doesn't appear that our testing methodology had much of a difference on how well the KNN worked, and that's actually kind of comforting.","Wouldn't it be more efficient to define 'label' and 'k' as keyword arguments rather than constructing a separate knn function through lambda abstraction? He discusses the development of much more general programs, but he has created two functions where a single, more versatile one could suffice, which would be more general than having two separate functions."
229,eg8DJYwdMyg,"So we worry about how big K should be.
And if we make it too big-- and this is a crucial thing-- we end up getting dominated by the size of the class.
So let's look at this picture we had before.
It happens to be more red dots than black dots.
If I make K 10 or 15, I'm going to classify a lot of things as red, just because red is so much more prevalent than black.
And so when you have an imbalance, which you usually do, you have to be very careful about K.
Does that make sense? AUDIENCE: [INAUDIBLE] choose K? PROFESSOR: So how do you choose K? Remember back on Monday when we talked about choosing K for K means clustering? We typically do a very similar kind of thing.
We take our training data and we split it into two parts.
So we have training and testing, but now we just take the training, and we split that into training and testing multiple times.
And we experiment with different K's, and we see which K's gives us the best result on the training data.
And then that becomes our K.
And that's a very common method.
It's called cross-validation, and it's-- for almost all of machine learning, the algorithms have parameters in this case, it's just one parameter, K.
And the way we typically choose the parameter values is by searching through the space using this cross-validation in the training data.",Why is the data for k-nearest neighbor training separated into testing and training sets again?
230,eg8DJYwdMyg,"You're not looking for a real number, you're looking for will they get sick, will they not get sick.
Maybe you're trying to predict the grade in a course A, B, C, D, and other grades we won't mention.
Again, those are labels, so it doesn't have to be a binary label but it's a finite number of labels.
So here's an example to start with.
We won't linger on it too long.
This is basically something you saw in an earlier lecture, where we had a bunch of animals and a bunch of properties, and a label identifying whether or not they were a reptile.
So we start by building a distance matrix.
How far apart they are, an in fact, in this case, I'm not using the representation you just saw.
I'm going to use the binary representation, As Professor Grimson showed you, and for the reasons he showed you.
If you're interested, I didn't produce this table by hand, I wrote some Python code to produce it, not only to compute the distances, but more delicately to produce the actual table.
And you'll probably find it instructive at some point to at least remember that that code is there, in case you need to ever produce a table for some paper.
In general, you probably noticed I spent relatively little time going over the actual vast amounts of codes we've been posting.","I believe the statement is incorrect. Contrary to what is claimed, he is indeed incorporating the full representation, including the number of legs. However, if you calculate using only the binary representation, the resulting distance matrix is inaccurate. To clarify, they are not factoring in the number of legs; instead, they mistakenly inserted a '2' for the final binary data element pertaining to chickens, when it should be a '0'. I ran my algorithm with this corrected number and obtained the same outcome as depicted in the video. Furthermore, shouldn't the last binary feature represent 'reptile'? In the Python dataset, the last element is '0' for several reptile instances. If there's something obvious I'm overlooking, please inform me."
231,esmzYhuFnds,"And so for the reptiles, he said, well, OK, we'll just make it a binary variable.
But maybe we don't want to make weight a binary variable, because maybe it is something we want to take into account.
So what we do is we scale it.
So this is a method called z-scaling.
More general than just making things 0 or 1.
It's a simple code.
It takes in all of the values of a specific feature and then performs some simple calculations, and when it's done, the resulting array it returns has a known mean and a known standard deviation.
So what's the mean going to be? It's always going to be the same thing, independent of the initial values.
Take a look at the code.
Try and see if you can figure it out.
Anybody want to take a guess at it? 0.
Right? So the mean will always be 0.
And the standard deviation, a little harder to figure, but it will always be 1.",Why is the mean 0 and the standard deviation 1?
232,esmzYhuFnds,"The first piece of our [? dendogram ?] says, well, all right, I have six cities, I have six clusters, each containing one city.
All right, what happens next? What's the next level going to look like? Yeah? AUDIENCE: You're going from Boston [INAUDIBLE] JOHN GUTTAG: I'm going to join Boston and New York, as improbable as that sounds.
All right, so that's the next level.
And if for some reason I only wanted to have five clusters, well, I could stop here.
Next, what happens? Well, I look at it, I say well, I'll join up Chicago with Boston and New York.
All right.
What do I get at the next level? Somebody? Yeah.
AUDIENCE: Seattle [INAUDIBLE] JOHN GUTTAG: Doesn't look like it to me.
If you look at San Francisco and Seattle, they are 808 miles, and Denver and San Francisco is 1,235.
So I'd end up, in fact, joining San Francisco and Seattle.
AUDIENCE: That's what I said.
JOHN GUTTAG: Well, that explains why I need my hearing fixed.
[LAUGHTER] All right.
So I combine San Francisco and Seattle, and now it gets interesting.
I have two choices with Denver.
Obviously, there are only two choices, and which I choose depends upon which linkage criterion I use.
If I'm using single-linkage, well, then Denver gets joined with Boston, New York, and Chicago, because it's closer to Chicago than it is to either San Francisco or Seattle.
But if I use complete-linkage, it gets joined up with San Francisco and Seattle, because it is further from Boston than it is from, I guess it's San Francisco or Seattle.
Whichever it is, right? So this is a place where you see what answer I get depends upon the linkage criteria.
And then if I want, I can consider to the next step and just join them all.
All right? That's hierarchical clustering.","The distance from Denver to Seattle is 1,307 miles, and the distance from Denver to Boston is 1,949 miles. So why did he cluster Denver with Seattle instead of Boston when using complete linkage? Shouldn't it be clustered with the location that has the greatest distance?"
233,esmzYhuFnds,"This is not something you want to do on a million examples.
The naive algorithm, the one I just sort of showed you, is N cubed.
N cubed is typically impractical.
For some linkage criteria, for example, single-linkage, there exists very clever N squared algorithms.
For others, you can't beat N cubed.
But even N squared is really not very good.
Which gets me to a much faster greedy algorithm called k-means.
Now, the k in k-means is the number of clusters you want.
So the catch with k-means is if you don't have any idea how many clusters you want, it's problematical, whereas hierarchical, you get to inspect it and see what you're getting.",Could anyone explain what the professor means when he mentions n-squared and n-cubed algorithms?
234,esmzYhuFnds,"Well, couple of things you can do about that.
You could be clever and try and select good initial centroids.
So people often will do that, and what they'll do is try and just make sure that they're distributed over the space.
So they would look at some picture like this and say, well, let's just put my centroids at the corners or something like that so that they're far apart.
Another approach is to try multiple sets of randomly-chosen centroids, and then just select the best results.
And that's what this little algorithm on the screen does.
So I'll say best is equal to k-means of the points themselves, or something, then for t in range number of trials, I'll say C equals k-means of points, and I'll just keep track and choose the one with the least dissimilarity.
The thing I'm trying to minimize.
OK? The first one is got all the points in one cluster.
So it's very dissimilar.
And then I'll just keep generating for different k's and I'll choose the k that seems to be the best, that does the best job of minimizing my objective function.
And this is a very common solution, by the way, for any randomized greedy algorithm.
And there are a lot of randomized greedy algorithms that you just choose multiple initial conditions, try them all out and pick the best.",Can anyone assist here? How can we compare the dissimilarity mentioned in the IF statement using Python? I urgently need help with this.
235,f9cVS_URPc0,"Why is that? Why is that? Because this is the weight of some vertex-- this is the weight-- the shortest-path distance to my predecessor using one fewer edge.
And so this in particular is the weight of some path that uses V edges.
So if this is the shortest such path distance, this has to upper bound it at least-- at most.
Yeah? AUDIENCE: Is that the triangle inequality? JASON KU: That is a statement of the triangle inequality, thank you.
All right.
So, yes, this is just by triangle inequality.
OK.
Now what we can say is, let's take this equation summed over all vertices in my cycle.
So I'm just going to add summation here of all vertices in my cycle of this whole thing.
I'm going to do that out a little bit neater.
Summation of delta, not d.
Delta V S, V.
I guess I don't need this open parentheses.
Equals-- or less than or equal to sum of V and C of delta V minus 1 V prime.
And here, I'm summing over V and C, and this is just my notation for the predecessor.
And then I'm going to sum over the weights in my cycle V and C.
These are the sum of the weights in my cycle.
Well, what do I know about this cycle? This is just the weight of C.
The weight of C-- that's awful handwriting.
C, what do I know about the weight of the cycle? It's negative.
So, this is less than 0, which means that if I remove this, this needs to be a strict equality.
But if the sum of all of these is strictly less than the sum of all these, we can't have none of the vertices in my graph satisfying-- not satisfying this property.
If all of them are not witnesses, then this thing is bigger than this thing-- at least as big as this thing for every vertex in my cycle, which is a contradiction.","I don't understand how the instructor's statement is valid. In the earlier claim to which he refers, we had distances between s and v on both sides of the inequality. However, in this case, we have s and v on the left side and s and pred(v) on the right side, not s and v. Therefore, I cannot see how the claim is applicable in this context."
236,fQvg-hh9dUw,"Or just what's called, repeated random sampling.
But we can use this same idea of validating new data to try and figure out whether the model is a good model or not.
Leave one out cross-validation.
This is as written in pseudocode, but the idea is pretty simple.
I'm given a dataset.
It's not too large.
The idea is to walk through a number of trials, number trials equal to the size of the data set.
And for each one, take the data set or a copy of it, and drop out one of the samples.
So leave one out.
Start off by leaving out the first one, then leaving out the second one, and then leaving out the third one.","I believe the correct code should be ""training = D[:i] + D[i+1:]"" because the method list.pop(index) returns the value at the index, not the modified list. Please correct me if I am mistaken."
237,g0bXSXuLVb0,"Insert, delete one of them.
And then rebuild.
OK, and if I was writing a P set answer, I would say a little bit more detail what I mean in this step.
I've done it in the notes.
Not that hard.
But we can afford linear expected time.
I can afford to call build again.
I guess, technically, I'm calling this build, sequence build.
So I can afford just to extract things into an array, do the linear time operation on the array with the shifting and everything, and then just call build again.
Yeah, question? AUDIENCE: I had a question about get_at.
ERIK DEMAINE: Yeah.
AUDIENCE: [INAUDIBLE] get_at.
ERIK DEMAINE: Sorry, no.
These are separate definitions, yeah? Sorry, they got a little close.
AUDIENCE: Oh.",Could anyone explain why it is 2^10 * log(n)? Where does the 2 come from?
238,g5v-NvNoJQQ,"And Anna took this other treatment from John, and this is what happened for Anna.
And that's why I conjecture that, for John, the difference between Y1 and Y0 is as follows.
And so then, that can be criticized.
So for example, a clinician who has some domain expert, can look at Anna, look at John, and say, oh, wait a second, these two individuals are really different from one another.
Let's say the treatment, for example, had to do with something which was gender specific.
Comparing two individuals which are of different genders are obviously not going to be comparable to one other, and so then the domain expert would be able to reject that conclusion and say, nuh-uh, I don't trust any of these statistics.
Go back to the drawing board.
And so type of interpretability is very attractive.
The second aspect of this, which is very attractive is that it's a non-parametric method, non-parametric in the same way that neural networks or random forest are non-parametric.
So this does not rely on any strong assumption about the parametric form of the potential outcomes.
On the other hand, this approach is very reliant on the underlying metric.
If your distance function is a poor distance function, then it's going to give poor results.","Thank you for the lecture! I believe there should be an additional assumption for the linear model, such that Cov(t, ) = 0, to ensure that the gamma provides a consistent estimate. Additionally, Random Forest and Neural Networks are not parametric models, as they involve training parameters and do not use training data in predictive mode, unlike the NN classifier and matching algorithms."
239,gFD1Lp6zK3w,"So total and function means that there's exactly one arrow out, and that's probably the most familiar case of functions.
And lots of fields just assume that functions are total, but the truth is that there often is not total.
And people aren't careful about.
So let's look at a calculus-like example.
Here's a function g that takes a pair of reals and returns a real.
It maps the real plane into the real line.
And the definition of it is g of x, y is 1 over x minus y.
Now, the domain of this function g is in fact all the pairs of reals.
That's what it means to say that it goes from R cross R-- shorthand R squared-- to the codomain R.
The codomain is the set of all reals.
But this g is obviously not total because 1 over 0 is not defined, which means that on the 45 degree line, g is not defined.
g of r, r is not defined.
So g in fact, is not a total function even though it's familiar.
And you'd not worry about partial functions normally.
You wouldn't notice that this was partial because you're not used to paying attention to that.
OK.
Let's look at a slight variation.
This is function g 0 that goes from some unspecified domain.
I'll specify it in a minute to the reals.
It has exactly the same formula g of x, y is 1 over x minus y.
But now, I'm going to tell you that the domain-- instead of being all the reals-- is the reals except for that 45 degree line.
I just want to get rid of the bad points and not worry about them.
The minute I do that, I have these two functions relations that have the same graph but different domains.","The slide displays ""D ::= R^2 - { (x,y) | x = y }."" What does the ""^2"" signify?"
240,gGQ-vAmdAOI,"AUDIENCE: I meant like, does the map-- PROFESSOR: So now I'm taking the sum of the actual distance, plus the estimated distance to go.
AUDIENCE: All right.
I'm just wondering if the original map has to be [INAUDIBLE].
PROFESSOR: See this is not a map.
She was asking if the map has to be geometrically accurate.
See, this could be a model of something that's not a map.
And so, I'm free to put any numbers on those links that I want, including estimates, as long as they're underestimates of the distance along the lengths.
So this tells me that my estimated distance here, so far, is 1.
So I'll, surely, go down here to C.
And if I go to C, then my accumulated distance is 11.
And my estimate of the remaining distance is 0.
So that's a total of 11.
So now I'm following my heuristic again and saying what's the shortest path on a base of the accumulated distance plus the estimated distance? Here, the accumulated distance plus the estimated distance is 101.
Here, it's only 11.
So plainly, I extend this guy.
And that gets me to the goal.
And the total accumulated distance is now 111 plus 0 equals 111.
And that's not the shortest path, but wait.
I still have to do my checking, right? I have to extend A.
I when I extend A, I get to B.
And now, when I get to B that way, my accumulated distance is 2 plus my-- oh, sorry.
S, A, C.
My accumulate distance it 2.
My estimated distance is 0, so that's equal to 2.
So I'm OK because I'm still going to extend to this guy, right? Wrong.
I've already extended that guy.
So I'm hosed.
I won't find the shortest path because I'm going to stop there.","Why is the estimated distance 0 if the model indicates that it is at least 100?
Why not extend node C again, since the total distance is less than that of the previously extended C node?"
241,gGQ-vAmdAOI,"And I'm going to stop there because this is an admissible heuristic and that's not good enough unless it's a map.
It's not good enough for this particular case because this is not geometric.
This cannot be done as a map on a plane.
So that's a situation where what I've talked to you about, so far, works with branch-and-bound.
Works with branch-and -bound plus an extended list.
But doesn't work when we added an admissible heuristic.
So if we're going to do this in general, we need something stronger than admissibility, which works only on maps.
And so the flourish that I'll tell you about here in the last few seconds of today's lecture is to add a refinement as follows.
So far, we've got admissibility.
And if we want to write this down in a kind of mathematical notation, we could say that it's admissible if the estimated distance between any node X and the goal is less than or equal to the actual distance between X and the goal.","Guys, I'm confused. Whether or not the situation we're addressing involves a map, the heuristic never yields a value less than the actual distance. So why does the professor consider it admissible even though it doesn't satisfy the admissibility rule?"
242,gGQ-vAmdAOI,"And the actual distances are 1, 1, 1, and 10.
And over here, we'll make that 100.
So it's a kind of oddly constructed map, but it's there because we need a pathological case to illustrate the idea.
Now that's the actual distances.
And if we did branch and down with an extended list, everything would work just fine.
But we're not.
We're going to use an admissible heuristic.
And we're going to say that this guy has an estimated distance to the goal of 100.
This guy is 0.
And this guy is 0.
Now, 0 is always an underestimate of the actual distance to the goal, right? So I'm always free to use 0.
Is that 100 OK? Yeah because the actual distances is 101, so it's less than that the actual distance.
So it's OK as an admissible heuristic.
So these numbers that I put up here, together, constitute an admissible heuristic set of estimates to the goal.
So now, let's just simulate A star and see what happens.
So first of all, you start with S, and that can either go to A or B.","He stated that 100 was acceptable because it was less than the actual distance. Can we ever assert that the actual distance is less than an admissible heuristic? Moreover, do we measure the actual distance before seeking an admissible heuristic? Wouldn't that be a waste of time?"
243,gGQ-vAmdAOI,"So now we have to carry on with the same algorithm that we started with.
The Oracle checking algorithm.
And when we do that, we look for this shortest path, so far, that has not been extended.
That's B, S, A, B.
That goes to C.
That's 11.
So we're done there.
A goes to D.
That adds 3.
That's 12.
C goes to E.
That adds 6.
That's 15.
And sure enough, we're done.
OK? Elliot? AUDIENCE: Does it know that there's know that there isn't a chance that you could have a zero distance extension from the [INAUDIBLE]? PROFESSOR: The question is, does it know that there's no zero distance length that's coming up.","Please correct me if I'm mistaken, but isn't Dr. Winston asserting that the shortest path is always guaranteed to be unique provided that all paths have a non-zero length? Isn't that incorrect? Consider a rectangular graph; starting from one corner and aiming for the opposite corner, there would clearly be two shortest paths, wouldn't there?"
244,gGQ-vAmdAOI,"That's the definition of admissible.
As long as heuristic does that it's admissible.
And A star works if it's a map.
But for that kind of situation where it's not a map we need a stronger condition, which is called consistency.
And what that says is that the distance between X and the goal minus the distance between some other node in the goal, Y.
Take the absolute value of that.
That has to be less than or equal to the actual distance between X and Y.
So this heuristic satisfy the consistency condition? Well, let's see.
Here the guess is 100.
Here it's 0.
So the absolute difference is 100.
But the actual distance is only 2.
So it satisfies admissibility, but it doesn't satisfy consistency.
And it doesn't work.
So you can almost be guaranteed we'll give you a situation where if you use an admissible heuristic you'll lose.
And if you use a consistent heuristic, you'll still win.
So how can we bring this back into the fold? Well, we can't use that heuristic.
It's no good.
But if this heuristic estimate of the goal were 2, then we'd be OK because then it would still be admissible.
But it would also be consistent.
So the bottom line is that you now know something you didn't know when you started out two lectures ago.
You now know how MapQuest and all of its' descendents work.
Now you can find an optimal path, as well as a heuristically good path.
You see that if you don't do anything other than branch and bound it can be extremely expensive.
And you can even invent pathological cases where it's exponential and the distance to the goal.
So because it can be so computationally horrible, you want to use every advantage you can, which, generally, involves using an extended list.
As well as-- no laptops, please.
It still holds.
No smoking, no drinking, and no laptops.
So you're going to use all the muscles you can.
And those muscles include using an extended list and an admissible or consistent heuristic, depending on the circumstances.","How can one calculate the ""actual distance"" between nodes x and y when the problem is not map-based? Should the absolute difference between the accumulated cost from the start to x and the accumulated cost from the start to y be used?"
245,gGQ-vAmdAOI,"The actual distance is 1 plus an estimate on the remaining distance.
That gives us 100 plus 100.
That's equal to 101.
If we go to B instead, the actual distance is 1 plus the heuristic's distance is 0, so that's equal to 1.
OK, good.
So now we know that we always extend the shortest path so far.
Did I goof this, or are you asking a question? AUDIENCE: [INAUDIBLE]? PROFESSOR: Yeah, when I say actual, it's the actual distance that you've traveled.
AUDIENCE: But that's [INAUDIBLE].
PROFESSOR: So wait a second.
If I go from S to A, the actual distance I've traveled is 1.","Revised sentence: ""How was the estimated distance from B to G determined to be zero? Why can't a non-map search space be represented geometrically?"""
246,gRkUhg9Wb-I,"For example, suppose you just had two random variables.
Because any distribution could be represented by probability of X1 times probability of X2 given X1, according to just rule of conditional probability, and similarly, any distribution can be represented as the opposite, probability of X2 times probability of X1 given X2, which would look like this, the statement that one would make is that if you just had data involving X1 and X2, you couldn't distinguish between these two causal graphs, X1 causes X2 or X2 causes X1.
And usually another treatment would say, OK, but if you have a third variable and you have a V structure or something like X1 goes to x2, X1 goes to X3, this you could distinguish from, let's say, a chain structure.
And then the final answer to what is causal inference from this philosophy would be something like, OK, if you're in a setting like this and you can't distinguish between X1 causes X2 or X2 causes X1, then you do some interventions, like you intervene on X1 and you look to see what happens to X2, and that'll help you disentangle these directions of causality.
None of this is what we're going to be talking about today.
Today, we're going to be talking about the simplest, simplest possible setting you could imagine, that graph shown up there.","At that point, XXX is characterized as a V-structure, which is identifiable from a chain structure by using data. However, it is not a V-structure in the conventional sense; for that, you would require XXX."
247,gqaHkPEZAew,"So if you do that, you then get this property that the difference between two vectors, its similarity to another word corresponds to the log of the probability ratio shown on the previous slide.
So the GloVe model wanted to try and unify the thinking between the co-occurrence matrix models and the neural models by being in some way similar to a neural model.
But actually calculated on top of a co-occurrence matrix count.
So we had an explicit loss function.
And our explicit loss function is that we wanted the dot product to be similar to the log of the co-occurrence.
We actually added in some bias terms here but I'll ignore those for the moment.
And we wanted to not have very common words dominate.",Can you explain how the log-bilinear model incorporating 'vector differences' was derived? Which property of conditional probability was utilized? Could you also provide any useful links? What is the timestamp for this information?
248,gvmfbePC2pc,"And just so you know I'm not cheating, there's a little slider here that rotates that third object.
Let's see, why are there just two known objects and one unknown? Well that's because I've restricted the motion to rotation around the vertical axis and some translation.
So now that I've spun that around a little bit, let me pick some corresponding points.
Oops.
What's happened? Wow.
Let me run that by again.
OK.
So there's one point I've selected from the model objects.
The corresponding point over here on the unknown is right there.
I'm going to be a little off.
But that's OK.
So let me just pick that one and then that corresponds to this one.","I don't understand how three objects are sufficient for rotations around three axes and translation, while two objects are sufficient for rotation around one axis and translation. Can you explain?"
249,h0e2HAPTGF4,"And notice right here.
It's going to be really hard to separate those two examples from one another.
They are so close to each other.
And that's going to be one of the things we have to trade off.
But if I think about using what I learned as a classifier with unlabeled data, there were my two clusters.
Now you see, oh, I've got an interesting example.
This new example I would say is clearly more like a receiver than a lineman.
But that one there, unclear.
Almost exactly lies along that dividing line between those two clusters.
And I would either say, I want to rethink the clustering or I want to say, you know what? As I know, maybe there aren't two clusters here.
Maybe there are three.
And I want to classify them a little differently.
So I'll come back to that.
On the other hand, if I had used the labeled data, there was my dividing line.
This is really easy.","""Sir, is that one black dot called noise?"""
250,h0e2HAPTGF4,"It's called the Manhattan metric.
The one you've seen more, the one we saw last time, if p is equal to 2, this is Euclidean distance, right? It's the sum of the squares of the differences of the components.
Take the square root.
Take the square root because it makes it have certain properties of a distance.
That's the Euclidean distance.
So now if I want to measure difference between these two, here's the question.
Is this circle closer to the star or closer to the cross? Unfortunately, I put the answer up here.
But it differs, depending on the metric I use.
Right? Euclidean distance, well, that's square root of 2 times 2, so it's about 2.8.",How is it 2.8 instead of 2?
251,h0e2HAPTGF4,"This is a standard way to do it, simply repeating what we had on an earlier slide.
If I want to cluster it into groups, I start by saying how many clusters am I looking for? Pick an example I take as my early representation.
For every other example in the training data, put it to the closest cluster.
Once I've got those, find the median, repeat the process.
And that led to that separation.
Now once I've got it, I like to validate it.
And in fact, I should have said this better.
Those two clusters came without looking at the two black dots.
Once I put the black dots in, I'd like to validate, how well does this really work? And that example there is really not very encouraging.
It's too close.
So that's a natural place to say, OK, what if I did this with three clusters? That's what I get.","I have a comment on a part of the lecture that seemed counterintuitive to me. In the football example, it appeared that he was dealing with four distinct types of football players. However, Professor Grimson chose not to work with four clusters due to concerns about overfitting, opting instead for three, even though he was clearly aware of the existence of four different types of football players. Personally, I would have selected four clusters to accurately represent the four different types of football players and proceeded from that point."
252,h0e2HAPTGF4,"This solid line has the property that all the Democrats are on one side.
Everything on the other side is a Republican, but there are some Republicans on this side of the line.
I can't find a line that completely separates these, as I did with the football players.
But there is a decent line to separate them.
Here's another candidate.
That dash line has the property that on the right side you've got-- boy, I don't think this is deliberate, John, right-- but on the right side, you've got almost all Republicans.
It seems perfectly appropriate.
One Democrat, but there's a pretty good separation there.
And on the left side, you've got a mix of things.
But most of the Democrats are on the left side of that line.
All right? The fact that left and right correlates with distance from Boston is completely irrelevant here.
But it has a nice punch to it.
JOHN GUTTAG: Relevant, but not accidental.
ERIC GRIMSON: But not accidental.
Thank you.
All right.
So now the question is, how would I evaluate these? How do I decide which one is better? And I'm simply going to show you, very quickly, some examples.","""What does he mean by saying that the choice to use the distance of voters from Boston is not accidental?"""
253,h0e2HAPTGF4,"We'll see that, if the examples are well separated, this is easy to do, and it's great.
But in some cases, it's going to be more complicated because some of the examples may be very close to one another.
And that's going to raise a problem that you saw last lecture.
I want to avoid overfitting.
I don't want to create a really complicated surface to separate things.
And so we may have to tolerate a few incorrectly labeled things, if we can't pull it out.
And as you already figured out, in this case, with the labeled data, there's the best fitting line right there.
Anybody over 280 pounds is going to be a great lineman.
Anybody under 280 pounds is more likely to be a receiver.","""Wouldn't it be a better idea to draw a straight line through the data, with the line perpendicular to it serving as the 'dividing line'?"""
254,het9HFqo1TQ,"Right? So this is probability of all the values of y of y1 up to ym given all the xs and given, uh, the parameters theta parameterized by theta.
Um, this is equal to the product from I equals 1 through m of p of yi given xi parameterized by theta.
Um, because we assumed the examples were- because we assume the errors are IID, right, that the error terms are independently and identically distributed to each other, so the probability of all of the observations, of all the values of y in your training set is equal to the product of the probabilities, because of the independence assumption we made.
And so plugging in the definition of p of y given x parameterized by theta that we had up there, this is equal to product of that.
Okay? Now, um, again, one more piece of terminology.
Uh, you know, another question I've always been asked if you say, hey, Andrew, what's the difference between likelihood and probability, right? And so the likelihood of the parameters is exactly the same thing as the probability of the data, uh, but the reason we sometimes talk about likelihood, and sometimes talk of probability is, um, we think of likelihood.","I have a question regarding this likelihood function; could someone assist me with it? Under the IID assumption, the probability of all observations is equal to the product of their individual probabilities. However, isn't the expression actually a density rather than a probability in the context of a normal distribution? I am quite confused. I believe the probability should be represented by the integral of the density function. If we are dealing with densities, what does the product of these densities signify?"
255,het9HFqo1TQ,"[NOISE] And then you return Theta transpose x, right? So you fit a straight line and then, you know, if you want to make a prediction at this value x you then return say the transpose x.
For locally weighted regression, um, you do something slightly different.
Which is if this is the value of x and you want to make a prediction around that value of x.
What you do is you look in a lo- local neighborhood at the training examples close to that point x where you want to make a prediction.
And then, um, I'll describe this informally for now but we'll- we'll formalize this in math for the second.
Um, but focusing mainly on these examples and, you know, looking a little bit at further all the examples.
But really focusing mainly on these examples, you try to fit a straight line like that, focusing on the training examples that are close to where you want to make a prediction.
And by close I mean the values are similar, uh, on the x axis.
The x values are similar.
And then to actually make a prediction, you will, uh, use this green line that you just fit to make a prediction at that value of x, okay? Now if you want to make a prediction at a different point.
Um, let's say that, you know, the user now says, ""Hey, make a prediction for this point."" Then what you would do is you focus on this local area, kinda look at those points.
Um, and when I say focus say, you know, put most of the weights on these points but you kinda take a glance at the points further away, but mostly the attention is on these for the straight line to that, and then you use that straight line to make a prediction, okay.
Um, and so to formalize this in locally weighted regression, um, you will fit Theta to minimize a modified cost function [NOISE] Where wi is a weight function.","How is this anything other than a form of interpolation using shape functions? To me, that doesn't quite qualify as ""learning."""
256,iTMn0Kt18tg,"So this is n squared.
All this work, still n squared.
Clearly what we need is for x to get smaller, too.
If x-- if, in this recursion-- let me, in red, draw the recursion I would like to have.
If x became x over 2 here, that's the only change we'd need.
Then n and x change in exactly the same way, and so then we can just forget about x-- it's going to be the same as n.
Then we get 2 times n over 2 plus order n.
Look familiar? It is our bread and butter recurrence-- merge sort recurrence.
That's n log n.
That's what we need to do.
Somehow, when we convert our set x to x squared, I want x to get smaller.
Is that at all plausible? Let's think about it.
What's the base case? To keep things simple let's say the base case when x equals 1, I'll just let x be-- let's say I want to compute my a at 1.
Keep it simple.
What if I want two values in x? I'd like to have the feature that when I square all the values in x-- so I want two values, but when I square them I only have one value.
Solve for x.
Yeah.
AUDIENCE: Negative 1 and 1? ERIK DEMAINE: Negative 1 and 1.
Whew, tough one.","I think we need to compute the sum of costs at each level, not just the last one!"
257,iTMn0Kt18tg,"So, all right, 2 times-- because there's two subproblems-- each with size n over 2 and size x, plus what goes here is however much it costs to do the divide step, plus however much it costs to do the combined step-- all the non-recursive parts.
So this is just partitioning the vectors-- linear scan, linear time.
This is-- we talked about it-- it's a constant number of arithmetic operations for each x.
So this cause-order x time, this cause-order n time.
So, in general, we get n plus x.
Now, this is, again, not a recurrent solvable by the master method because it has two variables.
So usually when you're faced with this sort of thing, you want to do back of the envelope picture.
Draw a recursion tree.
That's a good way to go.
So at the root-- now, I know that initially x equals n.","Can someone explain why the last item is O(n + |x|)? Where does this 'n' originate from? I believe it should be O(|X|) because once we have computed Aeven(x) and Aodd(x) for any x in X, it takes constant time to calculate Aeven(x) + x*Aodd(x) for each x. Now, we have |X| points to compute, so it should take O(|X|) time to obtain A(x) for all x in X. Please correct me if I am mistaken."
258,iTMn0Kt18tg,"When you multiply those together, you get x to the k, so this is the coefficient of x to the k.
Cool? So, that's what we'd like to compute.
Given a and b we want to compute this polynomial c.
How long does it take? We have to do this for all k.
So, to compute the k-th term takes order k time, so the total time is n squared.
So, that's not good for this lecture.
We want to do better.
In fact, today we will achieve n log n.
That's our goal-- polynomial multiplication in n log n.","""Am I the only one who thinks the clock is wrong?""
Can anybody explain why it is O(n^2)?"
259,iTMn0Kt18tg,"You get the inverse.
Cool.
Very cool because what this tells us is we run exactly the same algorithm and do exactly the same transformation.
If we want to do the inverse we can actually just use v, but with a different choice of xk.
Namely, for the inverse, we'll call it xk inverse.
We just take the complex conjugate of this thing, which turns out to be e to the minus ijk tau over n, and then divide the whole thing by n.
I'm using a fact here, which is that the complex conjugate of this number is actually, just, you put a minus sign here.
Why does that hold? Because geometry.
Theta is usually measuring the counterclockwise angle from the x-axis.","""If the complex conjugate simply involves negating the exponent in the exponential, why did he write exp(-ijkT/n/n)? Why is there a division by n followed by another division by n? Could this be an error?""
If the complex conjugate simply involves negating the exponent in the exponential, why did he write exp(-ijkT/n/n)? Why is there a division by n twice? Could this be an error? (Also, apologies for asking in a subcomment; I was concerned it might be overlooked amidst the clutter.)"
260,iZTeva0WSTQ,"ouple of announcements, uh, before we get started.
So, uh, first of all, PS1 is out.
Uh, problem set 1, um, it is due on the 17th.
That's two weeks from today.
You have, um, exactly two weeks to work on it.
You can take up to, um, two or three late days.
I think you can take up to, uh, three late days, um.
There is, uh, there's a good amount of programming and a good amount of math you, uh, you need to do.
So PS1 needs to be uploaded.
Uh, the solutions need to be uploaded to Gradescope.
Um, you'll have to make two submissions.
One submission will be a PDF file, uh, which you can either, uh, which you can either use a LaTeX template that we provide or you can handwrite it as well but you're strongly encouraged to use the- the LaTeX template.
Um, and there is a separate coding assignment, uh, for which you'll have to submit code as a separate, uh, Gradescope assignment.
So they're gonna- you're gonna see two assignments in Gradescope.
One is for the written part.
The other is for the, uh, is for the programming part.
Uh, with that, let's- let's jump right into today's topics.
So, uh, today, we're gonna cover, uh- briefly we're gonna cover, uh, the perceptron, uh, algorithm.
Um, and then, you know, a good chunk of today is gonna be exponential family and, uh, generalized linear models.
And, uh, we'll- we'll end it with, uh, softmax regression for multi-class classification.
So, uh, perceptron, um, we saw in logistic regression, um.
So first of all, the perceptron algorithm, um, I should mention is not something that is widely used in practice.
Uh, we study it mostly for, um, historical reasons.
And also because it is- it's nice and simple and, you know, it's easy to analyze and, uh, we also have homework questions on it.
So, uh, logistic regression.
Uh, we saw logistic regression uses, uh, the sigmoid function.
Right.
So, uh, the logistic regression, uh, using the sigmoid function which, uh, which essentially squeezes the entire real line from minus infinity to infinity between 0 and 1.","Revised sentence: ""I thought that for sufficient statistics in the context of a Bernoulli distribution, as with a Generalized Linear Model (GLM), it would not be necessary to sample these 'Ys' and 'Xs' because it is assumed that there is already enough data."""
261,iZX8WEGZTVw,"And I'm interested in the way that these probabilities update after one step.
If p prime B is the probability of being in state B after one step, and p prime O is the probability of being in the orange state one step later-- and likewise for green-- what are these probabilities? Well it's easy to see just reading off this graph that the only place you're at is B.
So the only way to get probability of being somewhere is by following an edge out of B.
So the probability of being at one step at the orange vertex is 1/4.
And it's likewise 1/4 for being at the green state.
And it's 1/2 for staying at the blue state.
So what we can say is that the updated probabilities of being at these different states is 1/2, 1/4, and 1/4, as we've just reasoned.
OK.
Let's keep going.
Given that the probability that I'm at the states blue, orange, and green are given by this vector of probabilities, what's the distribution after two steps? So let p double-prime B be the probability of being at state B after two steps, starting from B.
Well, the way we can figure that out is by using conditional probabilities.
Let's look at the example of calculating the probability of being in the orange state two steps after you've started at the blue state.
So here was the probabilities of being at the different states after one step.
How do I get to the orange state? Well I can get to the orange state from the blue state.","""Is the state supposed to be green and read 'G' at this point?"""
262,j080VBVGkfQ,"Uh, what about for, what action do we take from S2? Right.
So for that one, we get a 1.
So we basically, uh, distributing your experience.
So now if you were going to take a max over those, then you would get the same thing that we saw last time for Monte Carlo, which would be 11100000 to the end, um, but here we- we're subdividing our samples.
So, you only get to get an experience for the action that you actually took in the state.
And because we're in the Monte Carlo case, we'll see the TD case or, Q-learning we'll call it later, um, then we get to add up all the rewards to the end of the episode.
So G here is gonna be the sum of all these steps, and I didn't speci- oh, I did.
Good.
And we're keeping Gamma equal to 1 here just to make all the math.
Just adding.
Yeah? Should we just [OVERLAPPING].
Sorry.
Can just be one half for Q S1A1 or in Q S3A1? Uh, is talking about whether or not if we did every visit, if anything would change here.
[NOISE] Excuse me.
It would not change in this case, because, um, both times when you visited S3, the sum of rewards to the end of the episode was 1.","I believe the confusion and the varying answers arose because people overlooked the fundamental principle of Monte Carlo (MC) for policy evaluation. In MC policy evaluation, the process begins only after the completion of a full episode. Consequently, in this scenario, G_{i,t} for all existing state-action pairs(s3,a1), (s2,a2), and (s1,a1)is 1, since the discount factor gamma is zero. By adhering to the pseudocode for MC, you will arrive at the correct answer. However, if you are employing Temporal Difference (TD) learning, where policy evaluation commences immediately and does not wait for the episode to end, then I believe the first student's answer was accurate."
263,j1H3jAAGlEA,"And it is informed, because it's taking advantage of this extra information.
It may not be in your problem.
It's not often the case you've got this information in a map.
Your problem may not have any heuristic measurement of distance to the goal.
In which case, you can't do it.
But if you've got it, you should use it.
Oh, yeah, there's one more.
And I've already given it away by having it on my chart.
It's called Beam Search.
And just as Hill Climbing is an analog of Depth-first Search, Beam Search is a complement or addition of an informing heuristic to Breadth-first Search.
What you do is you start off just like Breadth-first Search.
But you say I'm going to limit the number of paths I'm going to consider at any level to some small, fixed number, like, in this case, how about two.
So, I'm going to say that I have a Beam of two for my Beam Search.
Otherwise, I proceed just like Breadth-first Search, b, d, a, g.",Beam Search: How is Node B connected to Node G when there is no path drawn from B to G on the map?
264,j1H3jAAGlEA,"And the reason is that there are large lawns on the shoulders of Mt.
Washington.
It's quite flat.
So, it's the telephone pole problem.
That space looks like this.
Well, this isn't what Mt.
Washington looks like.
But it's the telephone pole problem.
So, when you're wandering around here, the idea of trying a few directions and picking the one that's best doesn't help any, because it's flat.
That can be a problem with Hill Climbing.
Now, there's one more problem with Hill Climbing that most people don't know about.
But it works like this.
This is a particularly acute problem in high dimensional spaces.
I'll illustrate it here just in two.
And I'm going to switch from a regular kind of view to a contour map.
So, my contour map is going to betray the presence of a sharp bridge along the 45 degree line.","""I don't understand the second drawback of hill climbing. He mentioned something called the 'telephone pole problem'what is that?"""
265,j1H3jAAGlEA,"But of course, all isn't lost.
Because we have the choice of backing up to the place where we last made a decision and choosing another branch.
So, that process is called variously back-up or backtracking.
At this point, we would say, ah, dead end.
The first place we find when we back up the tree where we made a choice is when we chose b instead of d.
So, we go back up there and take the other route.
s, a, d now goes to g.
And we're done.
We're going to make up a little table here of things that we can embellish our basic searches with.
And one of the things we can embellish our basic searches with is this backtracking idea.
Now, backtrack is not relevant to the British Museum algorithm, because you've got to find everything.
You can't quit when you've found one path.
But you'd always want to use backtracking with Depth-first Search, because you may plunge on down and miss the path that gets to the goal.
Now, you might ask me, is backtracking, therefore, always part of Depth-first Search? And you can read textbooks that do it either way.
Count on it.
If we give you a Search problem on a quiz, we'll tell you whether or not your Search is supposed to use backtracking.
We consider it to be an optional thing.
You'd be pretty stupid not to use this optional thing when you're doing Depth-first Search.
But we'll separate these ideas out and call it an optional add-on.
so, that's Depth-first Search, very simple.
Now, the natural companion to Depth-first Search will be Breadth-first Search, Breadth-first.
And the way it works is you build up this tree level by level, and at some point, when you scan across a level, you'll find that you've completed a path that goes to the goal.","""Isn't the idea of backtracking contrary to the rule against biting one's own tail?"""
266,j1H3jAAGlEA,"But, of course, I keep s, a, d and s, b on the queue.
Now, I take the front off the queue again, and I get s, a, b, c, e, and not to forget s, a, d and s, b.
I take the first one off the queue.
It doesn't go to the goal.
I try to extend it, but there's nothing there.
I've reached a dead end.
So, in this operation, all I'm doing is taking the front one off the queue and shortening the queue.
We're almost home.
I take s, a,d off of queue.
And I get s, a, d, c.
And, of course, I still have s, b.
Now, the next time I visit the situation, buried in that first step, I discover a path that actually does get to goal, and I'm done.
So, each time around I visualize the queue.
I check to see if I'm done.
If not, I take the extensions and put them somewhere on the queue.","""Doesn't DFS use a stack while BFS uses a queue?"""
267,j1H3jAAGlEA,"In Hill Climbing Search, just like a Depth-first Search, we have a and b.
And we're still going to list them lexically on underneath the parent node.
But now which one is so closer to the goal? Now, this time b is closer to the goal than a.
So, instead of following the Depth-first course, which would take us down through a, we're going to go to the one that's closest which goes through b.
And b can either go to a or c.
b is six units away from the goal.
a is about seven plus, not drawn exactly to scale.","Hill Climbing Search: Why does the instructor claim that Node B is closer to the goal than Node A? According to the route map and the given weights, it seems that Node A is only 8 units from the goal, as the path S -> A -> D -> G totals 11 units, whereas the path via Node B, S -> B -> A -> D -> G, amounts to 17 units. So, why would the hill climbing search opt for the path through Node B?
Although I'm unfamiliar with this material, I believe the program estimates the distance to the target in a straight line, disregarding any paths. Otherwise, it wouldn't have selected path B as the closest option. I'm unsure how it could determine such a measurement without a map because, without one, it couldn't count pixels or discern the shortest route in any way."
268,j1H3jAAGlEA,"In here there's an explicit step where I've checked to see if that first path is a winner.
If it's not, I extend it.
And I have to put those paths onto the queue.
So, I'll say that what I do is I end queue.
Now, I've done one step.
And let's let me do another step.
I'm going to take this first path off.
I'm going to extend that path.
And where do I put these new paths on the queue if I'm doing Depth-first Search? Well, I want to work with the path that I've just generated.
I'm taking this plunge down deep into the search tree.
So, since I want to keep going down into the stuff that I just generated, where then do I want to put these two paths? At the end of the queue? I don't think so, because it'll be a long time getting there.
I want to put them on the front of the queue.
For Depth-first Search, I want to put them on the front of the queue.
And that's why s, a, b goes here, and s, a, d, and then that's s, b.
So, s, b is still there.
That's still a valid possibility.
But now I've stuck two paths in front of it, both of the ones I generated by taking a path off the front of the queue, discovering that it doesn't go to the goal, extending it and putting those back on the queue.
I might as well complete this illustration here.
While I'm at it, I take the s, a, b off, s, a, b, and I can go only there to c.","""Thanks for helping me understand a lot! However, doesn't enqueueing paths at the front of the queue violate the FIFO (First In, First Out) property of a queue?"""
269,j1H3jAAGlEA,"So, level by level, s can go to either a or b.
a can go either to b or d.
And b can go to either a or c.
So, you see what we're doing.
We're going level by level.
And we haven't hit a level with a goal in it yet, so we've got to keep going.
Note that we're building up quite a bit of stuff here, quite a lot of growth in the size of the path set that we're keeping in mind.
At the next level, we have b going to c, d going to g, a going to d, and c going to e.
And now, when we scan across, we do hit g.
So, we found a path with Breadth-first Search, just as we found a path with Depth-first Search.
Now, you might say, well, why didn't you just quit when you hit g? Implementation detail.
We'll talk about a sample implementation.
You can write it in any way you want.
But now that we know what these searches are, let's speed things up a little bit here and do a couple searches that now have names.
The first type will be Depth-first, boom.
That's the one that produces the thief path.
And then we can also do a Breadth-first Search, which we haven't tried yet.
What do you suppose is going to happen? Is it going to be fast, slow, produce a good path, produce a bad path? I don't know, let's try it.
I had to speed it up, you see, because it's doing an awful lot of Search.
It's generating an awful lot of paths.
Finally, you got a path.
Is it the best path? I don't think so.
But we're not going to talk about optimal paths today.
We're just going to talk about pretty good paths, heuristic paths.
Let's move the starting position here in the middle.","""Sir, does breadth-first search require more computational time compared to depth-first search?"""
270,j9AcEI98C0o,"This dataset has 2.5 thousand training examples.
And before sort of the big transformers came around we had 60% accuracy on it.
We run transformers on it, we get 10 points just by pretraining.
And this has been a trend that is just continued.
So why do anything, but pretrain encoders? But like we know encoders are good, and we like the fact that you have bidirectional context.
We also saw that BERT did better than GPT.
But it has-- if you want to actually get it to do things, right? You can't just generate it-- generate sequences from it the same way that you would from a model like GPT, a pretrained decoder, right? You can sort of sample what things should go in a mask.
So here's a mask, you can put a mask somewhere, sample the words that should go there.
But if you want to sample the whole context, right, if you want to get that story about the unicorns, for example, the encoder is not what you want to do.
So they have sort of different contracts in there.
It can be used naturally at least in different ways.
OK.
So let's talk very briefly about extensions of BERT.
So there are variants like RoBERTa and SpanBERT, and there's just a bunch of papers with the word BERT in the title, that did various things.","""Why not add a [MASK] at the end of a sentence and iterate using the encoder as a generative method?"""
271,jGwO_UgTS7I,"My advice to students is that um, CS229, uh, CS229a, excuse me, let me write this down.
I think I'm- so CS229a, uh, is taught in a flipped classroom format which means that, uh, since taking it, we'll mainly watch videos um, on the Coursera website and do a lot of uh, programming exercises and then, meet for weekly discussion sections.
Uh, but there's a smaller class with [inaudible] .
Um, I, I would advise you that um, if you feel ready for CS229 and CS230 to do those uh, but CS229, you know, because of the math we do, this is a, this is a very heavy workload and pretty challenging class and so, if you're not sure you're ready for CS229 and CS229a, it may be a good thing to, to, to take first, uh, and then uh, CS229, CS229a cover a broader range of machine learning algorithms uh, and CS230 is more focused on deep learning algorithms specifically, right.
Which is a much narrow set of algorithms but it is, you know, one of the hardest areas of deep learning.
Uh, there is not that much overlap in content between the three classes.
So if you actually take all three, you'll learn relatively different things from all of them uh, in the past, we've had students simultaneously take 229 and 229a and there is a little bit of overlap.
You know, they, they do kind of cover related algorithms but from different points of view.
So, so some people actually take multiple of these courses at the same time.","It could be that the Coursera course is indeed CS229A. This is suggested by Andrew when he explains the differences between CS229 and CS229A, noting that CS229A is available on Coursera and the described format appears identical. However, I am not certain if this is accurate. He begins discussing this topic at a specific point in the video."
272,jGwO_UgTS7I,"Um, uh, we actually kept the enrollments to CS229a at a relatively low number at 100 students.
So I actually don't want to encourage too many of you to sign up because uh, I think we might be hitting the enrollment cap already so, so please don't all sign up for CS229a because um, we- CS229a, does not have the capacity this quarter but since CS229a is uh, um, much less mathematical and much more applied, uh, uh, a relatively more applied version of machine learning and uh, so I, I guess I'm teaching CS229a and CS230 and CS229, this quarter.
Of the three, CS229, is the most mathematical.
Um, it is a little bit less applied than CS229a which is more applied machine learning and CS230 which is deep learning.","It could be that the Coursera course is indeed CS229A. This is suggested by Andrew when he explains the differences between CS229 and CS229A, noting that CS229A is available on Coursera and the described format appears identical. However, I am not certain if this is accurate. He begins discussing this topic at a specific point in the video."
273,kh3I_UTtUOo,"Um, here, I call it loopy belief propagation, because in practice, people tend to apply this algorithm to, um, graphs that have cycles or loops as well, even though than any kind of convergence guarantees and this kind of probabilistic interpretation that I gave here, uh, gets lost.
So, um, if you consider graphs with cycles, right, there is no- no longer a fixed ordering on- of the nodes which is, er, which, um, otherwise the fixed ordering exists in terms if the graphs are trees.
But if a graph has a cycle, you cannot- you cannot, uh, sort, uh, the nodes, uh, in- in a- in a- in a nice order.
And basically the idea is that we apply the same algorithm as in the previous slides, but we start from arbitrary nodes and then follow the edges to update the messages.
So basically, we kind of propagate this in some kind of, uh, random order, again, until it converges or until, uh, some fixed number of, uh, iterations is, uh, is reached.","Is there a mistake in the indexing? On the previous page, messages (\Pi term) come from all neighbors except j. However, here we are considering all neighbors, including j (using an ambiguous iterative notation called j), and then we obtain Y_j when the message is from m_{j \to i}. Shouldn't it be m_{j \to i}(Y_i)?"
274,krZI60lKPek,"So P equals NP.
SO we should all be getting Turing award for that.
So clearly something's wrong.
But there's no problem with this solution.
This covers all the cases.
And our analysis is definitely correct.
So does anyone get what I'm asking? So what's the contradiction here? I will probably discuss this later, in later lectures when we get to complexity or reduction.
But to give a short answer, the problem is that when we say the input is n, its size is not n.
So I only need log n this to represent this input.
Make sense? Therefore, for log n length input, my runtime is n.
That means my runtime is exponential.",Could anyone explain why it is logN? I'm really confused here.
275,l-tzjenXrvI,"I'm fusing it with the table because I want to consider it to be one object.
We can view it from the three objects indicated by those three little chalk pieces.
And ask ourselves, well, in the event that we look at it from those three places, what do we see? And if we look at it from the perspective of the piece of purple chalk-- I'll have to walk around and be sure-- looks like an arrow junction with two concaves and a convex.
Did I get that right? So I'm looking at it from this perspective.
It's an arrow.
This is convex and these two are concave because I fused the paper box with the table.
Looking at it from the perspective of this blue guy-- let me rotate it so you can have a better understanding of the blue guy-- it looks like a concave line and a boundary.
So it's an L.
And this one is a boundary.
And that's concave.
And by a kind of symmetry, we're also going to get that one from the other side, the third of the three octants.
Well, we're off and running.
But we still have an awful lot to go.
And we could manage to deal with it by thinking about this object once again.
But instead of this situation out here, to turn it around and look at this vertex.
Think about the junctions that it can produce.","I understand that, but I didn't comprehend why the line at the bottom is concave instead of having a ""+"" at the bottom. Is it because of the angle from which one views that junction?
""Hi, I am confused about the direction of the arrow; can you please help me understand why?""
How come there are two convex lenses and one concave lens? Shouldn't they all be concave?"
276,lDwow4aOrtg,"So for example, hypothetically you could impose a constraint, the normal w is equal to 1, or another way to do that would be to take w and b and replace it with w over normal b and replace b with, [NOISE] right, just the value of parameters through by the magnitude, by the- by the Euclidean length of the parameter vector w, and this doesn't change any classification, It's just rescaling the parameters.
Ah, but, ah, but, but that it prevents, you know, display of cheating on the functional margin.
Okay.
Um, and in fact, more generally you could actually scale w and b by any other values you want and- and it doesn't- doesn't matter, right? You can choose to replace this by w over 17 and b over 17 or any other number or any, right, and the classification stays the same.
Okay.
So we'll come back and use this property, in a little bit.
Okay.
[NOISE] All right.
So to find the functional margin, let's define the geometric margin.","""I don't understand how, with w=17 and b=17, one can prevent the functional margin from increasing simply by increasing the weight and bias."""
277,lDwow4aOrtg,"This is kinda according to Bayes rule.
Okay? Um, all right.
So it turns out this algorithm will almost work and here's where it breaks down, which is, um, you know, so actually eve- every year, there are some CS229 students and some machine learning students, they will do a class project and some of you will end up submitting this to an academic conference.
Right? Some, some- actually some, some of CS229 class projects get submitted, you know, as conference papers pretty much every year.
One of the top machine learning conferences, is the conference NIPS.
NIPS stands for Neural Information Processing Systems, um, ah, and let's say that in your dictionary, you know, you have 10,000 words in your dictionary.
Let's say that the NIPS conference, the word NIPS corresponds to word number 6017, right? In your, in your 10,000 word dictionary.
But up until now, presumably you've not had a lot of emails from your friends asking, ""Hey, do you want to submit the paper to the NIPS Conference or not."" Um, and so if you use your current, you know, email, set of emails to find these maximum likelihood estimates of parameters, you will probably estimate that, um, probability of seeing this word given that it's spam email, is probably zero.
Right? Zero over the number of, ah, examples that you've labeled as spam in your email.
So if, if you train up this model using your personal email, probably none of the emails you've received for the last few ones had the word NIPS in it, um, maybe.
Uh, and so if you plug in this formula for maximum likelihood estimate, the numerator is 0 and so your estimate of this is probably 0.
Um, and then similarly, this is also 0 over, you know, the number of non-spam emails I guess.
Right.","Wouldn't it be more accurate to use ""multinoulli"" instead of ""multinomial,"" given that the concept of the number of trials, which is a parameter of the multinomial distribution, doesn't really apply in this context?
Shouldn't there generally be ""ni"" instead of the 10,000 that is being added?"
278,lDwow4aOrtg,"Um, so here's what I mean.
Uh, we're gonna go to binary classification, and we're gonna use logistic regression, right? So, so let's, let's start by motivating this with logistic regression [NOISE].
So this, this is a classifier H of theta equals the logistic function of pi to theta transpose x.
And so, um, if you turn this into a binary classification, if, if, if you have this algorithm predict not a probability but predict 0 or 1, then what this classifier will do is, uh, predict 1.
If theta transpose x is greater than 0, right? Um, and predict 0 otherwise.
Okay.
Because theta transpose x greater than 0, this means that, um, g of theta transpose x is greater than 0.5 [NOISE], and you can have greater than or greater than equal to, it doesn't matter.",I just had a question: what does g(z) denote? Is it the sigmoid function?
279,lNHaZlZJATw,"Is it faster or is it more advantageous to use that? Is it more advantageous to use that thing? It could be more advantageous, um, for- for, um, it so happens that the- the situations where we try- where we need to use, um, uh, uh, SGD or mini-batch SGD happen to be with deep learning or neural networks, where the cost function is not convex, right? And once you have, uh, uh, a nonconvex cost function, it's very hard to, um- it's very hard to analyze and make precise statements of what helps and what doesn't help.
And so in- in- in those situations, it- the answer is almost always try and see if it works better.","Wouldn't mini-batch gradient descent be prone to overfitting to that particular batch, since it effectively reduces the training set size?"
280,lU_QT5GSuxI,"It's a pretty tight bounds.
What that means is that informally speaking, the sum of the logs is about this term plus that term.
Plus, let's take the average value of that term, which is half this term.
So we could say that the sum of logs is approximately equal.
That's a little vague, but live with it.
n log n over e plus half of log n.
Well, now, if I'm interested, remember, in an estimate for n factorial-- so let's exponentiate both sides.",Why are you ignoring the +1 that is coming out of the definite integral on the left side?
281,leXa7EKUPFk,"And if this is an and tree, are there any and nodes? Sure, there's one right there.
So do you think then that you can answer questions about your own behavior as long as you build an and-or tree? Sure.
Does this mean that the integration program could answer questions about its own behavior? Sure.
Because they both build goal trees, and wherever you got a goal tree, you can answer certain kinds of questions about your own behavior.
So let me see if in fact it really does build itself a goal tree as it solves problems.
So this time, we'll put B6 on B3 this time.
But watch it develop its goal tree.
So in contrast to the simple example I was working on the board, this gets to be a pretty complicated goal tree.
But I could still answers questions about behavior.
For example, I could say, why did you put B6 on B3? Because you told me to.
All right, so the complexity of the behavior is largely a consequence not of the complexity of the program in this particular case, but the building of this giant goal tree as a consequence of the complexity of the problem.",Wait. It's not merely an AND tree; there's more to it than just AND operations. What about the order or sequence of execution? Simply combining all those actions with AND will not yield the correct outcome. Where is the information about the sequence encoded or represented?
282,mJQrtXZT5pw,"So now you can say, I have these nodes, they have these spokes.
These are kind of these rough edges.
And now I wont to randomly connect, uh, these, uh, endpoints.
And of course, maybe between a pair of nodes, I will allow multiple edges because perhaps both of these two end points, randomly, you know, by chance decide to connect to these two end points so it'll be kind of a double-edge.
But for the purpose of this, uh, discussion right now, that's completely fine and okay.
So then, you can ask yourself, what is the expected number of edges between a pair of nodes i and j, where node i has degree, uh, k_i, and node j has deg- degree k_j.","I suspect there is a simplification in the math. Our process for creating random edges involves randomly selecting a node (i) and then choosing one of its (k_i) stubs to connect to any other available stub (which totals 2m-1, accounting for the one stub already taken from node (i)). Therefore, the probability that a stub from node (i) connects to a stub from node (j) is k_j / (2m - 1). However, to continue the process, shouldn't we account for the stubs that have already been used? Constructing edges in this manner may not be independent. Please correct me if I'm mistaken."
283,mUBmcbbJNf4,"You don't think of any of this when you're just trying to run an algorithm on one computer.
So distributed algorithms can be pretty complicated.
It's not easy to design them.
And after you design them, you still have to make sure they're correct.
So there are issues involved in proving them correct and analyzing them.
A little bit of history, the field pretty much started around the late '60s.
Edsger Dijkstra was one of the earliest leaders in the field.
He won of the first Turing Awards.
Leslie Lamport won the Turing Award last year.
Although he actually started as a very young guy, way back in the early days of the field.
If you want to look at some sources, I have a book.
There's another textbook by Attiya and Welch.
There's a new series of monographs that basically try to summarize many of the important research topics in distributed computing theory.
And the last two lines have a couple of the main conferences in the field.
OK so I can't do that much in one week.","What are the monographs to which she is referring at the start of the lecture, and where can I find them?"
284,n0lce1dMAh8,"That's one simple example where I'm translating a spec into because something that I can express easily as a products and disjoint sums of stuff that I already know the size of.
Let's just do another example.
Suppose that I want to count the number of 4-digit numbers.
So the elements of these 4-digit numbers are 0 through 9-- there are 10 possibilities-- with at least one 7-- the number of 4-digit sequences of digits that have at least one 7 in them.
And one way to count is I can make it a sum of different 4-digit numbers containing one 7, depending on where the first 7 is.","""You're calculating the probability of getting at least one 7, but it seems you're actually calculating the probability of getting exactly one 7."""
285,nt63k3bfXS0,"Um, and so here the feature vector is 0, 1 to the n, because there's a n-dimensional binary feature vector, where- where for the purpose of illustration, let's say, n is 10,000 because you're using, you know, take the top 10,000 words, uh, that appear in your e-mail training set as the dictionary that you will use.
So, um.
So in other words, X_i is indicator word i appears in the e-mail, right? So it's either 0 or 1 depending on whether or not that word i from this list appears in your e-mail.
Now, um, in the Naive Bayes algorithm, we're going to build a generative learning algorithm.
Um, and so we want to model P of x given y, right? As well as P of y, okay? But there are, uh, 2 to the 10,000 possible values of x, right? Because x is a binary vector of this 10,000 dimensional.
So we try to model P of x in the straightforward way as a multinomial distribution over, you know, 2 to the 10,000 possible outcomes.
Then you need, right, uh, uh, you need, you know 2 to the 10,000 parameters, right? Which is a lot, or technically, you need 2 to 10,000 minus 1 parameter because that adds up to 1, and you can see one parameter.","""Why do we need 2^10000 parameters? I think it's 2*10000 because each dimension requires two parameters, and the variance is the same regardless."""
286,nykOeWgQcHM,"And the last one deals mostly with the computer science part in Introduction to Programming and Computer Science in Python.
We're going to talk about, once you have learned how to write programs in Python, how do you compare programs in Python? How do you know that one program is better than the other? How do you know that one program is more efficient than the other? How do you know that one algorithm is better than the other? That's what we're going to talk about in the last part of the course.
OK.
That's all for the administrative part of the course.
Let's start by talking at a high level what does a computer do.","What does a computer do?
How do you determine which algorithm is better than the other?"
287,nykOeWgQcHM,"How do you represent knowledge with data structures? That's sort of the broad term for that.
And then, as you're writing programs, you need to-- programs aren't just linear.
Sometimes programs jump around.
They make decisions.
There's some control flow to programs.
That's what the second line is going to be about.
The second big part of this course is a little bit more abstract, and it deals with how do you write good code, good style, code that's readable.
When you write code, you want to write it such that-- you're in big company, other people will read it, other people will use it, so it has to be readable and understandable by others.
To that end, you need to write code that's well organized, modular, easy to understand.
And not only that, not only will your code be read by other people, but next year, maybe, you'll take another course, and you'll want to look back at some of the problems that you wrote in this class.
You want to be able to reread your code.
If it's a big mess, you might not be able to understand-- or reunderstand-- what you were doing.
So writing readable code and organizing code is also a big part.
And the last section is going to deal with-- the first two are actually part of the programming in Introduction to Programming and Computer Science in Python.","Revised sentence: ""Question: On the second line of the slide, should the term 'branching' be used instead of 'recursion'?"""
288,nykOeWgQcHM,"If you want to go on the internet, send email with it, you can't.
It can only do this one thing.
And if you wanted to create a machine that did another thing, then you'd have to create another fixed-program computer that did a completely separate test.
That's not very great.
That's when stored-program computers came into play.
And these were machines that could store a sequence of instructions.
And these machines could execute the sequence of instructions.
And you could change the sequence of instructions and execute this different sequence of instructions.
You could do different tasks in the same machine.
And that's the computer as we know it these days.","""Is that the von Neumann architecture, right?"""
289,nykOeWgQcHM,"We're trying to find the square root of 16.
We're going to calculate g times g is 9.
And we're going to ask is if g times g is close enough to x, then stop and say, g is the answer.
I'm not really happy with 9 being really close to 16.
So I'm going to say, I'm not stopping here.
I'm going to keep going.
If it's not close enough, then I'm going to make a new guess by averaging g and x over g.
That's x over g here.
And that's the average over there.
And the new average is going to be my new guess.
And that's what it says.
And then, the last step is using the new guess, repeat the process.
Then we go back to the beginning and repeat the whole process over and over again.
And that's what the rest of the rows do.
And you keep doing this until you decide that you're close enough.
What we saw for the imperative knowledge in the previous numerical example was the recipe for how to find the square root of x.
What were the three parts of the recipe? One was a simple sequence of steps.
There were four steps.
The other was a flow of control, so there were parts where we made decisions.
Are we close enough? There were parts where we repeated some steps.",How do we know that we need to average g and x/g?
290,o57CTwt1-ck,"That's just for practice and fun, let's look at the space station Mir again.
Suppose that I tell you that there is a 1 in [? 10,000ths ?] chance that in any given hour, the Mir is going to crash into some debris that's out there in orbit.
So the expectation of f is 10 to the fourth, about 10,000 hours.
And the sigma is going to be the variance of f, which is about 1 over ten thousandths, that is 10,000 times 10,000 minus 1, which is pretty close to 10,000 squared for the variance.
And when I take the square root, I get back to 10,000.
So sigma is just a tad less than 10,000, is 10 to the fourth.
So with those numbers, I can apply the Chebyshev's Theorem and conclude that the probability that the Mir lasts more than 4 times 10 to the fourth hours is less than 1 chance in four.
If we translate that into years-- if it was really the case that there was a 1 in 10,000 chance of the Mir being destroyed in any given hour, then the probability that it lasts more than 4.6 years before destructing is less than 1/4.","""Isn't that the Markov's theorem you've applied?"""
291,o9nW0uBqvEo,"I shouldn't say cheated.
I probably should have counted the return as one more operation, so that would be 1 plus 3x plus 1, or 3x plus 2 operations.
Why should you care? It's a little closer to what I'd like.
Because now I've got an expression that tells me something about how much time is this going to take as I change the size of the problem.
If x is equal to 10, it's going to take me 32 operations.
If x is equal to 100, 302 operations.
If x is equal to 1,000, 3,002 operations.
And if I wanted the actual time, I'd just multiply that by whatever that constant amount of time is for each operation.
I've got a good estimate of that.
Sounds pretty good.
Not quite what we want, but it's close.
So if I was counting operations, what could I say about it? First of all, it certainly depends on the algorithm.
That's great.
Number of operations is going to directly relate to the algorithm I'm trying to measure, which is what I'm after.
Unfortunately, it still depends a little bit on the implementation.
Let me show you what I mean by that by backing up for a second.
Suppose I were to change this for loop to a while loop.
I'll set i equal to 0 outside of the loop.
And then while i is less than x plus 1, I'll do the things inside of that.
That would actually add one more operation inside the loop, because I both have to set the value of i and I have to test the value of i, as well as doing the other operations down here.
And so rather than getting 3x plus 1, I would get 4x plus 1.
Eh.
As the government says, what's the difference between three and for when you're talking about really big numbers? Problem is in terms of counting, it does depend.
And I want to get rid of that in a second, so it still depends a little bit on the implementation.
I remind you, I wanted to measure impact of the algorithm.
But the other good news is the count is independent of which computer I run on.","You're suggesting that the count varies with different implementations because 'while' involves two basic operations: assignment and testing, whereas 'for' only performs assignment. You claim that 'while' and 'for' actually take the same amount of time, but the count differs when we measure it. Nevertheless, let's agree to count two operations when tallying the operations in 'for', assuming 'for' and 'while' take the same time in reality. However, doesn't the 'for' loop also test if 'i' is less than 'x+1'? You mention it only assigns 'i' values from a range, implying it's a single operation."
292,o9nW0uBqvEo,"If I call that n, it's going to take that n times over the outer loop.
But what about n here? All of the earlier examples, we had a constant number of operations inside of the loop.
Here, we don't.
We've got another loop that's looping over in principle all the elements of the second list.
So in each iteration is going to execute the inner loop up to length of L2 times, where inside of this inner loop there is a constant number of operations.
Ah, nice.
That's the multiplicative law of orders of growth.
It says if this is order length L1.
And we're going to do that then order length of L2 times, the order of growth is a product.
And the most common or the worst case behavior is going to be when the lists are of the same length and none of the elements of L1 are in L2.
And in that case, we're going to get something that's order n squared quadratic, where n is the length of the list in terms of number of operations.
I don't really care about subsets.
I've got one more example.
We could similarly do intersection.
If I wanted to say what is the intersection of two lists? What elements are on both list 1 and list 2? Same basic idea.
Here, I've got a pair of nested loops.
I'm looping over everything in L1.
For that, I'm looping over everything in L2.
And if they are the same, I'm going to put that into a temporary variable.
Once I've done that, I need to clean things up.","Why would the worst-case scenario occur when the lists are of the same length and none of the elements in L1 are present in L2? In that situation, the program wouldn't iterate through all the elements in L1, as it would exit the outer loop and return False after a single complete pass through L2 without finding any matching elements. Shouldn't the worst-case scenario arise when the program exhaustively checks all elements in both the inner and outer loops? For example, such a case might be when L1 is [1, 2] and L2 is [1, 3, 4, ...]. Could I be misunderstanding something? I would appreciate any assistance."
293,o9nW0uBqvEo,"So I'm going to write another loop that sets up an internal variable and then runs through everything in the list I accumulated, making sure that it's not already there.
And as long as it isn't, I'm going to put it in the result and return it.
I did it quickly.
You can look through it.
You'll see it does the right thing.
What I want it to see is what's the order of growth.
I need to look at this piece.
Then I need to look at that piece.
This piece-- well, it's order length L1 to do the outer loop.
For each version of e1, I've got to do order of length L2 things inside to accumulate them.
So that's quadratic.
What about the second loop? Well, this one is a little more subtle.
I'm only looping over temp, which is at most going to be length L1 long.","I'm struggling to understand one aspect. It states, ""I'm only looping over tmp, which will be at most the length of L1,"" but that's the best-case scenario. The worst case occurs when L1 and L2 are of equal length and contain identical elements (imagine all 1's). In this scenario, tmp would have a length of L1 squared, as the if condition in the first nested loop would always hold true. It's only under these circumstances that I can envision the second loop being O(n^2) in the worst case, since the implicit ""e in res"" loop would run in constant time due to the res array never exceeding one item. If my understanding is incorrect, I fail to see how the second loop could be O(n^2)."
294,o9nW0uBqvEo,"So looping around it is order n.
There's the actual expression.
But again, the pattern I want you to see here is that this is order n.
OK.
Last example for today.
I know you're all secretly looking at your watches.
Standard loops, typically linear.
What about nested loops? What about loops that have loops inside of them? How long do they take? I want to show you a couple of examples.
And mostly, I want to show you how to reason about them.
Suppose I gave you two lists composed of integers, and I want to know is the first list a subset of the second list.
Codes in the handbook, by the way, if you want to go run it.
But basically, the simple idea would be I'm going to loop over every element in the first list.
And for each one of those, I want to say is it in the second list? So I'll use the same kind of trick.","What does ""if not matched:"" mean? Does it refer to a condition where something does not equal a match?"
295,o9nW0uBqvEo,"So this will be an example of a linear algorithm.
And you can see, I'm looping length of L times over the loop inside of there.
It's taking the order one to test it.
So it's order n.
And if I were to actually count it, there's the expression.
It's 1 plus 4n plus 1, which is 4n plus 2, which by my rule says I don't care about the additive constant.
I only care about the dominant term.
And I don't care about that multiplicative constant.
It's order n.
An example of a template you're going to see a lot.
Now, order n where n is the length of the list and I need to specify that.","Shouldn't it be 1 + 3n + 1, instead of 1 + 4n + 1? Why is there a 4n?
Why is it 1+4n+1? Shouldn't it be 1+3n+1, or does the len function in the range loop add another constant to make it 4n?"
296,oS9aPzUNG-s,"It's just going to be slow.
Yes? AUDIENCE: [INAUDIBLE] JUSTIN: Yeah, that's right.
So actually, I don't know in this class.
I guess, the interface and the way that we've described it here is dynamic.
We can just keep adding stuff to it.
In that case, remember this amortized argument from Erik's lecture says that on average that it will take order n time.
AUDIENCE: [INAUDIBLE] JUSTIN: What was that? AUDIENCE: [INAUDIBLE] JUSTIN: Oh, that's true.
That's an even better-- sorry.
Even if it weren't dynamic.
If I wanted to replace an existing key-- like, for some reason, two students had the same ID.
This is a terrible analogy.
I'm sorry.
But in any event, if I wanted to replace an object with a new one, well, what would I have to do? I'd have to search for that object first, and then replace it.
And that search is going to take order n time from our argument before.
Thank you.
OK.
So in some sense, we're done.","I was slightly confused because inserting a new element at the end should take Theta(1) amortized time, not O(n) amortized time as the instructor suggested. However, since we must search through the entire dynamic array on every insertion to check if an element with that key already exists, the insertion operation runs in O(n)."
297,o_i5F1zGPLs,"Um, the second thing we do and this relates to, um, question about importance sampling.
Um, I- is we have this second quantity in here, where this is the probability of an action under our new policy.
Um, we do have access to that, in the sense that, if someone gives us a state we can tell, um, we can say exactly what our probability would be under all the actions.
But again, this often can be a continuous set.
And so instead of doing sort of this continuous set, we are just going to say we're gonna use importance sampling and we can take samples.
This is typically goi- going to be from pi old.
So we look at what times we have taken an action given our current policy and we re-weight them according to the probability we would have taken those actions that drive the new policy.
So it allows us to approximate that expectation using data that we have.
And then the third substitution is switching the advantage back to the Q function, and it's just important to note that all of these three substitutions don't change the solution to the object- to the optimization problem.
These are all sort of taking at, uh, these different substitutions or different ways to evaluate these quantities, okay? So we end up with the following: um, uh, we have this objective function that we are optimizing.
This is after we've done the substitutions I just mentioned, and we have this constraint on how far away we can be.
Um, and empirically, they generally just sample, um, this sort of alternative sampling distribution Q is just your existing old policy.
So there's a bunch of other stuff in the paper.
It's a really nice paper.
Um, a lot of really interesting ideas.","""Why are you switching from A to Q?"""
298,p61QzJakQxg,"The idea here is d is the dimension of your original data and p is some high-dimensional space, potentially infinite, right? So, uh, so this is the, um, um, update rule that we get.
And now, uh, imagine our Phi to be, um, a feature map like - like this, like Phi of x equals, there's just one example, right? 1, x_1, x_2, and all the - then x_1 square, x_1 x_2, x_1 x_3, and so on.
So I write x_1 cubed, x_1 square x2, and so on, right? So basically, a set of all monomial terms of order less than equal to 3, right? Now, what we see is, uh, the number of - the dimension of the feature vector, in this case, will be - p will be approximately already cubed, right? It's going to be, um, um, cubic times for all the three order terms, and some two order terms, one order term, but overall it's going to be, you know, - the - the cubic term is gonna dominate and it's gonna be, uh, approximately d cubed number of, uh, features, which means, um, to perform each gradient, uh, uh, descent update, we now move from calculating dot products in d dimension.
For example, if d was 1,000 - d was 1,000, right? This dot product would take about, uh, order d, uh, order d, right? Whereas, this dot product is gonna take about order d cubed, so which means if - if you had d equals to 1,000, this will take about, say, a - a - a - a 1,000 time-steps, whereas, this would take about 1,000 cubed, would be like a billion time-steps, right? So potentially, each - performing each update rule can be a million times slower.
And that expense is mostly because we chose - we just happened to choose a higher dimensional feature space, right? Now, let's make a few observations.","Why did you take x1, x2, x3, etc., when the input vector is just in terms of x, right?"
299,ptuGllU5SQQ,"So by their dimensionality d by d over h, where h is the number of heads.
So they're going to still apply to the X matrix, but they're going to transform it to a smaller dimensionality, d by h.
And then each attention head is going to perform attention independently, it's like you just did it a whole bunch of times, right? And so output l is equal to softmax of, here's your QK but now it's in l form, times XVl and now you have sort of these indexed outputs.
And in order to sort of have the output dimensionality be equal to the input dimensionality and sort of mix things around, combine all the information from the different heads, you concatenate the heads.
So that's output 1 through output h, stack them together.
Now, the dimensionality of this, is equal to the dimensionality of X again.","I'm not certain, but it appears that 'output_l' should have the Txd/h dimension, and the final output 'Y' should have the Txd dimension."
300,ptuGllU5SQQ,"We've been saying nonlinearities, abstract features, they're great deep learning, end to end learning of representations is awesome.
But right now, we're just doing weighted averages.
And so what is our solution going to be? I mean, it's not going to be all that complex.
So all we're doing right now, is re-averaging vectors.
So you've got sort of the self attention here and if you just stacked another one, you just keep sort of averaging projections of vectors.
But what if we just add a feed forward network for every individual word? So within this layer, each of these feed forward neural networks shares parameters, but it gets in just the output of self attention for this word as we defined it, processes it, and admits something else.
And so you have output i from self attention, which we saw slides ago.
Apply a feed forward layer, where you take the output, multiply it by a matrix, non-linearity of the matrix.
And the intuition here you can think of at least, is well, you know, something like the feed forward network processes the result of the attention for each thing.","Doesn't self-attention include a softmax function, which is non-linear?"
301,qGZy1CRoZdE,"This loop counts, too, but it doesn't include V.
All this loop tells me is that the voltage drop across this element is equivalent to the voltage drop across this element.
Or, the voltage drop in this direction across that element is equal to the voltage drop in this direction across this element.
That's Kirchhoff's voltage law.
Kirchhoff's current law is that the current flow into a particular node is equal to 0.
Or, if you take all of the current flows in and out of a particular node and sum them, they should sum to 0.","""Hi! Thank you for all these excellent courses and the wonderful teacher; everything is explained extremely well :))) This is the best course I've ever had!!! I just wanted to make a remark: Is there a small mistake, or do I not understand it correctly? It should be the opposite in that direction, not equal. Just before that, she explained it correctly."""
302,qaRIBNE-4Ho,"And whatever is- is in the intersection, that is the answer to our question.
So in- our, um, the answer to our question would be, uh, Fulvestrant and, uh, uh, Paclitaxel, um, uh, drug, right? So the point is, um, that we have now, uh, two entities that are answer to our query, if we think of it as a knowledge graph, uh, uh, traversal, uh, type task.
And of course, similarly to what I was saying before, is a given- if some of the links on the path are missing, which is usually the case, then the- then a given entity would not be, uh, will not be able to predict or identify that it is the answer to our query.
So for example, if, uh, we don't know that, uh, ESR2 is associated with breast cancer, then the- then there is no way for us to discover that Fulvestrant is actually the answer, uh, to our question.
So, uh, again, if the knowledge graphs are incomplete, knowledge graph traversal, um, won't work.
So the question then becomes, how can we use embeddings to, uh, in some sense, implicitly impute these missing relations, um, and also, uh, how would we even be able to figure out that, you know, in this case, uh, you know, that there should be a link between ESR2 and breast cancer? And the hope is, right, that our method who will take a look at the entire knowledge graph will see that basically, uh, ESR2 here is also associated with, um, uh, ESR1 and, uh, uh, BRCA1, right? And we see that there are kind of these strong relations here.","""Shortness of breath is not caused by Paclitaxel! So why did you make a link between them?"""
303,r4KjHEgg9Wg,"So basically what the idea is that I mean you've got this bounds table here and it's got a bunch of entries.
But it basically needs entries to cover all of p size, all the allocation size.
OK, so in this case it was very simple because basically this is just one slot, due to the size.
Here it's multiple slot sizes, right.
So what's going to happen is that imagine then that we had a pointer that's moving in the range of p.
You have to have some of the back end table slot for each one of those places where p [INAUDIBLE], right.
And so it's this second piece that makes the paper a little bit confusing I think.
But it doesn't really go into depth about that, but this is how that works.","""When you check whether 'p' is out of bounds, you actually have access to 'p'. This means you can obtain the binary logarithm of the allocation size directly from table[p >> slot_size]. Why do you need the table to cover every possible allocation size of 'p'?"""
304,rMq21iY61SE,"So the way we are going to do this, um, is that we are going to represent, uh, a graph, as a- as a- with an adjacency matrix.
Um, and we are going to think of this, um, in terms of its adjacency matrix, and we are not going to assume any feature, uh, uh, represe- features or attributes, uh, on the nodes, uh, of the network.
So we are just going to- to think of this as a- as a- as a set of, um, as a- as an adjacency matrix that we wanna- that we wanna analyze.","The sentence can be revised for grammatical correctness and clarity while preserving the original meaning as follows:

""The text states that, for the sake of simplicity, no node features are used when creating node embeddings. However, if nodes did have features, would it be acceptable to concatenate these features to the feature representation embedding (i.e., a vector in R^d) obtained from, for instance, node2vec, and then use this augmented vector as the input for the downstream prediction task?"""
305,rUxP7TM8-wo,"And then they said, well, since they're falling at random, the ratio of the needles in the circle to needles in the square should exactly equal the area of the square over the area of the circle, exactly, if you did an infinite number of needles.
Does that make sense? Now, given that, you can do some algebra and solve for the area of the circle and say it has to be the area of the square times the number of needles in the circle divided by the needles in the square.
And since we know that the area of the circle is pi that tells us that pi is going to equal four times the needles in the circle.
That's 4 is the area of the square divided by the number of needles in the square.
And so the argument was you can just drop a bunch of these needles, see where they land, add them up and from that you would magically now know the actual value of pi.
Well, we tried a simulation one year in class but rather than using needles we had an archer and we blindfolded him so he would shoot arrows at random and we would see where they ended up.","I am confused by the equation: ""needle in a circle/needle in a square = area of the circle/area of the square."""
306,rUxP7TM8-wo,"Now, why would I want to look at the fraction within approximately 200 of the mean? What is that going to correspond to in this case? Well, if I divide 200 by 2 I get 100.
Which happens to be the standard deviation.
So in this case, what I'm going to be looking at is what fraction of the values fall within two standard deviations of the mean? Kind of a check on the empirical rule, right? All right, when I run the code I get this.
So it is a discrete approximation to the probability density function.
You'll notice, unlike the previous picture I showed you which was nice and smooth, this is jaggedy.
You would expect it to be.
And again, you can see it's very nice that the peak is what we said the mean should be, 0.
And then it falls off.
And indeed, slightly more than 95% fall within two standard deviations of the mean.
I'm not even surprised that it's a little bit more than 95% because, remember the magic number is 1.96, not 2.
But since this is only a finite sample, I only want it to be around 95.
I'm not going to worry too much whether it's bigger or smaller.
All right? So random.gauss does a nice job of giving us Gaussian values.
We plotted them and now you can see that I've got the relative frequency.",Can someone explain what the code v[0][] means?
307,rUxP7TM8-wo,"So that is something we believe is true by the math we've been looking at for the last two lectures.
Next statement, with a probability of 0.95, the actual value of pi is between these two things.
Is that true? In fact, if I were to say, with a probability of 1, the actual value is pi is between those two values, would it be true? Yes.
So they are both true facts.
However, only the first of these can be inferred from our simulation.
While the second fact is true, we can't infer it from the simulation.
And to show you that, statistically valid is not the same as true, we'll look at this.
I've introduced a bug in my simulation.
I've replaced the 4 that we saw we needed by 2, now, an easy kind of mistake to make.
And now, if we go to the code-- well, what do you think will happen if we go to the code and run it? We'll try it.
We'll go down here to the code.
We'll make that a 2.
And what you'll see as it runs is that once again we're getting very nice confidence intervals, but totally bogus values of pi.
So the statistics can tell us something about how reproducible our simulation is but not whether the simulation is an actually, accurate model of reality.","The slide states, ""Both are factually correct,"" yet I am puzzled as to how the second statement can be true. Is it accurate to claim that the value of pi falls between X and Y with a 95% probability when, in reality, we are certain that pi's value lies within X and Y with a 100% probability? The second statement suggests there is a 5% chance that pi does not fall between X and Y, which is incorrect."
308,rUxP7TM8-wo,"So the first bin might be, well, let's say we only had values ranging from 0 to 100.
The first bin would be all the 0's, all the 1's up to all the 99's.
And it weights each value in the bin by 1.
So if the bin had 10 values falling in it, the y-axis would be a 10.
If the bin had 50 values falling in it, the y-axis would go up to 50.
You can tell it how much you want to weight each bin, the elements in the bins.
And say, no, I don't want them each to count as 1, I want them to count as a half or a quarter, and that will change the y-axis.
So that's what I've done here.
What I've said is I've created a list and I want to say for each of the bins-- in this case I'm going to weigh each of them the same way-- the weight is going to be 1 over the number of samples.
I'm multiplying it by the len of dist, that will be how many items I have.
And that will tell me how much each one is being weighted.
So for example, if I have, say, 1,000 items, I could give 1,000 values and say, I want this item weighted by 1, and I want this item over here weighted by 12 or a half.","""What is a bin?"""
309,rUxP7TM8-wo,"What are the values on the y-axis? We kind of would like to interpret them as probabilities, right? But we could be pretty suspicious about that and then if we take this one point that's up here, we say the probability of that single point is 0.4.
Well, that doesn't make any sense because, in fact, we know the probability of any particular point is 0 in some sense, right? So furthermore, if I chose a different value for sigma, I can actually get this to go bigger than 1 on the y-axis.
So if you take sigma to be say, 0.1-- I think the y-axis goes up to something like 40.
So we know we don't have probabilities in the range 40.
So if these aren't probabilities, what are they? What are the y values? Well, not too surprising since I claimed this was a probability density function, they're densities.
Well, what's a density? This makes sense.
I'll say it and then I'll try and explain it.
It's a derivative of the cumulative distribution function.
Now, why are we talking about derivatives in the first place? Well, remember what we're trying to say.
If we want to ask, what's the probability of a value falling between here and here, we claim that that was going to be the area under this curve, the integral.","Could you elaborate on the statement ""the probability of any particular point being chosen is 0""?"
310,rmVRLeJRkl4,"And so at that point it's sort of interesting thing has happened that we've ended up getting straight back exactly the softmax formula probability that we saw when we started.
And we can just rewrite that more conveniently as saying this equals U0 minus the sum over X equals 1 to V of the probability of X given C times UX.
And so what we have at that moment is this thing here is an expectation.
And so this isn't an average over all the context vectors waited by their probability according to the model.
And so it's always the case with these softmax style models, that what you get out for the derivatives is you get the observed minus the expected.
So our model is good if our model on average predicts exactly the word vector that we actually see.","""This might be silly, but after we take the derivative with respect to ux, why do we lose the transpose operator?""
Calculus question from a beginner: Why don't the two terms cancel each other out when summing over u_x^T*v_c from w=1 to V?
How are the initial probabilities of the context word vectors calculated? They are mentioned, but the method of their determination is not specified."
311,rmVRLeJRkl4,"And so at that point we have to actually remember something, we have to remember that the derivative of the log is the 1 on X function.
So this is going to be equal to the 1 on X for Z.
So that's then going to be 1 over the sum of W equals 1 to V of exp of UTVC multiplied by the derivative of the inner function.
So the derivative of the part that is remaining, I'm getting this, the sum of-- and this one trick here at this point we do want to have a change of index.
So we want to say the sum of X equals 1 to V of exp of U of X VC.
Since we can get into trouble if we don't change that variable to be using a different one.","""I don't understand how to get it.""
Why is the change in the variable necessary at this point? Does it not represent the same quantity whether we use uw or ux?"
312,rmVRLeJRkl4,"OK, so at that point we're making some progress, but we still want to work out the derivative of this.
And so what we want to do is apply the chain rule once more.
So now here is our F and in here is our new Z equals G of VC.
And so, we then sort of repeat over, so we can move the derivative inside a sum always.
So we then taking the derivative of this, and so then the derivative of exp is itself, so we're going to just have exp of the UXTVC times there's is a sum of X equals 1 to V times the derivative of UX TVC.
OK, and so then this is what we've worked out before, we can just rewrite as UX.
OK, so now we're making progress.
So if we start putting all of that together, what we have is the derivative, well the partial derivatives with VC of this log probability.
All right, we have the numerator, which was just U0 minus-- we then had the sum of the numerator, sum over X equals 1 to V of exp UXTVC times U of X, then that was multiplied by our first term that came from the 1 on X, which gives you the sum of W equals 1 to V of the exp of UWTVC.
And this, the fact that we changed the variables became important.
And so by just sort of rewriting that a little, we can get that equals U0 minus the sum V equals sorry-- X equals 1 to V of this X, V of XTVC over the sum of W equals to V of exp UWTVC times U of X.","""When we take the derivative of a chain, why do we lose the transpose operation? For example, why is there just u_x and not u_x^T?"""
313,rmVRLeJRkl4,"So dot product is a natural measure for similarity between words because in any particular mention of opposite, you'll get a component that adds to that dot product sum.
If both are negative, it'll add a lot to the dot product sum.
If one's positive and one's negative, it'll subtract from the similarity measure.
Both of them are zero, won't change the similarity.
So it sort of seems sort of plausible idea to just take a dot product and thinking, well, if two words have a larger dot product, that means they're more similar.
And so then after that, we sort of really doing nothing more than OK, we want to use dot products to represent words similarity.","What do ""uo"" and ""vc"" stand for?"
314,rmVRLeJRkl4,"So to start to make that a bit more concrete, this is what we're doing.
So we have a piece of text, we choose our center word which is here in two and then we say, well, if a model of predicting the probability of context words given the center word and this model, we'll come to in a minute, but it's defined in terms of our word vectors.
So let's see what probability it gives to the words that actually occurred in the context of this word.
It gives them some probability, but maybe it'd be nice if the probability it assigned was higher.
So then how can we change our word vectors to raise those probabilities? And so we'll do some calculations with into being the center word, and then we'll just go on to the next word and then we'll do the same kind of calculations, and keep on chunking.
So the big question then is, well what are we doing for working out the probability of a word occurring in the context of the center word? And so that's the central part of what we develop as the word2vec object.
So this is the overall model that we want to use.
So for each position in our corpus, our body of text, we want to predict context words within a window of fixed size M, given the center word WJ.
And we want to become good at doing that, so we want to give high probability to words that occur in the context.
And so what we're going to do is we're going to work out what's formerly the data likelihood as to how good a job we do at predicting words in the context of other words.
And so formally that likelihood is going to be defined in terms of our word vectors.
So they're the parameters of our model, and it's going to be calculated as taking the product of using each word as the center word, and then the product of each word and a window around that of the probability of predicting that context word in the center word.","""Isn't 'wt' the central word instead of 'wj' on slide 23?"""
315,rmVRLeJRkl4,"So when you want to collapse two vectors for the same work, did you usually take the average? Different people have done different things.
But the most common practice is after you've-- there's still a bit more I have to cover about running word2vec that we didn't really get through today.
So I still got a bit more work to do on Thursday, but once you've run your word2vec algorithm and you sort of your output is two vectors for each word and kind of when it's center and when its context, and so typically people just average those two vectors and say OK, that's the representation of the word croissant, and that's what appears in the sort of word vectors file, like the one I loaded.
Makes sense, thank you.
I think-- so my question is, if a word to have two different meanings or multiple different meanings, can we still represent it as the same single vector? Yes, that's a very good question.
And actually there is some content on that in Thursday's lecture, so I can say more about that.
But yeah, the first reaction is you kind of should be scared because something I've said nothing about at all is most words, especially short common words have lots of meanings.
So if you have a word like star, that can be astronomical object or it can be a film star, a Hollywood star, or it can be something like the gold stars that you got in elementary school.",Why would you average the two vectors together when it might be useful to maintain both vectors separately for different tasks that need to be completed?
316,sh3EPjhhd40,"ROFESSOR PATRICK WINSTON: You know, some of you who for instance-- I don't know, Sonya, Krishna, Shoshana-- some of you I can count on being here every time.
Some of you show up once in a while.
The ones of you who show up once in a while happen to be very lucky if you picked today, because what we're going to do today is I'm going to tell you stuff that might make a big difference in your whole life.
Because I'm going to tell you how you can make yourself smarter.
No kidding.
And I'm also going to tell you how you can package your ideas so you'll be the one that's picked instead of some other slug.
So that's what we're going to do today.
It's the most important lecture of the semester.
The sleep lecture is only the second most important.","What does ""sleep lecture"" mean?
To what is he referring when he mentions the ""sleep lecture""?"
317,soZv_KKax3E,"If we look at this-- and I'm looking just for the uniform distribution, but we'll see the same thing for all three-- it more or less doesn't matter.
Quite amazing, right? If you have a bigger population, you don't need more samples.
And it's really almost counterintuitive to think that you don't need any more samples to find out what's going to happen if you have a million people or 100 million people.
And that's why, when we look at, say, political polls, they're amazingly small.
They poll 1,000 people and claim they're representative of Massachusetts.
This is good news.
So to estimate the mean of a population, given a single sample, we choose a sample size based upon some estimate of skew in the population.","""Well, when he compares different distributions to population size, why does the uniform distribution at a sample size of 25 have a difference of approximately 7.5%, while in the next slide, it's about 25%?"""
318,soZv_KKax3E,"So here's what they look like.
Quite different, right? We've looked at uniform and we've looked at Gaussian before.
And here we see an exponential, which basically decays and will asymptote towards zero, never quite getting there.
But as you can see, it is certainly not very symmetric around the mean.
All right, so let's see what happens.
If we run the experiment on these three distributions, each of 100,000 point examples, and look at different sample sizes, we actually see that the difference between the standard deviation and the sample standard deviation of the population standard deviation is not the same.
We see, down here-- this looks kind of like what we saw before.
But the exponential one is really quite different.
You know, its worst case is up here at 25.
The normal is about 14.
So that's not too surprising, since our temperatures were kind of normally distributed when we looked at it.
And the uniform is, initially, much better an approximation.
And the reason for this has to do with a fundamental difference in these distributions, something called skew.
Skew is a measure of the asymmetry of a probability distribution.
And what we can see here is that skew actually matters.","""Well, when he compares different distributions with population size, why does the uniform distribution have a difference of more or less 7.5% at a sample size of 25, as shown in one slide, and about 25% in the next?"""
319,soZv_KKax3E,"Well, we can again run the experiment.
I did run the experiment.
I changed the sample size from 100 to 200.
And, again, you can run this if you want.
And if you run it, you'll get a result-- maybe not exactly this, but something very similar-- that, indeed, as I increase the size of the sample rather than the number of the samples, the standard deviation drops fairly dramatically, in this case from 0.94 0.66.
So that's a good thing.
I now want to digress a little bit before we come back to this and look at how you can visualize this-- Because this is a technique you'll want to use as you write papers and things like that-- is how do we visualize the variability of the data? And it's usually done with something called an error bar.","""Increase the size of the sample rather than the number of samples. What does he mean?"""
320,tKwnms5iRBU,"Either way, let's just suppose than an edge e-- I should mention, I guess I didn't say, this graph is undirected.
A minimum spanning tree doesn't quite make sense with directed graphs.
There are other versions of the problem but here, the graph is undirected.
So probably, I should write this as a unordered set, u, v.
And there are possibly many minimum spanning trees.
There could be many solutions with the same weight.
For example, if all of these edges have weight 1, all of these trees are actually minimum.
If all the edges have weight 1, every spanning tree is minimum, because every spanning tree has exactly n minus 1 edges.
But let's suppose we know an edge that's guaranteed to be in some minimum spanning tree, at least one.
What I would like to do is take this, so let me draw a picture.
I have a graph.
We've identified some edge in the graph, e, that lives in some minimum spanning tree.
I'm going to draw some kind of tree structure here.
OK.
The wiggly lines are the tree.
There are some other edges in here, which I don't want to draw too many of them because it's ugly.
Those are other edges in the graph.
Who knows where they are? They could be all sorts of things.
OK? But I've highlighted the graph in a particular way.
Because the minimum spanning tree is a tree, if I delete e from the tree, then I get two components.
Every edge I remove-- I'm minimally connected.
So if I delete an edge, I disconnect into two parts, so I've drawn that as the left circle and the right circle.
It's just a general way to think about a tree.
Now there are other unused edges in this picture, who knows where they live? OK? What I would like to do is somehow simplify this graph and get a smaller problem, say a graph with fewer edges.
Any suggestions on how to do that? I don't actually know where all these white edges are, but what I'd like to do is-- I'm supposing I know where e is, and that's an edge in my minimum spanning tree.
So how could I get rid of it? Yeah.
AUDIENCE: Find the minimum weight spanning tree of the two edges.","""In the example provided, are the two sets of vertices supposed to be connected in the spanning tree by paths that do not include edge e?"""
321,tKwnms5iRBU,"In fact, the minimum spanning tree, T star, has to connect vertex u to vertex v, somehow.
It doesn't use e, but there's got to be-- it's a tree, so in fact, there has to be a unique path from u to v in the minimum spanning tree.
And now u is in S, v is not in S.
So if you look at that path, for a while, you might stay in S, but eventually you have to leave S, which means there has to be an edge like this one, which I'll call it e prime, which transitions from S to V minus S.
So there must be an edge e prime in the minimum spanning tree that crosses the cut, because u and v are connected by a path and that path starts in S, ends not in S, so it's got to transition at least once.
It might transition many times, but there has to be at least one such edge.
And now what I'm going to do is cut and paste.
I'm going to remove e prime and add an e instead.
So I'm going to look at T star minus e prime plus e.
I claim that is a minimum spanning tree.
First I want to claim, this is maybe the more annoying part, that it is a spanning tree.
This is more of a graph theory thing.
I guess one comforting thing is that you've preserved the number of edges, so it should still be if you get one property, you get the other, because I remove one edge, add in one edge, I'm still going to have n minus 1 edges.","I believe Professor Demaine made a slight error relevant to your question in his proof that every edge of least weight crossing a cut in an (undirected) graph is part of a minimum spanning tree of that graph. He previously discussed how, in a possibly different minimum spanning tree, T*, there exists a unique path from vertex u to vertex v that must also cross the cut. However, he did not apply this lemma in his proof. In his argument, he stated that we could select any edge e' in T* that crosses the cut. I think he intended to specify that e' should also be one of the edges on the unique path from u to v within T*. By doing so, removing e' would disconnect the subgraphs containing u and v, and then adding edge e would reconnect them without forming a cycle."
322,tKwnms5iRBU,"There's no point in keeping the higher weight one, so I'm just going to throw away the higher weight one.
Take them in.
So this is a particular form of edge contraction and graphs.
And I claim it's a good thing to do, in the sense that if I can find a minimum spanning tree in this new graph-- this is usually called a G slash e, slash instead of negative, to remove e.","""Doesn't the picture suggest that the graph is cyclic? However, spanning trees are acyclic. It seems like a case of a poor example, or am I overlooking something?"""
323,tKwnms5iRBU,"We assumed that e is in there.
So I'm basically removing, I'm just deleting that edge, maybe I should call it minus e.
Then that should be a spanning tree of G slash e.
So when I contract the edge in the graph, if I throw away the edge from this spanning tree, I should still have a spanning tree, and I don't know whether it's minimum.
Probably, it is, but we won't prove that right now.
I claim it's still a spanning tree.
What would that take? It still hits all the vertices, because if I removed the edge, things would not be connected together.
But this edge was in the spanning tree, and then I fused those two vertices together, so whatever spanning-- I mean, whatever was connected before is still connected.","""Isn't it true that T* was divided into two subtrees because he deleted the edge instead of merging it? Can we then say that the two trees together form a spanning tree?"""
324,tOsdeaYDCMk,"Now I just play a nice trick to get these 2 exponents to look alike.
I say that the depth of s is less than or equal to the max of the depth of s and the depth of t, and likewise for the depth of t.
So in both of those terms here, I can replace the exponent or replace the depth of s by the max of depth s and t, and likewise here.
Now I've got the same term twice, so I can say that it's simply twice the max depth.
And of course, that is equal to, by definition of the depth of r, twice 2 to the depth of r, which is of course 2 to the depth of r plus 1.","He stated that max(d(s), d(t)) + 1 equals d(r), but previously he claimed it was max(1 + d(s), d(t)). These two expressions are not necessarily equivalent; am I overlooking something?"
325,tOsdeaYDCMk,"This is a somewhat interesting one.
Let's define the depth of a string as follows, and the idea is it's how deeply nested are the successive pairs of left and right brackets.
Well, the depth of the empty string is 0.
You got to start somewhere, and it's got no brackets, we'll call it depth 0.
Now, what about the depth of the constructor putting brackets around s and then following it by t? Well, putting brackets around s gives you a string that's 1 deeper than s is, and then you follow it by t, and it's as deep as t is.
So the result is that the depth of the constructor is a string which is a number which is equal to 1 plus the depth of s and the depth of t, whichever is larger.
The max of 1 plus depth of s and depth of t, and that's our recursive definition of depth.
Let's look at maybe another even more familiar example of recursive definition.
Let's define the nth power of an integer or real number k.
The zeroth power k is defined to be 1, and the n plus first power of k is defined to be k times the nth power of k, and this would be an executable definition of the exponentiation function in a lot of programming languages.
And my point here is that this familiar definition, recursive definition on a nonnegative integer n, is in fact a structural induction using the fact that the nonnegative integers can be defined recursively as follows.
0 is a nonnegative integer, and if n is a nonnegative integer, then n plus 1 is a nonnegative integer.","He stated that max(d(s), d(t)) + 1 equals d(r), but previously he claimed it was max(1 + d(s), d(t)). These two expressions are not necessarily equivalent; am I overlooking something?"
326,tkJd2B98hII,"Why? This is basically the Rademacher complexity of the hypothesis class h, but not the family of losses, right? Before we were talking about a hypothesis class of the family of losses.
And now you're talking about exactly the Rademacher complexity of the hypothesis class h.
So basically, this is saying that for binary-- I think I'm missing something.
I'm missing 1/2 here.
Where did the 1/2 go? Yeah, I think I lost the 1/2.
Sorry.
I think I lost-- oh, I have the 1/2 in the notes.
It's just I forgot to copy it.
So 1/2.
So basically, it's the 1/2 times the Rademacher complexity.
So what we achieved is that the Rademacher complexity of f in this special case of binary classification and 0, 1 loss is equal to 1/2 times the Rademacher complexity of the hypothesis class.
So that's a slightly simpler way of thinking about this.
Because what's this? This is basically saying that how well h can memorize the random label.
You can think of sigma 1 up to sigma n as random label.
And R and h is big when you can-- there exist an h in the capital H such that h of xi is equals to sigma i.
This is the best situation, right? This has the strongest correlation.
So basically, if you can memorize all the random label with some hypothesis, hypothesis class, that means your Rademacher complexity is the biggest.
And that gives you the worst generalization bound.
And vice versa, if you cannot memorize, then you get better generalization bound.
Right.
OK.
I have a question [INAUDIBLE].
So [INAUDIBLE] yi [INAUDIBLE].
But [INAUDIBLE]? I see.","""When the professor mentioned that R_n(H) describes 'how well H can memorize random labels,' what exactly does 'memorize' mean in this context? I'm still somewhat confused."""
327,tutlI9YzJ2g,"And in practice, uh, this means it requires tuning the learning rate.
And this SGD idea is kind of a common core idea that then many other, um, optimizers, uh, improve on, like ada- adagrad, adadelta, M RMSprop and so on.
Essentially, all use this core idea of selecting the subsets of the- subset of data, evaluating the gradient over it and making the steps.
Now- now the details, uh, vary in terms of what data points you select, how big of a step you make, how do you decide on the step size, um, and so on and so forth.
But essentially, this minibatch stochastic gradient descent is the core of, um, optimization in deep learning.
So now that we have discussed the objective function, we discussed the notion of a minibatch, we discussed the notion of a stochastic gradient descent, now, we need to, uh, talk about how is- is this actually done? How are these, um, gradients, uh, computed, evaluated, right.
Because in the old days, pre-deep learning, you actually had to write down the model with the set of equations and then you have to do by hand computed these gradients essentially, you know, like we did it in high school, uh, many of you are computing the gradients by hand- by hand on the whiteboard.","Why is the notation for the Jacobian ""W W^T""? Doesn't ""W^T"" usually stand for the transpose of W?"
328,tutlI9YzJ2g,"And now I want to evaluate f, uh, with respect to the loss.
So we have this kind of, uh, nesting or chaining of functions.
And if I wanna do back-propagation now, back-propagation means I have to now compute the derivative, the gradient, and I wanna work backward.
So what does this means is that if these are my model parameters, I start from the loss and compute gradients backwards.
So I would start with a loss, for example, and I'm interested to compute the gradient of the loss, uh, with respect to W2.
Then I have to go from the loss, compute- take the derivative with respect to f, and then I have to take, uh, f and take a derivative with respect to W2, right? So I went from lost to f to W, uh,2.
While, for example, to compute the derivative of the loss, uh, with the- with respect to W_1, I have to take the- the loss compute f of the derivative with respect to f.
Take f compute the derivative with respect to W_2, and then kind of take the result of that W_2, take a derivative, uh, with respect to, uh, W_1.
And you can see kind of how I'm working backwards and how, uh, as I go deeper into the network, I can kind of re-use, uh, uh, previous computations.
And this is why this is called a back-propagation because I kind of- kind of, uh, working backwards, um, uh, from the output all the way towards the, uh- the input.","""On slide 24, when we compute the derivative of the loss with respect to W_1 (as shown in the bottom right formula), shouldn't the W_2 terms in the formula be replaced by the function h?"""
329,tw6cmL5STuY,"So now you've gone from, um, I guess n parameters to just one parameter, right? Uh, and this means that you are constraining the covariance matrix to- you are constraining the Gaussian you use to have circular contours.
So this is an example where you can model.
Uh, and this would be another example, right? And this is- I guess this is another example, okay? So you can model things like this, where every feature, not only is every feature uncorrelated but every feature further has the same variance as every other feature.
Um, and the maximum likelihood is this, okay? And again, not, not, not a huge surprise, just the average over, uh, the previous values.
So what we'd like to do is, um, not quite use either of these options, right? Which assumes- really, the biggest problem is it assumes the features are uncorrelated.
Um, and what I'd like to do is build the model that you can fit even when you have very high dimensional data and a relatively small number of examples, um, but that allows you to capture some of the correlations, right? So if you have 30 temperature sensors in this room, you know, probably there are some correlations, right? Probably, this side of the room temperature is gonna be correlated, and that side of the room temperature is gonna be correlated and maybe the ambient temperature in this whole building.
The, the temperature of this room really goes up and down as a whole, but maybe some of the lamps on the side heat up that side of the room a bit more, so different, the different.
There are correlations but maybe you don't need a full covariance matrix either.
So what [NOISE], what factor analysis will do is, um, give us a model that you can fit even when you have, you know, [NOISE] 100 dimensional data and 30 examples.
They capture some of the correlations but that doesn't run into the a, a- uninvertible, um, covariance matrixes is that the naive Gaussian model does, okay? All right.
So let me- just check any- let me, let me describe the model, let me just check, any questions before I move on? Okay.","""Is this related to singular value decomposition?"""
330,uK5yvoXnkSk,"You'll notice it will always be the case that the leftmost leaf of this tree has got all the possible items in it, and the rightmost leaf none.
And then I just check which of these leaves meets the constraint and what are the values.
And if I compute the value and the calories in each one, and if our constraint was 750 calories, then I get to choose the winner, which is-- I guess, it's the pizza and the burger.
Is that right? The most value under 750.
That's the way I go through.
It's quite a straightforward algorithm.
And I don't know why we draw our trees with the root at the top and the leaves at the bottom.
My only conjecture is computer scientists don't spend enough time outdoors.","Should the better option be a beer and burger combo valued at 140 with fewer than 750 calories, instead of choosing a pizza and burger combo valued at 80?"
331,uXt8qF2Zzfo,"And these will tell us how much improvement we're getting by making a little movement in those directions, right? How much a change is given that we're just going right along the axis.
So maybe what we ought to do is if this guy is much bigger than this guy, it would suggest we mostly want to move in this direction, or to put it in 1801 terms, what we're going to do is we're going to follow the gradient.
And so the change in the w vector is going to equal to this partial derivative times i plus this partial derivative times j.
So what we're going to end up doing in this particular case by following that formula is moving off in that direction right up to the steepest part of the hill.
And how much we move is a question.
So let's just have a rate constant R that decides how big our step is going to be.
And now you think we were done.
Well, too bad for our side.
We're not done.
There's a reason why we can't use-- create ascent, or in the case that I've drawn our gradient, descent if we take the performance function the other way.","Hello Sobhy, it appears you are quite knowledgeable about neural networks. I have a question regarding backpropagation. I understand that we aim to minimize our error function, and to do so, we calculate the partial derivatives of the weights W_1, ..., W_n. My question is, how do we apply stochastic gradient descent to determine the optimal weights? Is it similar to the method you described previously?
Thank you for uploading such an awesome lecture. However, there is one point I didn't understand: Could anyone please explain what 'i' and 'j' represent in the function used to calculate the delta of the weights? Did I miss the part where the professor explained their origin?"
332,uXt8qF2Zzfo,"And we need to look at the picture.
And the reason I turned this guy around was actually because from a point of view of letting the math sing to us, this piece here is the same as this piece here.
So part of what we needed to do to calculate the partial derivative with respect to w1 has already been done when we calculated the partial derivative with respect to w2.
And not only that, if we calculated the partial wit respect to these green w's at both levels, what we would discover is that sort of repetition occurs over and over again.
And now, I'm going to try to give you an intuitive idea of what's going on here rather than just write down the math and salute it.
And here's a way to think about it from an intuitive point of view.
Whatever happens to this performance function that's back of these p's here, the stuff over there can influence p only by going through, and influence performance only going through this column of p's.
And there's a fixed number of those.
So it depends on the width, not the depth of the network.
So the influence of that stuff back there on p is going to end up going through these guys.
And it's going to end up being so that we're going to discover that a lot of what we need to compute in one column has already been computed in the column on the right.
So it isn't going to explode exponentially, because the influence-- let me say it one more time.
The influences of changes of changes in p on the performance is all we care about when we come back to this part of the network, because this stuff cannot influence the performance except by going through this column of p's.","""Dynamic programming employs a similar concept to prevent exponential blowup. Perhaps backpropagation could also be considered a form of dynamic programming."""
333,uXt8qF2Zzfo,"No, the output is z.
So it's z time 1 minus e.
So whenever we see the derivative of one of these sigmoids with respect to its input, we can just write the output times one minus alpha, and we've got it.
So that's why it's mathematically convenient.
It's mathematically convenient because when we do this differentiation, we get a very simple expression in terms of the output.
We get a very simple expression.
That's all we really need.
So would you like to see a demonstration? It's a demonstration of the world's smallest neural net in action.
Where is neural nets? Here we go.
So there's our neural net.
And what we're going to do is we're going to train it to do absolutely nothing.",Why is it z(1-z) instead of P2(1-P2)?
334,uXt8qF2Zzfo,"So it's not going to blow up exponentially.
We're going to be able to reuse a lot of the computation.
So it's the reuse principle.
Have we ever seen the reuse principle at work before.
Not exactly.
But you remember that little business about the extended list? We know that we've seen-- we know we've seen something before.
So we can stop computing.
It's like that.
We're going to be able to reuse the computation.
We've already done it to prevent an exponential blowup.
By the way, for those of you who know about fast Fourier transform-- same kind of idea-- reuse of partial results.
So in the end, what can we say about this stuff? In the end, what we can say is that it's linear in depth.","""Dynamic programming uses a similar concept to avoid exponential blowup, and perhaps backpropagation could also be considered a form of dynamic programming."""
335,uXt8qF2Zzfo,"So that's going like a zipper down that string of variables expanding each by using the chain rule until we got to the end.
So there are some expressions that provide those partial derivatives.
But now, if you'll forgive me, it was convenient to write them out that way.
That matched the intuition in my head.
But I'm just going to turn them around.
It's just a product.
I'm just going to turn them around.
So partial p2, partial w2, times partial of z, partial p2, times the partial of p with respect to z-- same thing.
And now, this one.
Keep me on track, because if there's a mutation here, it will be fatal.
Partial of p1-- partial of w1, partial of y, partial p1, partial of p2, partial of y, partial of z.",Why did the 'y' change to a partial 'y'?
336,uXt8qF2Zzfo,"So we can test the other way like so.
And we can see that the desired output is pretty close to the actual output in that case too.
And it took 694 iterations to get that done.
Let's try it again.
To 823-- of course, this is all a consequence of just starting off with random weights.
By the way, if you started with all the weights being the same, what would happen? Nothing because it would always stay the same.
So you've got to put some randomization in in the beginning.
So it took a long time.
Maybe the problem is our rate constant is too small.
So let's crank up the rate counts a little bit and see what happens.
That was pretty fast.
Let's see if it was a consequence of random chance.
Run.
No, it's pretty fast there-- 57 iterations-- third try-- 67.
So it looks like at my initial rate constant was too small.
So if 0.5 was not as good as 5.0, why don't we crank it up to 50 and see what happens.
Oh, in this case, 124-- let's try it again.
Ah, in this case 117-- so it's actually gotten worse.
And not only has it gotten worse.
You'll see there's a little a bit of instability showing up as it courses along its way toward a solution.","""Starting off with equal weights does not necessarily mean they will remain unchanged. If the neurons were in the same layer, it might be the case, but here they are not. Am I overlooking something?"""
337,uXt8qF2Zzfo,"So we got rid of it.
Now, we're a situation where we can actually take those partial derivatives, and see if it gives us a way of training the neural net so as to bring the actual output into alignment with what we desire.
So to deal with that, we're going to have to work with the world's simplest neural net.
Now, if we've got one neuron, it's not a net.
But if we've got two-word neurons, we've got a net.
And it turns out that's the world's simplest neuron.
So we're going to look at it-- not 60 million parameters, but just a few, actually, just two parameters.
So let's draw it out.
We've got input x.
That goes into a multiplier.
And it gets multiplied times w1.
And that goes into a sigmoid box like so.
We'll call this p1, by the way, product number one.
Out here comes y.
Y gets multiplied times another weight.
We'll call that w2.
The neck produces another product which we'll call p2.
And that goes into a sigmoid box.
And then that comes out as z.
And z is the number that we use to determine how well we're doing.
And our performance function p is going to be one half minus one half, because I like things are going in a direction, times the difference between the desired output and the actual output squared.","Why did he divide by 2? Is it because of the two neurons?
""Does anyone know the origin of the 1/2 in this context?""
What do we call the diagram shown here?"
338,uXt8qF2Zzfo,"Think about it.
If this is t, and this is minus 1, then this is minus t.
And so this thing ought to fire if everything's over-- if the sum is over 0.
So it makes sense.
And it gets rid of the threshold thing for us.
So now we can just think about weights.
But still, we've got that step function there.
And that's not good.
So what we're going to do is we're going to smooth that guy out.
So this is trick number two.
Instead of a step function, we're going to have this thing we lovingly call a sigmoid function, because it's kind of from an s-type shape.
And the function we're going to use is this one-- one, well, better make it a little bit different-- 1 over 1 plus e to the minus whatever the input is.
Let's call the input alpha.
Does that makes sense? Is alpha is 0, then it's 1 over 1 plus 1 plus one half.
If alpha is extremely big, then even the minus alpha is extremely small.
And it becomes one.
It goes up to an asymptotic value of one here.
On the other hand, if alpha is extremely negative, than the minus alpha is extremely positive.
And it goes to 0 asymptotically.
So we got the right look to that function.
It's a very convenient function.
Did God say that neurons ought to be-- that threshold ought to work like that? No, God didn't say so.
Who said so? The math says so.
It has the right shape and look and the math.
And it turns out to have the right math, as you'll see in a moment.
So let's see.
Where are we? We decided that what we'd like to do is take these partial derivatives.
We know that it was awkward to have those thresholds.
So we got rid of them.
And we noted that it was impossible to have the step function.","Shouldn't the input definitely be +1?
Why did he choose to smooth out the step function?"
339,uXt8qF2Zzfo,"Why can't we use it? AUDIENCE: Local maxima.
PATRICK WINSTON: The remark is local maxima.
And that is certainly true.
But it's not our first obstacle.
Why doesn't gradient ascent work? AUDIENCE: So you're using a step function.
PATRICK WINSTON: Ah, there's something wrong with our function.
That's right.
It's non-linear, but rather, it's discontinuous.
So gradient ascent requires a continuous space, continuous surface.
So too bad our side.
It isn't.
So what to do? Well, nobody knew what to do for 25 years.
People were screwing around with training neural nets for 25 years before Paul Werbos sadly at Harvard in 1974 gave us the answer.
And now I want to tell you what the answer is.
The first part of the answer is those thresholds are annoying.
They're just extra baggage to deal with.
What we really like instead of c being a function of xw and t was we'd like c prime to be a function f prime of x and the weights.","""At which point is the function not continuous?"""
340,vIFKGFl1Cn8,"We're going to see examples later on where, in fact, minimizing things where you minimize that distance is the right thing to do.
When we do machine learning, that is how you find what's called a classifier or a separator.
But actually here we're going to pick y, and the reason is important.
I'm trying to predict the dependent value, which is the y value, given an independent new x value.
And so the displacement, the uncertainty is, in fact, the vertical displacement.
And so I'm going to use y.
That displacement is the thing I'm going to measure as the distance.
How do I find this? I need an objective function that's going to tell me what is the closeness of the fit.
So here's how I'm going to do it.
I'm going to have some set of observed values.
Think of it as an array.
I've got some index into them, so the indices are giving me the x values.
And the observed values are the things I've actually measured.","Out of curiosity, what would be the result of trying to minimize the area of the triangle as opposed to minimizing the distance y?"
341,wEKFGdo4Sck,"Because I've already argued to you that it's never optimal to.
I can check this.
It's not going to be optimal.
It's going to be more optimal to play some time over here.
But how far do I have to check? Well, maybe I have to check up to 7.
Does that work? Not quite.
So let's say I played here, and I played here, and I played here, and I played here, I actually can't play here, here, here, here, here.
I'm not allowed to play those.
I guess these should be O's.
I played there.
And I'm not allowed to play here, 1, 2, 3, 4, 5.
But I am allowed to play anywhere in here.
So I basically want to shrink this until these X's collide with each other.
Because then it's possible that an optimal solution would require me to pick these two and then require me to pick these two way over there.
So this is 10 things in the middle.
I only have to go up to, at most, 11.
It's 11.
Now, you can move any constant above 11 and get the same running time bound.
But that's my analysis.
OK, so we have our recursive relation.
And so what am I doing? I'm just looping over my choices of next day to play.
I'm rehearsing on this thing where I actually do play on that day.
But I'm remembering the information about what I'm allowed to play next by limiting, based on what my previous value was.
So that's the kind of key thing.
I'm remembering something further in advance-- or I'm remembering what happened in the past by describing it as a restriction of something in the future.
So this was a pretty difficult problem.
I think it was one of our first dynamic programming problems on that term.
It was probably a little ambitious.
AUDIENCE: Are you saying this recurrence goes up to 11? JASON KU: Yes, the recurrence goes up to 11, not 10, 11.",Could someone explain this to me?
342,wEKFGdo4Sck,"Either I-- if I didn't match Ai, I may match Bj in the future.
So I only want to reduce Ai.
So xi plus 1, I leave everything else the same.
Assuming if i less than n, I don't want to move off the end of this thing, or x, i, j plus 1, ki, kj if j is less than n.
So those are my four choices.
If I match the letter n, great.
Otherwise, I decrease the size of my subproblem and I recurse.
So fun recursion, topological sort, these subproblems only depend on what? Larger i? Not quite.
Larger j? Not quite.
Changing k or-- these don't even change here.
So we're going to use depend on larger, I guess, strictly-- that's kind of an important thing, i plus j.
Because at least one of these two things is increasing.
And then the nice thing about that is it kind of tells us when we should stop.
We should stop when either i or j get to n.
We should know enough, at that point, to be able to determine if we succeeded or not, possibly.
So we have our base case.
What's the easy base case? When we succeeded.
When have we succeeded? If we have nothing left in A and B And we have nothing left in C.
I have nothing left to match.
So I have nn.
And I don't need a match anything else.
That's just going to be true.
All roads point to this subproblem to get to a true solution.
Otherwise, we have some false base cases.","""I wonder why we don't use x(i+1, j+1, k_i, k_j) in the case where neither A[i] nor B[j] matches?"""
343,wfr4XbR5VP8,"It doesn't hurt at all for the theorem that r1 and r2 are different.
And so there are two roots.
But ambiguity can be problematic.
And let me give you an illustration of that.
When there's ambiguity, I can do things like proving easily that 1 is equal to minus 1.
Here's the proof.
And I will let you contemplate that and try to figure out just where in this reasoning that step by step seems pretty reasonable, but nevertheless I've concluded that 1 is equal to minus 1, which is absurd.
It's taking advantage of the fact that you don't know whether the square root of minus 1 means i or minus i.
So the moral of all of this is that, first of all, be sure that you are applying the rules properly.
There's an assumption of an algebraic rule in there that isn't true.
And again, that kind of mindless calculation is risky when you don't really understand what you're doing, you don't have a clear memory of what the exact rules are.
So it's understanding that bails you out of this kind of blunder.
Let's look at 1 equals minus 1 a little because it lets us wrap up with an amusing remark.
What's terrible about 1 equals minus 1? Well, it's false, and you don't want to ever conclude something that's false.
That's worrisome.
It's disastrous when you conclude that something is false.
Let me give you an illustration of why.","""Good afternoon, Professor. I am guessing that mathematically, 1 equals -1; is that correct? The modulus of |-3| equals 3. I didn't understand the point..."""
344,wr9gUr-eWdA,"All right, and so this would be your region R_p right now, right? And so then you can split it into these two other regions, right? Say R_1 and R_2.
And say that what you've achieved now is you have the 700 positive, 100 negatives on this side versus, uh, 200 positives and 0 negatives on this side, okay? Now, this seems like a pretty good split since you're getting out some more examples.
But what you can see is that, if you just drew the same thing again, right, R_p with 900 and 100, split split, and say in this case, instead, you've got 400 positives over here, 100 negatives, and 500 positives and 0 negatives.
So most people would argue that this bright decision boundary is better than the left one because you're basically isolating out even more positives in this case.
However, if you're just looking at your misclassification loss, it turns out that on this left one here, let's call this R_1 and R_2 versus this right one, Let's call this R_1 prime, R_2 prime, okay? So your loss of R_1 plus R_2, on this left case it's just 100 plus 0.","Why is it that L(R_1) + L(R_2) equals 100 + 0? I was under the impression that p-hat-c represents the proportion of examples in R that belong to class C, rather than the actual count of examples in class C. According to the definition of misclassification loss, L(R_1) + L(R_2) should be calculated as (1 - 700/800) + (1 - 200/200), which equals 0.125, and L(R_1') + L(R_2') should be (1 - 400/500) + (1 - 500/500), resulting in 0.1."
345,wr9gUr-eWdA,"All right, so it's just 100.
And then on the right side here, it's actually still just the same, right? And in fact, if you'd look at the original loss of your parent it's also just 100, right? So you haven't really according to this loss metric changed anything at all.
And so that sort of brings up one problem with the misclassification loss is that, it's not really sensitive enough, okay? So like instead what we can do is we can define this cross-entropy loss, okay? So which we'll define as L_cross.
Let me just write this out here.
And so really what you're doing is you're just summing over the classes and it's the probability- that the proportion of elements in that class times the log of the proportion in that class.","He defines the classification loss as L(R) = 1 - max(p^c) across all c in C classes. Subsequently, he introduces L(parent) - sum(L(children)), initially suggesting that it should be minimized, but later revises this to maximization. However, when comparing two trees, he ignores these concepts and simply adds the values that will be misclassified for each child (100 + 0 for the first tree and 100 + 0 for the second tree). This method of adding misclassified values without considering proportions is inaccurate. When proportions are taken into account, as per L(R) = 1 - max(p^c), the quantity to be maximized is not the same for both trees. For the first tree, L(R_1) + L(R_2) equals (1 - 700/800) + (1 - 200/200), which is 0.125. For the second tree, L(R_1') + L(R_2') equals (1 - 400/500) + (1 - 500/500), which is 0.1. The discrepancy arises because the formula he defined as L(parent) - sum(L(children)) is flawed. To achieve the same value for both, one should consider the weighted average of the number of points in each node. Therefore, we abandon the misclassification loss in favor of cross-entropy."
346,wr9gUr-eWdA,"And so what is useful to define is a loss on a region, okay? So define your loss L on R, loss on R.
And so for now let's define our loss as something fairly obvious, is your misclassification loss.
It's how many examples in your region you get wrong.
And so assuming that you have, uh, given C classes total, you can define P hat c to be the proportion of examples in R that are of class c.","He defines the classification loss as L(R) = 1 - max(p^c) across all c in C classes. Subsequently, he introduces L(parent) - sum(L(children)), initially stating that this should be minimized, but later revising it to be maximized. However, when comparing two trees, he ignores these concepts and simply adds up the values that will be misclassified for each child (100 + 0 for the first tree and 100 + 0 for the second tree). This method of summing misclassified values without considering proportions is flawed. When proportions are taken into account, as per L(R) = 1 - max(p^c), the quantity to be maximized is not the same for both trees. For the first tree, L(R_1) + L(R_2) = (1 - 700/800) + (1 - 200/200) = 0.125. For the second tree, L(R_1') + L(R_2') = (1 - 400/500) + (1 - 500/500) = 0.2. The discrepancy arises because the formula he defined as L(parent) - sum(L(children)) is incorrect. To achieve the same value for both, one must consider the weighted average of the number of points in each node. Therefore, we abandon the misclassification loss in favor of cross-entropy."
347,wr9gUr-eWdA,"And yeah, it turns out it doesn't really matter, which, um, which way you put it.
It just- basically, you're trying to either minimize the loss of the children or maximize the gain in information, basically.
[inaudible].
Yeah.
Let's see.
Yeah, you're right.
That should actually be a max.
Let me fix that really quick.
Because you start with your parent loss, and then you're subtracting out your children's loss, and so the amount left, let's see, the higher this loss is- yeah.
So you really want to maximize this guy.
Makes sense, everyone? Thanks for that.
Okay, so I've sort of given this like, hand-wavy- Oh, sure, what's up? [inaudible].
So that would be log-based.
The question is, for the cross-entropy loss, is it log base 2 or log base c? It's log base 2.
Okay, here, I can write that out.
Yep.
[inaudible].
Oh, sorry, I didn't quite hear that.
[inaudible].
Okay.
Um, so the question is can- uh, what is the proportion that are correct versus incorrect for these two examples we've worked through here? Um, and so, yeah- basically, what we're starting with is, we're starting with we have 900/100, 900 positives and 100 negatives.
All right, so you can imagine that if you just stopped at this point, right, you would just cla- classify everything as positive, right, and so you get 100 negatives incorrect.","He defines the classification loss as L(R) = 1 - max(p^c), where the maximum is taken over all c in C classes. Subsequently, he introduces the concept of L(parent) - sum(L(children)), initially suggesting that it should be minimized, but later revising this to indicate that it should be maximized. However, when comparing two trees, he neglects these concepts and instead simply adds up the values that will be misclassified for each child (100 + 0 for the first tree and 100 + 0 for the second tree). This method of tallying misclassified values without considering proportions is flawed. When proportions are factored in, as per L(R) = 1 - max(p^c), the quantity to be maximized is not the same for both trees. For the first tree, L(R_1) + L(R_2) equals (1 - 700/800) + (1 - 200/200), which is 0.125. For the second tree, L(R_1') + L(R_2') equals (1 - 400/500) + (1 - 500/500), which is 0.1. The discrepancy arises because the formula he used for L(parent) - sum(L(children)) is flawed. To achieve the same value for both trees, one should consider the weighted average of the number of points in each node. Therefore, we should abandon the misclassification loss in favor of cross-entropy."
348,wr9gUr-eWdA,"So really all you're trying to do is minimize this negative sum of losses of your children, okay? So let's move to the next board here.
[NOISE] So I started to find this misclassification loss.
Let's get a little bit into actually why misclassification loss isn't actually the right loss to use for this problem, so, okay? And so for a simple example, let's pretend- So I've sort of drawn out a tree like this, Let's pretend that instead we have another setup here where we're coming into a decision node.
And at this point we have 900 positives and 100 negatives, okay? So this is sort of a misclassification loss, of 100 in this case because you'd predict the most common class and end up with 100 misclassified examples.","He defines the classification loss as L(R) = 1 - max(p^c) across all c in C classes. Subsequently, he introduces the concept of L(parent) - sum(L(children)), initially suggesting that it should be minimized, but later revising this to indicate that it should be maximized. However, when comparing two trees, he ignores these concepts and simply adds up the values that will be misclassified for each child (100 + 0 for the first tree and 100 + 0 for the second tree). This method of tallying misclassified values without considering proportions is flawed. When proportions are factored in according to L(R) = 1 - max(p^c), it becomes apparent that the quantity to be maximized is not identical for the two trees. For the first tree, L(R_1) + L(R_2) equals (1 - 700/800) + (1 - 200/200), which is 0.125. For the second tree, L(R_1') + L(R_2') equals (1 - 400/500) + (1 - 500/500), which is 0.2. The discrepancy arises because the formula he defined as L(parent) - sum(L(children)) is flawed. To achieve the same value for both, one must consider the weighted average of the number of points in each node. Therefore, we abandon the misclassification loss in favor of cross-entropy."
349,xFzvwTugIto,"But you can have variable n.
OK, thank you.
And then, asked in the chat, is task grouping considered a challenge? We see task grouping as a challenge in multi-task learning.
And I was reading a recent paper that comes up with a task grouping framework to deal with it.
Yeah, so different tasks may be more similar to one another.
Like, you could construct a task distribution that has medical images and natural images, for example.
And it may actually be helpful to learn a grouping of those tasks, if you find that your algorithm isn't finding common ground between those two sets of tasks.
And yeah, that's definitely a challenge that people have looked at, especially when you observe negative transfer between different sets of tasks.
Cool, and then one more question from [AUDIO OUT]..
What do we have, what to do if we have varying k for each task? Yeah, so you can also have varying k.","Thank you for the excellent content and for addressing the questions. Aren't medical and natural images from different distributions? Any clarification on the assumption that they come from independent and identically distributed (i.i.d.) sources would be appreciated. Personally, they seem more distinct from each other than sinusoidal functions are from other periodic functions."
350,xGDUYQGvRac,"And then, how is this-- how are we going to stop the generation-- if the edge level RNN is going to output the end of sequence at step 1, we know that no edges are connected to the new node, and we are going to stop the generation.
So it's actually the edge level RNN, and that, we have decided will determine whether we stop generating the graph or not.
So let me give you now an example.
So that you see how all this fits together.
So this is what is going to happen under the training time.
For, let's say, a given training-- observed training graph.
We are starting with the start of sequence and a hidden state.
The node level RNN will add the node, and then, the edge level RNN will be asked, shall this node that has just been added, shall it link to the previous nodes? Yes or no.
It will update the probability.
And we are then going to flip a coin that will determine-- with this given bias that will determine whether the edge is added or not.
And then, and then, we'll take this and use it as an input-- as an initialization back to the node level RNN who's now going to add the second node, and this would be node number 3.
And then, the edge level RNN is going to tell us, will node 3 link to node 1, will node 3 link to node 2.
And again, it's outputting probabilities, we are flipping the coins, whatever is the output of that coin is the input to the next level RNN.
So here are the probabilities 0.6.
Perhaps you were lucky, the output was 1, so this is the input for the next state.
And then, after we have traversed with all the previous edges, we are going over all the previous nodes, we are again going to ask node RNN to generate a new node.","""Whatever the output of that coin, it becomes the input for the next level. Is this a mistake? Since we are using teacher forcing, we should use actual values to indicate whether an edge exists or not, as I understand it."""
351,z0lJ2k0sl1g,"So remember dot products are just the sum from i equals 0 to r minus 1 of a1 times ki.
I want to do all of that modulo m.
We'll worry about how long this takes to compute in a moment I guess.
Maybe very soon.
But the hash family h is just all of these ha's for all possible choices of a.
a was a key so it comes from the universe u.
And so what that means is to do universal hashing, I want to choose one of these ha's uniformly at random.
How do I do that? I just choose a uniformly at random.
Pretty easy.
It's one random value from one random key.
So that should take constant time and constant space to store one number.
In general we're in a world called the Word RAM model.
This is actually-- I guess m stands for model so I shouldn't write model.
Random access machine which you may have heard.
The word RAM assumes that in general we're manipulating integers.
And the integers fit in a word.
And the computational assumption is that manipulating a constant number of words and doing essentially any operation you want on constant number of words takes constant time.","Dear All,

I have reviewed the CLRS book and listened to the corresponding lecture. However, I believe they omitted a detailed explanation of one aspect: the repeatability of universal hashing. Specifically, once I map a particular key, denoted as k-particular, to a slot, how can I ensure that the same ""a"" chosen randomly for the dot product with k-particular is used when I look it up again in the future? Here, ""a"" refers to a randomly chosen element, such as 569, 680, or 690. Yet, the hash family h comprises all possible ha's, ranging from 570, 690 to 560, for every potential selection of ""a."" In this context, 571, 276 to 860, ""a"" is a key and therefore originates from the universe U.

P.S. The notation ""569, 680 -->, 690"" and similar sequences are meant to represent specific examples of ""a"" values and their corresponding mappings."
352,z0lJ2k0sl1g,"We'll rewrite it.
The probability is over a.
I'm choosing a uniformly at random.
I want another probability that that maps k and k' to the same slot.
So let me just write out the definition.
It's probability over a that the dot product of a and k is the same thing as when I do the dot product with k' mod m.
These two, that sum should come out the same, mod m.
So let me move this part over to this side because in both cases we have the same ai.
So I can group terms and say this is the probability-- probability sum over i equals 0 to r minus 1 of ai times ki minus ki prime equals 0.
Mod m.
OK, no pun intended.
Now we care about this digit d.
d is a place where we know that this is non-zero.
So let me separate out the terms for d and everything but d.
So this is the same as ability of, let's do the d term first, so we have ad times kd minus kd prime.
That's one term.
I'm going to write the summation of i not equal to d of ai ki minus ki prime.
These ones, some of them might be zero.
Some are not.
We're not going to worry about it.
It's enough to just isolate one term that is non-zero.
So this thing we know does not equal zero.
Cool.
Here's where I'm going to use a little bit of number theory.
I haven't yet used that m is prime.
I required m is prime because when you're working modulo m, you have multiplicative inverses.","Those two sums need only be equivalent modulo m, not equal. Thus, I don't understand your demonstration."
353,zUazLXZZA2U,"What is this term? Let's go there.
Yeah.
Sigmoid.
So Sigmoid.
I'm just going to write it a_2 times 1 minus a_2.
Does that make sense? Sigmoid times 1 minus Sigmoid.
What is this term? Uh, oh sorry my bad.
That's not the right one.
This one, this one is that.
This one is Sigmoid.
a_2 is Sigmoid of z_2.
So this result comes from this term.
Was- what about this term? w_3.
Sorry.
w_3.
w_3.
Is it w_3 or no? I heard transpose.
How do we know if it's w_3 or w_3 transpose? So let's look at the shape of this.
What's z_3? One by one.
It's one by one.
It's a scalar.
It's the linear part of the last neuron.
What's the shape of that? This is 2, 1.
We have two neurons in the layer.
w_3.
We said that it was a 1 by 2 matrix, so we have to transpose it.
So the result of that is w_3 transpose.
And how about the last term? Same as here.
One layer before.
Yeah, someone said they won't transpose.
Okay.
Yeah? The numbers are [inaudible] that one.
This one? Yeah.
There is a transpose here.
[inaudible] w_5.
Oh yeah, yeah.
You're correct.
You're correct.
Thank you.
That's what you mean? Yeah.
Yeah.
This one was from the z_3, to w_2.
We didn't end up using that because we will get stuck, so there's no a_2 transpose here.
Thanks.
Any other questions or remarks? So that's cool.
Let's, let's, let's write- let's write down our derivative cleanly on the board.
So we have derivative of our loss function with respect to w_2, which seems to be equal to a_3 minus y, from the first term.
The second term seems to be equal to, uh, w_3 transpose.
Then we have a term which is a_2 times 1 minus a_2.","The use of ""shape"" in this context makes me nervous. Can we always rely on it to yield the correct answer? Are there potential architectures where some of these matrices could be square? If so, the transpose would have the same dimensions as the original. I would prefer to have a thorough understanding of what I'm doing so that I can determine the appropriate action without requiring such a ""cue."""
354,zUazLXZZA2U,"You just will compute the forward propagation, compute the backdrop, look at the direction and go to that direction.
What momentum is going to say is look at the past updates that you did and try to consider these past updates in order to find the right way to go.
So if you look at the past update and you take an average of the past update.
You would take an average of these update going up and the update after it going down.
The average on the vertical side is going to be small, because one went up, one went down.
But on the horizontal axis, both went to the same direction.
So the update will not change too much on the vert- on- on this axis.
So you're most likely to do something like that if you use momentum.
Does it make sense the intuition behind it? So that's the intuition why we want to use momentum.
And for those of you who do physics, sometimes you can think of momentum as friction.
You know like- like if you- if you launch a rocket and you wanna move it quickly around.
It's not gonna move, because the rocket has a certain weight and has a certain momentum.
You cannot change its direction very, very noisily.
[NOISE] So let's see the implementation of- of- of momentum gradient descent.
Oh, and I believe we- we're almost done, right? Yeah.
Okay.
[NOISE] So let's look at the- the implementation quickly.
So gradient descent was w equals w minus Alpha, derivative of the loss with respect to w.
What we are going to do is we're going to use another variable called velocity, which is going to be the average of the previous velocity and the current weight updates.
So we're going to use that, and instead of the updates being the derivative directly, we're going to update the velocity.
So the velocity is going to be a variable that tracks the direction that we should take regarding the current update and also the past updates with a factor Beta that is be- going to be the weights.","The analogy of the rocket was apt; however, the concept of momentum is not at all similar to friction. Friction does not propel you forward; it only ever attempts to decelerate you or prevent you from initiating movement. It consistently resists any motion that is occurring or attempting to occur."
355,zcvsyL7GtH4,"There's one axiom that covers things adequately, and that is that if two sets have the same members, then they are members of the same sets.
So if all the members of x and y are the same, then x and y are members of exactly the same thing, which we could say this way, for every x, y is an x, if and only if z is an x.
So that is one of the basic axioms of Set Theory, maybe the starting one.
Another one is the Power Set axiom, which simply says that every set has a power set.
How would you say that in the language of predicate set theory? Well, you'd say that for every x, there is a p, which is going to be the power set effects, such that for every set s, s is a subset of x, if and only if s is a member of p.","Is the axiom of extensionality correctly stating that ""x and y are members of the same set""? Shouldn't it state ""y and z"" instead? The current phrasing is nonsensical and confusing, especially since the 'X' at the beginning is not the same as the 'X' at the end, as far as I understand."
