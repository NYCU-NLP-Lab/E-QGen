video_id,paragraph,questions
-1BnXEwHUok,"So what we got with the sample was 0.
So what's the obvious thing to do? If you're doing a simulation of an event and the event is pretty rare, you want to try it on a very large number of trials.
So let's go back to our code.
And we'll change it to instead of 1,000, 1,000,000.
You can see up here, by the way, where I set the seed.
And now, let's run it.
We did a lot better.
If we look at here our estimated probability, it's three 0's 128, still not quite the actual probability but darn close.
And maybe if I had done 10 million, it would have been even closer.
So if you're writing a simulation to compute the probability of an event and the event is moderately rare, then you better run a lot of trials before you believe your estimated probability.
In a week or so, we'll actually look at that more mathematically and say, what is a lot, how do we know what is enough.
What are the morals here? Moral one, I've just told you-- takes a lot of trials to get a good estimate of the frequency of a rare event.
Moral two, we should always, if we're getting an estimated probability, know that, and probably say that, and not confuse it with the actual probability.",29:54 How do we know the number of samples required so that the estimated probability is equal to the actual probability ? Thanks.
-1BnXEwHUok,"The doctrine there from Bohr and Heisenberg, two very famous physicists, was one of what they called causal nondeterminism.
And their assertion was that the world at its very most fundamental level behaves in a way that you cannot predict.
It's OK to make a statement that x is highly likely to occur, almost certain to occur, but for no case can you make a statement x will occur.
Nothing has a probability of one.
This was hard for us to imagine today, when we all know quantum mechanics.
But at the turn of the century, this was a shocking statement.
And two other very well-known physicists, Albert Einstein and Schrodinger, basically said, no, this is wrong.
Bohr, Heisenberg, you guys are idiots.
It's just not true.
They probably didn't call them idiots.
And this is most exemplified by Einstein's famous quote that ""God does not play dice,"" which is indicative of the fact that this was actually a discussion that permeated not just the world of physics, but society in general people really turned it into literally a religious issue, as did Einstein.
Well, so now we should ask the question, does it really matter? And to illustrate that, I need two coins.
I forgot to bring any coins with me.
Does anyone got a coin they can lend me? AUDIENCE: I have some coins.
JOHN GUTTAG: All right.
Now, this is where I see how much the students trust me.
Do I get a penny? Do I get a silver dollar? So what do we got here? This is someone who's entrusting me with quarters, not so bad.
So we'll take these quarters, and we'll shake them up, and we'll put them down on the table.
And now, we'll ask a question-- do we have two heads, two tails, or one head and one tail? So who thinks we have two heads? Who thinks we have two tails? Who thinks we have one of each? Well, clearly, everyone except a few people-- for example, the Indians fan, who clearly believe in the counterfactual-- made the most probabilistic decision.","6:12, ""Um, professor I think you forgot one.. Namely, one head and one tail but the other way around."""
-DP1i2ZU9gk,"And the second thing that represents a list is going to be this second part, which is a pointer.
And internally this pointer is going to tell Python where is the memory location in the computer where you can access the element index 1.
So it's just essentially going to be a chain, going from one index to the other.
And at the next memory location you have the value at index 1, and then you have another pointer that takes you to the location in memory where the index 2 is located.
And in index 2 you have the value and then the next pointer, and so on and so on.
So this is how Python internally represents a list.
OK? How you manipulate lists, we've done this a lot, right? You can index into a list, you can add two lists together, you can get the length, you can append to the end of a list, you can sort a list, reverse a list, and so many other things, right? So these are all ways that you can interact with the list object as soon as you've created it.","I would like to get this clarified: At 5:53 , the diagram explains that python lists are represented internally as linked list. In my understanding, this is incorrect because python lists are internally represented as dynamic arrays as explained here https://youtu.be/CHhwJjR0mZA?t=2082
The section on the internal representation of lists in python is inaccurate. They are dynamic arrays, not linked lists, as stated in 5:40. Source: https://docs.python.org/3/faq/design.html#how-are-lists-implemented-in-cpython"
-DP1i2ZU9gk,"And then I've also implemented some other special methods.
How do I add two fractions? How do I subtract two fractions? And how do I convert a fraction to a float? The add and subtract are almost the same, so let's look at the add for the moment.
How do we add two fractions? We're going to take self, which is the instance of an object that I want to do the add operation on, and we're going to take other, which is the other instance of an object that I want to do the operation on, so the addition, and I'm going to figure out the new top.
So the new top of the resulting fraction.
So it's my numerator multiplied by the other denominator plus my denominator multiplied by the other numerator and then divided by the multiplication of the two denominators.
So the top is going to be that, the bottom is going to be that.
Notice that we're using self dot, right? Once again, we're trying to access the data attributes of each different instance, right, of myself and the other object that I'm working with.
So that's why I have to use self dot here.
Once I figure out the top and the bottom of the addition, I'm going to return, and here notice I'm returning a fraction object.
It's not a number, it's not a float, it's not an integer.
It's a new object that is of the exact same type as the class that I'm implementing.
So as it's the same type of object, then on the return value I can do all of the exact same operations that I can do on a regular fraction object.
Sub is going to be the same.
I'm returning a fraction object.
Float is just going to do the division for me, so it's going to take the numerator and then divide it by the denominator, just divide the numbers.","First thank you for this great tutorial! In 36:00, why do we have inverse function without the leading and trailing underscores? What does it mean those underscores make a function ""magical""?
~36:00. c = a + b. How is a and b assigned to ""self"" and ""other"", how do you know which gets assigned to what? Would appreciate an explanation what happens and where a and b values are sent."
-DP1i2ZU9gk,"And this one on the right essentially says, what's the name of the class, dot, dot notation, what's the method you want to call, and then in parentheses you give it all of the variables including self.
OK.
So in this case you're explicitly telling Python that self is C and other is 0.
So this is a little bit easier to understand, like that.
But it's a little cumbersome because you always have to write coordinate dot, coordinate dot, coordinate dot, for every data attribute you might want to access, for every procedural attribute you might want to access.
So by convention, it's a lot easier to do the one on the left.
And as I mentioned, Python implicitly says, if you're doing the one on the left, you can call this method on a particular object and it's going to look up the type of the object and it's going to essentially convert this on the left to the one on the right.
And this is what you've been using so far.
So when you create a list, you say L is equal to 1, 2, and then you say L.append, you know, 3 or whatever.",In 26:00 can we write zero.distance(c) instead of c.distance(zero)??
09mb78oiPkA,"PATRICK WINSTON: Yes? [? SPEAKER 1: Are we, ?] necessarily, have it done with some sort of a [? politidy distance ?] metric? PROF.
PATRICK WINSTON: Oh, here we go.
We're not going to use any [? politidy distance ?] metric.
We're going to use some other metric.
SPEAKER 1: Like alogrithmic, or whatnot? PROF.
PATRICK WINSTON: Well, algorithmic, gees, I don't know.
[LAUGHTER] PROF.
PATRICK WINSTON: Let me give you a hint.
Let me give you a hint.
There are all those articles up there, out there, and out there, just for example.
And here are the Town and Country articles.
They're out there, and out there, for example.
And now our unknown is out there.
Anybody got an idea now? Hey Brett, what do you think? BRETT: So you sort of want the ratio.
Or in this case, you can take the angle-- PROF.
PATRICK WINSTON: Let's be-- ah, there we go, we're getting a little more sophisticated.
The angle between what? BRETT: The angle between the vectors.
PROF.
PATRICK WINSTON: The vectors.
Good.
So we're going to use a different metric.
What we're going to do is, we're going to forget including a distance, and we're going to measure the angle between the vectors.
So the angle between the vectors, well let's actually measure the cosine of the angle between the vectors.
Let's see how we can calculate that.
So we'll take the cosine of the angle between the vectors, we'll call it theta.
That's going to be equal to the sum of the unknown values times the article values.
Those are just the values in various dimensions.
And then we'll divide that by the magnitude of the other vectors.
So we'll divide by the magnitude of u, and we'll divide by the magnitude of the art vector to the article.
So that's just the dot product right? That's a very fast computation.
So with a very fast computation you can see if these things are going to be in the same direction.","At 23:01 the professor says 'so that's just the dot product, right' - but that's to say that cosine similarity = dot product, which is not precise, right? The dot product is the numerator in this case."
09mb78oiPkA,"So we'll call this the feature detector.
And out comes a vector of values.
And that vector of values goes into a comparator of some sort.
And that comparator compares the feature vector with feature vectors coming from a library of possibilities.
And by finding the closest match the comparator determines what some object is.
It does recognition.
So let me demonstrate that with these electrical covers.
Suppose they arrived on an assembly line and some robot wants to sort them.
How would it go about doing that? Well it could easily use the nearest neighbor sorting mechanism.
So how would that work? Well here's how if would work.
You would make some measurements.
And it we'll just make some measurements in two dimensions.",What is comparitor 8:32? Couldn't find on the web. Is this a spelling mistake?
09mb78oiPkA,"What's the variance of that going to be? x over sigma sub x.
Anybody see, instantaneously, what the variance of that's going be? Or do we have to work it out? It's going to be 1, Work out the algebra for me.
It's obvious, it's simple.
Just substitute x prime into this formula for variance, and do the algebraic high school manipulation.
And you'll see that the variance turns out not to be of this new variable, this transformed variable you want.
So that problem, the non uniformity problem, the spread problem, is easy to handle.
What about that other problem? No cake without flour? What if it turns out that the data-- you have two dimensions and the answer, actually, doesn't depend on y at all.
What will happen? Then you're often going to get screwy results, because it'll be measuring a distance that is merely confusing the answer.
So problem number two is the what matters problem.
Write it down, what matters.
Problem number three is, what if the answer doesn't depend on the data at all? Then you've got the trying to build a cake without flour.
Once somebody asked me-- a classmate of mine, who went on to become an important executive in an important credit card company-- asked me if we could use artificial intelligence to determine when somebody was going to go bankrupt? And the answer was, no.
Because the data available was data that was independent of that question.
So he was trying to make a cake without flour, and you can't do that.
So that concludes what I want to say about nearest neighbors.
No I want to talk a little bit about sleep.","At 41:45, the professor indicates that you cannot use AI for predicting bankruptcies in credit card companies. That's like making cake without flour. Wouldn't the credit card company have relevant data to be able to use AI to predict bankruptcies? Why is the answer ""no""?
did someone understand in 40:00 the derivative of x ?"
0CdxkgAjsDA,"Among all those nodes that have a job in delta f, in delta.
This is probably a tricky part.
Then by definition, since delta f u is less than delta f v, right, and I defined v to be the one with the smallest delta that satisfies that.
So all the u's-- so u is a predecessor of v, so u shouldn't be one of those nodes that have a drop in delta.
So I know this is probably a tricky part.
Yeah, I'll stop for questions and make sure we resolve this part before we move on.","the tricky part at 16:55 can be proved by contradiction: Assume we have delta_f'(u) < delta_f(u), then u is in the set of vertices x that satisfy delta_f'(x) < delta_f(x). Meanwhile, we know delta_f'(u)<delta_f'(v) as u is the predecessor of v. However, this contradicts that v is the smallest delta_f'(x) in the set of vertices x. In this case, u is the smallest. The contradiction implies that the assumption (""Assume we have delta_f'(u) < delta_f(u)"") is wrong, which means ""delta_f'(u) >= delta_f(u)""."
0CdxkgAjsDA,"How many people get it? OK.
Only two.
That's not good.
OK.
AUDIENCE: I'm confused about how v can be the one with the smallest delta f if you have a predecessor with a smaller delta f.
PROFESSOR: OK.
So v to be the smallest-- the one with smallest f such that delta f prime is less than delta f.
AUDIENCE: OK.
PROFESSOR: OK, maybe, yeah, that's why I confused you guys.
Yeah.
Sorry about that.
So we have a bunch of nodes who have a drop in delta, and I defined v to be the one among them that has the smallest delta f prime.
Question? AUDIENCE: Sorry, I'm lost at what delta f prime is versus delta.
PROFESSOR: OK.
Delta f of a node is the shortest path from source to that node in G of f, which is the residual graph given a flow.
So f is, well, some flow, and f prime is the flow after we augmenting a certain path.
So f prime is one step after f.
OK.
How many people get that now? Still not everyone.
OK.
Any questions about that? How many people still haven't got that? OK, so some people-- it's like Schrodinger's cat.","On 17:55 he didn't ask the question. If delta_f^'(v) is the smallest one, then v must be the successor of s. How can there be a u between s and v?"
0LixFSa7yts,"And that turns out to be pretty problematic.
So the next couple of slides sort of say a little bit about the why and how this happens.
What's presented here is a kind of only semi-formal wave your hands at the kind of problems that you might expect.
If you really want to sort of get into all the details of this, you should look at the couple of papers that are mentioned in small print at the bottom of the slide.
But at any rate, if you remember that this is our basic recurrent neural network equation, let's consider an easy case.
Suppose we sort of get rid of our non-linearity and just assume that it's an identity function.
OK, so then when we're working out the partials of the hidden state with respect to the previous hidden state, we can work those out in the usual way according to the chain rule.
And then, if sigma is simply the identity function, well then, everything gets really easy for us.
So only-- the sigma just goes away.
And only the first term involves h at time t minus 1.
So the later terms go away.
And so our gradient ends up as Wh.
Well, that's doing it for just one time step.
What happens when you want to work out these partials a number of time steps away? So we want to work it out, the partial of time step i with respect to j.
Well, what we end up with is a product of the partials of successive time steps.
And well, each of those is coming out as Wh and so we end up getting Wh raised to the l-th power.
And while our potential problem is that if Wh is small in some sense, then this term gets exponentially problematic.",at 41:00 why is del J / del h even needed?
0UFwGJe6ubg,"The averages there and the distributions look moderately similar.
If you're coming from a skilled nursing facility, if you are in a skilled nursing facility, you're probably old because younger people don't typically need skilled nursing care.
And I'm not sure why transfers within the facility are significantly younger ages, but that's true from the MIMIC data.
What about age at admission by language? So some people speak English.
Some people speak not available.
Some people speak Spanish, et cetera.
So it turns out the Russians are the oldest.
And that may have to do with immigration patterns, or I don't know exactly why.
But that's what the data show.
If you do it by ethnicity, it turns out that African-Americans, on the whole, are somewhat younger than whites.
And Hispanics are somewhat younger yet.
So that means that those subpopulations apparently need intensive care earlier in life than whites.
So this is a topic that's very hot right now, discussions about how bias might play into health care.","27:44 I don't quite see how can the professor interpret from the plot that ""Afro-Americans are somewhat younger than the whites"". Can someone explain to me how you could draw this ? Thanks :)"
0jljZRnHwOI,"And that forces you to indent everything that's a code block.
So you can easily see sort of where the flow of control is and where decision making points are and things like that.
So in this particular example, we have one if statement here, and it checks if two variables are equal.
And we have an if, elif, else.
And in this example, we're going to enter either this code block or this one or this one, depending on the variables of x and y.
And we're only going into one code block.
And we'll enter the first one that's true.
Notice you can have nested conditionals.
So inside this first if, we have another if here.
And this inner if is only going to be checked when we enter the first-- this outter if.
I do want to make one point, though.
So sometimes, you might forget to do the double equals sign when you are checking for equality, and that's OK.
If you just use one equals sign, Python's going to give you an error.
And it's going to say syntax error, and it's going to highlight this line.
And then you're going to know that there's a mistake there.
And you should be using equality, because it doesn't make sense to be using-- to assign-- to be making an assignment inside the if.",at 25:16 what's the use of the nested if? and when will it be used!
0jljZRnHwOI,"So the for loop says, for some loop variable-- in this case, I named it n.
You can name it whatever you want.
In range 5-- we're going to come back to what range means in a little bit-- print n.
So every time through the loop, you're going to print out what the value of n is.
Range 5 actually creates internally a sequence of numbers starting from 0 and going to that number 5 minus 1.
So the sequence is going to be 0, 1, 2, 3, and 4.
The first time through the loop, you're going to say n is equal to 0.
Or internally, this is what happens.
N gets the value of 0.
You're going to print n.
Then you're going to go back to the top.
N gets the value 1.
Then you're going to go execute whatever is inside.
So you're going to print 1.
Then you're going to increment that to the next value in the sequence.
You're going to print out 2, and so on.","At 34:35, where do you get 5 - 1?
She points at 34:35, at the whiteboard and say ""that number, 5-1"". When it shows the whiteboard, there is not a number or equation (5-1). Where does it come from?"
0jljZRnHwOI,"So those are the sequences of numbers.
So in this first code right here, my sum is going to get the value 0.
And you're going to have a for loop.
We're going to start from 7, because we're giving it two numbers.
And when you give it two numbers, it represents start and stop with step being 1.
So we're starting at 7.
If step is 1, the next value is 8.
What's the value after that? If we're incrementing by 1? 9.
And since we're going until stop minus 1, we're not actually going to pick up on 10.
So this loop variable, i, the very first time through the loop is going to have the value 7.
So my sum is going to be 0 plus 7.
That's everything that's inside the code block.
The next time through the loop, i gets the value 8.
So inside the for loop, my sum gets whatever the previous value was, which was 7, plus 8.
OK.
The next time through the loop, my sum get the value 7 plus 8 plus 9.
Obviously, replacing that with the previous value.
So 15.
Since we're not going through 10, that's where we stop.
And we're going to print out my sum, which is going to be the value of 7 plus 8 plus 9.
Yeah? OK.
Yeah.
AUDIENCE: [INAUDIBLE] PROFESSOR: Do they have to be integers? That's a great question.
We can try that out.
I'm not actually sure right off the top of my head.
So you can go on Spider and say-- let's say in this example here.
So we can say 7.1, 10.3-- yeah.
So they have to be integers.
OK.
So that's that example.
And let's erase that.","38:54 both codes ARE WORNG SHOULD BE for i in range(5, 11, 2 ): print(i) NOT LIKE : x = 0 for i in range(5, 11, 2 ): x += i print(x) I get output : 21 with that code for some reasons"
0rt2CsEQv6U,"Um, and then expected value of [NOISE] this quadratic term.
Um, because this quadratic term here, kind of the inductive case was what we showed was V star for the- for the next time step, right? So it turns out that, um, let's see.
So this is a quadratic function, and this expectation is the expected value of a quadratic function with respect to s drawn from a Gaussian, right? With a certain mean and certain variance.
So it turns out that, um, the expected value of this thing, right? Well, this whole thing that I just circled.","At 1:06:15, shouldn't the ""Big Quadratic Function"" include the second term because a_t is there as well?"
0zuiLBOIcsw,"And I can- I wanna contract them into a super-node, ah, and, uh, create a new network, a next level network, where super-nodes are connected if there is at least one edge between the nodes, ah, of the corresponding communities, um, and the, ah, weights, ah, of the edges between two super-nodes is the sum of the edge weights across all edges between their corresponding communities.
And now I will have a super graph.
And I simply go and run, ah, phase 1, ah, again.
All right, so the idea is, I have my original network, I run phase 1 to identify clusters.
Now I contract each cluster into a super-node.
I connect two clusters, if there is at least one edge, ah, between them.
And now this will be a weighted network where the, the edge weights are denoted here, so this will be the total number of edges between C_1 and C_2.
And this would be the total number of edges, ah, between the members of, ah, C_2 ah, and so on.
And now that I have the super graph, I simply apply my, ah, phase 1 algorithm again.
So the way this will work is, ah, you know, to summarize, I have my original network, I pick a node and, ah, initially I put every node into its own community.
Um, and then I ask, ah, a node, what if I move you to the same community as your member node 2 is up.","On the slide at 12:25 it shows Delta M_0,4 = 0.26. Since nodes 0 and 4 are not connected, I don't think we would compute this change in modularity, correct? We would only compute it for neighbors of node 0?"
0zuiLBOIcsw,"Uh, and now after I have moved, uh, in to the community, see this node i, what do I get is the, uh, sum of- number of edges inside Sigma in increases by k_i, in, and the, uh, total number of edges, now increases bu- or total sum of the degrees now increases for the degree of node i.
So this is how I can write it, ah, write it all, ah, all, all, ah, all together now.
And then, right here is we said Delta modularity after I move i into community C is, you know, after minus before.
Ah, this was- this is after, right? I have the sum of the, uh, the in deg- the degrees inside the community is increased by this theorem, the total number of degrees is increased by the degree of node i, so this is the after modularity.","When you switch slide at around 9:15, there should be a factor 2 in front ok k_(i,in). Because its edges will now be counted twice : once for i, once for the its neighbours in C."
14UlXIZzwE4,"So in this particular example, assuming that things ended with 10 people in line, 0 through 9, you could get away with three commands.
You could say, 1 through 2-- and the implication, as I said, is this is inclusive-- flip.
And then you could say, 4, flip.
And then you could say, 7 through 9, flip.
And if you did it the other way, you could also say, person at 0, flip, and so this would go backwards.
Person at 3, flip.
Obviously, you can't say, 0 through 3 because that would be wrong.
And then you could say 5 through 6 here.","10:50 the lecturer forgot to count F under 13, there would be 4 moves if you flip all F->B His algorithm course was really solid, this lecture is filled with lots of ""um""s"
1A6VoEkQnhQ,"And, uh, the reason why a, uh, uh, plain GNN cannot differentiate between, you know, node 1 in, uh, a cycle of length 3 versus a cycle of leng- length 4 is that if you look at the GNN computation graph, re- both- for both of these nodes V_1 and V_2, the computation graph is exactly the same.
Meaning, V_1- V_1 and V_2 have two neighbors, um, each, and then, you know, these neighbors have, uh, one neighbor each and the computation graph will always look like this.
Uh, unless, right, you have some way to discriminate nodes based on the features.
But if the nodes have the same, um, set of, uh, features that not- you cannot discriminate them based on their attributes, then you cannot learn, uh, to discriminate node V_1 from V_2.
They will- from the GNN point of view, they will all, uh, look the same.
I'm going to go into more, uh, depth, uh, uh, around this, uh, this example and, er, what are some very important implications of it and consequences of it when we are going to discuss the theory of, uh, graph neural networks.",The computational graph are structurally same for the node v_1 at 14:00 but they'll be fed different embeddings. The embeddings will implicitly include information about the nodes. A GNN can still struggle to distinguish cycles of varying types and shapes?
247Mkqj_wRM,"So I take the coefficients e that we have just defined, I, uh, exponentiate them, and then, you know, divide by the s- exponentiated sum of them so that, uh, these, uh, attention weights, uh, alpha now are going to sum to 1.
And then, right when I'm doing message aggregation, I can now do a weighted sum based on the attention weights, uh, alpha.
So here are the alphas.
These are these alphas that depend on e, and e is the, uh, is the, um, is- depends on the previous layer embeddings of nodes, uh, u and v.
So, for example, if I now say, how would aggregation for node A look like? The way I would do this is I would compute these attention weights, uh- uh, Alpha_AB, Alpha_AC, and Alpha_AD because B, C, and D are its neighbors.
Uh, these alphas will be computed as I- as I show up here, and they will be computed by previous layer embeddings, uh, of these, uh, nodes on the endpoints of the edge.
And then my aggregation function is simply a weighted average of the messages coming from the neighbors, where message is, uh- uh- uh, multiplied by the weight Alpha that we have, uh, computed and defined up here.
So that's, um, [NOISE] basically the idea of the attention mechanism.
Um, now, what is the form of this attention mechanism a? We still haven't decided how embedding of one node and embedding of the other node get- get combined, computed into this, uh- uh, weight, uh, e.
Uh, the way it is usually done is, uh- um, you- you have many different choices.
Like you could use a simple, uh, linear layer, uh, one layer neural network to do this, um, or, uh, have alpha, uh, this, um, function a have trainable parameters.
Uh, so for example a p- uh, a popular choice is to simply to say: let me take the embeddings of nodes A and B at the previous layer, perhaps let me transform them, let me concatenate them, and then apply a linear layer to them, and that will give me this weight, uh, e_AB, to which then I can apply softmax, um, and then based on that, ah, softmax transformed weight, I use that weight as, uh- uh, in the aggregation function.","Two questions that I had after watching this lecture: Is there a qeuery key value interpretation of the simple ""linear attention"" mechanism in graph attention networks? At 22:07 when describing how to normalize the unnormalized attention weights a_{vu} in the graph attention network scheme why do we not just divide by the total sum of all the attention weights? Why do we use a softmax to suppresses the non-maximal weights to mostly focus in on a single neighbor only?"
247Mkqj_wRM,"So, last lecture, we talked about graph convolutional neural network or a GCN.
And I've wrote this equation, I said, ah-ha, the embedding of node v at layer l is simply an average of the embeddings of nodes u that are neighbors of we normalized the by the- by the N-degree of node v and transformed with matrix W and sent through a non-linearity.
So now the question is, how can I take this equation that I've written here and write it in this message transformation plus aggregation function.
And the way you can- you can do this is simply take this W and distribute it inside.
So basically now W times h divided by number of neighbors is the message transformation function.
And then the message aggregation function is simply a summation.
And then we have a non-linearity here.","This might be nitpicking, but I would still like to ask for conceptual clarity: Doesn't it make more sense to ascribe (at 10:20, when trying to understand the classical GCN layer as a message transformation + aggregation scheme) the normalizing factor 1/|N(v)| to the aggregation phase since it depends on the node towards which the messages are being passed and not just the message itself? In other words, pull it out of the sum using the distributive law? If one does that the message transformation step will only depend on the message and not on the node it is being delivered to. That seems conceptually cleaner to me for something to be thought of as ""message transformation""."
247Mkqj_wRM,"Then we have to aggregate these messages into a single message and pass it on.
So the way you can think of this is that we get messages, denoted as circles here, from the three neighbors, from the previous layer.
We also have our own message, right? Message of the node v from the previous layer.
Somehow we want to combine this information to create the next level embedding or to the next level message for this node of interest.
What is important here to note is that this is a set.
So the ordering in which we are aggregating these messages from the children is not important.
What is arbitrary? And for this reason, these aggregation functions that aggregate, that summarize, that compress in some sense, the messages coming from the children have to be order invariant because they shouldn't depend, in which ordering, am I considering the neighbors? Because there is no special ordering to the neighbors, to the lower level, to the children in the network.
That's an important detail.
Of course, another important detail is that we want to combine information coming from the neighbor- from the neighbors together with a node's own information from the previous layer as denoted here.","When he said we have the value of (1:30) the self node from previous later, is he referring to the value of V as in last itteration because here layer is used for actually the layer before it where v doesn't actually exist. Its drilling in to the words too much but just wanna know for absolute certainty. Or it is the self loop that is being used as a previous layer here"
2P-yW7LQr08,"So the last thing I'll do-- and I just have one more minute-- is give you a sense of a small change to interval scheduling that puts us in that NP complete domain.
So so far, we've just done two problems.
There's many others.
We did interval scheduling.
There was greedy linear time.
Weighted interval scheduling is order n squared according to this particular DP formulation.
It turns out there's a smarter DP formulation that runs an order n log n time that you'll hear about in section on Friday, but it's still polynomial time.
Let's make one reasonable change to this, which is to say that we may have multiple resources, and they may be non identical.","I don't get 1:19:41. If recursive calls are O(1) then shouldn't the complexity be n? Why is it n^2, can somebody explain?"
2g9OSRKJuzM,"Now, this looks kind of random.
Anybody recognize these numbers? No one from the great City of New York? No? Yup, yup.
AUDIENCE: On the subway stops? SRINIVAS DEVADAS: Yeah, subway stops on the Seventh Avenue Express Line.
So this is exactly the notion of a skip list, the fact that you have-- could you stand up? Great.
All right.
So the notion here is that you don't have to make a lot of stops if you know you have to go far.
So if you want to go from 14th Street to 72nd Street, you just take the express line.
But if you want to go to 66th Street, what would you do? AUDIENCE: Go to 72nd and then go back.
SRINIVAS DEVADAS: Well, that's one way.
That's one way.
That's not the way I wanted.
The way we're going to do this is we're not going to overshoot.
So we want to minimize distance, let's say.
So our secondary thing is going to be minimizing distance travel.
And so you're going to pop up the express line, go all the way to 42nd Street, and you're going to say if I go to the next stop on the Express Line, I'm going too far.
And so you're going to pop down to the local line.
So you can think of this as being link list L0 and link list L1.","12:00 I don't understand why he avoids overshooting here. Since traveling on L1 is faster, going to 72 and then back to 66 should have the minimum amount of nodes, right ?"
3pU-Hrz_xy4,"So this is actually equal to 2 in this case.
And this corresponds to this value in the table which is again the agent is following a maximizer assuming the opponent is a minimizer.
Opponent was not a minimizer, opponent was just following Pi 7.
And this is just equal to 2 .
Okay.
So so far, the things I've shown are actually very intuitive.
They seem a little complicated but they're very intuitive.
What I've shown is that this value of minimax, it's an upper bound.
If you're assuming our, our opponent is a terrible opponent, now it's going to be an upper bound because the best thing I can do is maximize.
I've also shown it's a lower bound if my opponent is not as bad.
So, so that's what I've shown so far.
A question.
So here the opponent's policy is completely hidden to the agent.
Yeah.
So here, like, because- Yeah, the agent actually doesn't see the opponent- where the opponent goes, right? Even in the expectimax case, it thinks the opponent is going to follow Pi 7, but maybe the opponent follows Pi 7, maybe not.
Right so, so like when we talk about expectimax and minimax, it's always the case that the opponent doesn't actually see what the opponent does.
But the opponent can think- the agent can think what the opponent does, okay? And I'm going to talk about one more property.","47:09 Why is V(pimax,pi7)=2 and not 5, assuming agent will try to maximize his value while the opponent will act stochastically (ie. 0,2,5 as distributions)"
3v5Von-oNUg,"I think that's right.
But now we have our original aR bR plus 2q plus 8q is equal to this thing.
And finally, we can divide this thing by R very cheaply.
Because we just discard the low four zeros.
Make sense? Question.
AUDIENCE: Is aR bR always going to end in, I guess, 1,024 zeros? PROFESSOR: No, and the reason is that-- OK, here is the thing that's maybe confusing.
A was, let's say, 512 bits.
Then you multiply it by R.
So here, you're right.
This value is that 1,000 bit number where the high bit is a, the high 512 bits are a.
And the low bits are all zeros.
But then, you're going [? to do it with ?] mod q to bring it down to make it smaller.","45:08 omfg and I was so confused whole time lmaoo wonderful lecture tho, I was able to follow easily and deduce things from his prompts. Nonetheless, I think many properties he mentioned, like of inverses at 48:10 applies only to finite fields with order prime right? I think it should've just been mentioned once, in case I am not mistaken, will probably give it a look"
3v5Von-oNUg,"So instead of doing 500 mod qs for every multiplication step, you do it twice mod q.
And then you keep doing these divisions by R cheaply using this trick.
Question.
AUDIENCE: So when you're adding the multiples of q and then dividing by R, [INAUDIBLE] PROFESSOR: Because it's actually mod q means the remainder when you divide by q.
So x plus y times q, mod q is just x.
AUDIENCE: [INAUDIBLE] PROFESSOR: So in this case, dividing by-- so another sort of nice property is that because it's all modulus at prime number-- it's also true that if you have x plus yq divided by R, mod q is actually the same as x divided by R mod q.
The way to think of it is that there's no real division in modular arithmetic.
It's just an inverse.
So what this really says is this is actually x plus yq times some number called R inverse.
And then you compute this whole thing mod q.
And then you could think of this as x times R inverse mod q plus y [? u ?] R inverse mod q.
And this thing cancels out because it's something times q.
And there's some closed form for this thing.
So here I did it by bit by bit, 2q then 8q, et cetera.
It's actually a nice closed formula you can compute-- it's in the lecture notes, but it's probably not worth spending time on the board here-- for how do you figure out what multiple of q should you add to get all the low bits to turn to 0.","45:08 omfg and I was so confused whole time lmaoo wonderful lecture tho, I was able to follow easily and deduce things from his prompts. Nonetheless, I think many properties he mentioned, like of inverses at 48:10 applies only to finite fields with order prime right? I think it should've just been mentioned once, in case I am not mistaken, will probably give it a look"
4b4MUYve_U8,"Okay? Um, so in order to- [NOISE] so in order to simplify the notation, [NOISE] um, [NOISE] in order to make that notation a little bit more compact, um, I'm also gonna introduce this other notation where, um, we want to write a hypothesis, as sum from J equals 0-2 of theta JXJ, so this is the summation, where for conciseness we define X0 to be equal to 1, okay? See we define- if we define X0 to be a dummy feature that always takes on the value of 1, then you can write the hypothesis h of x this way, sum from J equals 0-2, or just theta JXJ, okay? It's the same with that equation that you saw to the upper right.
And so here theta becomes a three-dimensional parameter, theta 0, theta 1, theta 2.
This index starting from 0, and the features become a three dimensional feature vector X0, X1, X2, where X0 is always 1, X1 is the size of the house, and X2 is the number of bedrooms of the house, okay? So, um, to introduce a bit more terminology.
Theta [NOISE] is called the parameters, um, of the learning algorithm, and the job [NOISE] of the learning algorithm is to choose parameters theta, that allows you to make good predictions about your prices of houses, right? Um, and just to lay out some more notation that we're gonna use throughout this quarter.","May I ask, down to 7:50 what does O (teta) represent?"
4b4MUYve_U8,"That's the word bedrooms, [NOISE] right? Then, I guess- [NOISE] All right.
Yeah.
Cool.
My- my- my father-in-law lives a little bit outside Portland, uh, and he's actually really into real estate.
So this is actually a real data-set from Portland.
[LAUGHTER] Um, so more generally, uh, if you know the size, as well as the number of bedrooms in these houses, then you may have two input features [NOISE] where X1 is the size, and X2 is the number of bedrooms.
[NOISE] Um, I'm using the pound sign bedrooms to denote number of bedrooms, and you might say that you estimate the size of a house as, um, h of x equals, theta 0 plus theta 1, [NOISE] X1, plus theta 2, X2, where X1 is the size of the house, and X2 is- [NOISE] is the number of bedrooms.",It should be 1/2m where m is the size of the data set. That's because we'd like to take the average sum of squared differences and not have the cost function depend on the size of the data set m https://youtu.be/ZzeDtSmrRoU He explains it here at 6:30 minutes
4b4MUYve_U8,"Uh, that's times the partial derivative of Theta J of X Theta X minus Y, right? So if you take the derivative of a square, the two comes down and then you take the derivative of what's inside and multiply that, right? [NOISE] Um, and so the two and one-half cancel out.
So this leaves you with H minus Y times partial derivative respect to Theta J of Theta 0X0 plus Theta 1X1 plus th- th- that plus Theta NXN minus Y, right? Where I just took the definition of H of X and expanded it out to that, um, sum, right? Because, uh, H of X is just equal to that.","28:51, what is x0 and x1? If we have a single feature, say # of bedrooms, how can we have x0 and x1? Wouldn't x0 be just nothing? I'm confused. Or, in other words, if my Theta0 update function relies on x0 for the update, but x0 doesn't exist, theta0 will always be the initial theta0..."
4b4MUYve_U8,"Um, and then finally, by convention, we put a one-half there- put a one-half constant there because, uh, when we take derivatives to minimize this later, putting a one-half there would make some of the math a little bit simpler.
So, you know, changing one- adding a one-half.
Minimizing that formula should give you the same as minimizing one-half of that but we often put a one-half there so to make the math a little bit simpler later, okay? And so in linear regression, we're gonna define the cost function J of Theta to be equal to that.","Dear Dr. Andrew I saw yours other video with the cost function with linear regression by 1/2m but this video 1/2, so what is different between it?(footnote 16:00)"
4dj1ogUwTEM,"Anyway, just as there's a decimal expansion of every real number, there's a binary expansion just using base 2.
So here's the binary expansion of 3 and 1/3.
So what I'm going to do is I'm going to map 3 and 1/3 to this binary sequence.
I'm going to ignore the decimal place.
Binary is not decimal place.
It's a [? becimal ?] place, or binary position.
And I'm just going to take this to mapping the sequence, 11010101.
And I claim that this is a surjection because you're going to hit every possible binary sequence in this way.
Well, almost.
Let's take a closer look.
There's a problem with mapping to things that start with 0, because let's examine that a half is 0.10000000.
So I would map it to that.
And it will end.
But there's an ambiguity, because a half is also equal to 0.011111, just as 0.999999 is equal to 1.000000 in decimal, you get the same infinite carry issue here in binary.
So numbers that end in all ones have another way to represent the very same number by a sequence that ends in all zeroes.",18:50 How do we know we will hit every possible inifinite string?
4dj1ogUwTEM,"But we'll think about it as though we could.
Let's think about this matrix again.
So suppose A is this set of elements, a, b, s, t, d, e.
I'm scrambling up the alphabet on purpose, because I don't want you to get the idea that we're assuming that A is countable, that you can list all the elements of A.
I'm not assuming that.
But I'm just writing out a sample of elements of A.
And let's suppose that I was trying to get a surjection from A to the power set of A.
So suppose I have a function f that maps each of the successive elements of A to some subset of A.","9:44 Wth, why is f(a) supposed to have anything to do with subsets??? How is running a function on every element of a set supposed to make it its powerset???"
4nXw-f6NJ9s,"And you can even download the level-- an example of the level and play it, if you dare.
So that's a lot of-- we have a lot of fun in that world of hardness of different games and puzzles.
Where do I want to go next? OK.
Next topic is balloon twisting.
Totally different.
This is recreational, but not about hardness.
This is an octahedron twisted from one balloon.
I made another one on a stick.
Each of these is made for one balloon.
What graphs can you make for one balloon? Well, you should read our paper.
And you can characterize how many balloons you need to make each polyhedron.
And some of these problems are NP-hard, and it's a lot of fun.
Cool.
I think that's the end of the slides.
The last thing I wanted to show you is a problem, a puzzle/magic trick-- it comes from the puzzle world-- called the picture hanging problem.
So imagine you have a picture.
You want to hang it on a wall.
So you invested in some nice rope, and you hang it on a nail.","30:15 Is that basically an AND gate, using rope? The picture is hanging when both values are used (TRUE), else the picture drops (FALSE)"
51-b2mgZVNY,"Again, 8 factorial.
OK.
Finally, how many permutations are there that have all three patterns-- 60, 04, and 42? That of course, is exactly the same as the set of sequences with the single pattern 6042, the four digit pattern.
And again, we count that by saying that it's the number of permutations of the digits other than 6042-- six of them plus the 6042 object.
There are seven of these , and so there are 7 factorial permutations that have all three patterns.
So that means that I can go back to my inclusion-exclusion formula for the sequences that have one of the three patterns-- 60, 04, and 42-- and plug them in.
So I get 3 9 factorials for the first sum of three terms.
The intersections-- we all figured out each of them were-- I'm sorry it's 8 factorial.
So I'm going to subtract 3 times 8 factorial.
And this last term we figured out was 7 factorial.
Well, I can think of 3 times 9 factorial as 9 times 8 times 3 times 7 factorial, and this is 3 times 8 times 7 factorial.
And I wind up [NO AUDIO] PROFESSOR: 72,720.
That's how many permutations of the digits 0 through 9 there are that have one or another of these three patterns.
Turns out that's about 27% of the 10 factorial permutations of 0 through 9.
So that's the significance of applying the disjunction of constraints, this union of having either 60, 04, or 42.
",Which method is he using to calculate the result at 12:16?
51-b2mgZVNY,"So it's also odd and occurs positively.
All right.
Well, now we can apply the formula and say that the set of permutations that have a 60, a 04, and a 42 is equal to the sum of the number that have a 60, the number that have a 04, and the number that have a 42 minus the numbers that have two of the patterns minus those that have all three patterns.
At let's count these individual intersections and sets of permutations separately.
It turns out that each one is easy to count, which is a typical situation which is why inclusion-exclusion is a valuable principle because this thing that is harder to count can be broken up into counting a bunch of other things-- intersections-- that are often easier to count.",At 8:18 you said minus the intersection of all three intending to say that you add the intersection of P60 intersected with P04 intersected with P42.
5Bx5UhrJbJI,"This slide has a bunch of other annotations on it.
And the reason I included them is that the course repository includes a reference implementation of an autoencoder and all the other deep learning models that we cover in pure NumPy.
And so if you want it to understand all of the technical details of how the model is constructed and optimized, you could use this as a kind of cheat sheet to understand how the code works.
I think the fundamental idea that you want to have is simply that the model is trying to reconstruct its inputs.
The error signal that we get is the difference between the reconstructed and actual input.
And that error signal is what we use to update the parameters of the model.
Final thing I would mention here is that it could be very difficult for this model if you feed in the raw current vectors down here.
They have very high dimensionality.
And their distribution is highly skewed as we've seen.
So it can be very productive to do a little bit of reweighting and maybe even dimensionality reduction with LSA before you start feeding inputs into this model.","11:00 everything is confusing from there on , at least explain the labels a bit"
5cF5Bgv59Sc,"So that's the triangle inequality.
Pretty intuitive notion, right? Why is this useful? OK, well, if I find-- if I find an edge in my graph, if there's an edge u, v, in my graph such that this condition is violated for the estimates that I have-- it obviously can't be violated on my shortest path distances, but if it violates it on the estimates-- u, v, is bigger than u, x-- sorry, u-- how am I going to do this? I want this to be s.
I'm calculating shortest path distances from s and shortest path distances from s to some incoming vertex u plus the edge weight from u to v.
All right, so what is this doing? I have some edge u, v in my graph.
Basically, what I've said is that I have some distance estimate to u, but going through-- making a path to v by going through u, and then the edge from u to v is better than my current estimate, my shortest path estimate to v.
That's bad, right? That's violating the triangle inequality.
These cannot be the right weights.
These cannot be the right distances.
So how we're going to do that is lower-- this is what we said, repeatedly lower these distance estimates.
I'm going to lower this guy down to equal this thing.
In a sense, this constraint was violated.
But now we're relaxing that constraint so that this is no longer violated.","42:30 shouldnt it be d(s, v) > delta(s, u) + w(u,v)? bcoz d(s,u) is also an upper bound estimate, i.e, infinity"
5rlIYGJdPy4,"Exactly what we did on the example on the previous slide.
So let's revisit the Australia example and apply AC-3.
OK.
So here is the empty assignment.
And here are all the domains of each of the variables.
So let's suppose we set WA to be red, OK? So as before, we eliminate the other values from WA's domain, of course.
And then we enforce arc consistency on the neighbors of WA.
In this case, NT and SA.
So out goes red on both of these.
And now we continue to try to enforce our consistency on the neighbors of NT and SA.
But in this case, I can't actually eliminate anything.",6:04 why not? Why wasn't that possible
5wCZqdCDafc,"ROFESSOR: So connectivity is more than just an all or nothing affair.
We can talk about how connected a graph is.
So let's begin with two vertices.
Two vertices are said to be k-edge connected if they remain connected if you remove fewer than k edges from the graph.
Let's look at an example.
So here's a graph, and let's focus on those two vertices that I've highlighted in magenta.
They are 1-edge connected because they're connected, and if you remove one edge they become disconnected.
So they're 1-edge connected, but they're not 2-edge connected.
In particular, if I delete that edge, then they no longer is a path between the two magenta vertices.","Why do I think that the definition at the start (0:13) is slightly wrong. Shouldn't it be ""Vertices v and w are k-edge connected if they GET DISCONNECTED whenever fewer than k edges are deleted"" ? Please correct me :)"
6LOwPhPDwVc,"And then let's look at merge sort and do one more visualization of this.
Again let me remove that.
If we run it-- again, I've just put some print statements in there.
Here you can see a nice behavior.
I start off calling Merge Sort with that, which splits down into doing Merge Sort of this portion.
Eventually it's going to come back down there and do the second one.
It keeps doing it until it gets down to simple lists that it knows are sorted.
And then it merges it.
Does the smaller pieces and then merges it.
And having now 2 merged things, it can do the next level of merge.
So you can see that it gets this nice reduction of problems until it gets down to the smallest size.
So let's just look at one more visualization of that and then get the complexity.
So if I start out with this list-- sorry about that.
What I need to do is split it.
Take the first one, split it.
Keep doing that until I get down to a base case where I know what those are and I simply merge them.
Pass it back up.
Take the second piece.
Split it until I get down to base cases.
Do the merge, which is nice and linear.
Pass that back up.
Having done those two pieces, I do one more merge.
And I do the same thing.
I want you to see this, because again you can notice how many levels in this tree log.
Log in the size.
Because at each stage here, I went from a problem of 8 to two problems of 4.
Each of those went to two problems of 2, and each of those went to two problems of size 1.
All right.
So the last piece is, what's the complexity? Here's a simple way to think about it.
At the top level, I start off with n elements.
I've got two sorted lists of size n over 2.
And to merge them together, I need to do order n work.","42:12 merge isnt logn iterations its at least n iterations and at most 2n iterations. You can see at 39:49 that number of prints(iterations) isnt logn. 37:32 we arent cutting down the problem in half, its a tree 40:33. Its logn levels of iterations(not iterations themselves) where iterations at each level together have O(n) complexity(cost of each step(iteration) is O(n) but turns out that cost of steps at same level of a tree is also O(n) 41:24. He just mixes levels with steps 42:04 Normally what we do is we multiply number of steps(O(n->2n)) to their complexity(O(n)) but in this case we use the fact that at each LEVEL of a tree sum of conplexity of steps is O(n). So number of LEVELS in a tree (O(logn)) multipled with each levels complexity(O(n)) = O(nlogn)"
6LOwPhPDwVc,"Because as I said I got to do at least n comparisons where n is the length of the list.
And then I've got to do n plus n copies, which is just order n.
So I'm doing order n work.
At the second level, it gets a little more complicated.
Now I've got problems of size n over 4.
But how many of them do I have? 4.
Oh, that's nice.
Because what do I know about this? I know that I have to copy each element at least once.
So not at least once.
I will copy each element exactly once.
And I'll do comparisons that are equal to the length of the longer list.
So I've got four sublists of length n over 4 that says n elements.
That's nice.
Order n.
At each step, the subproblems get smaller but I have more of them.
But the total size of the problem is n.
So the cost at each step is order n.
How many times do I do it? Log n.
So this is log n iterations with order n work at each step.
And this is a wonderful example of a log linear algorithm.","42:12 merge isnt logn iterations its at least n iterations and at most 2n iterations. You can see at 39:49 that number of prints(iterations) isnt logn. 37:32 we arent cutting down the problem in half, its a tree 40:33. Its logn levels of iterations(not iterations themselves) where iterations at each level together have O(n) complexity(cost of each step(iteration) is O(n) but turns out that cost of steps at same level of a tree is also O(n) 41:24. He just mixes levels with steps 42:04 Normally what we do is we multiply number of steps(O(n->2n)) to their complexity(O(n)) but in this case we use the fact that at each LEVEL of a tree sum of conplexity of steps is O(n). So number of LEVELS in a tree (O(logn)) multipled with each levels complexity(O(n)) = O(nlogn)"
6LOwPhPDwVc,"Break the problem in half.
Keep doing it until I get sorted lists.
And then grow them back up.
So there's merge sort.
It says, if the list is either empty or of length 1, just return a copy of the list.
It's sorted.
Otherwise find the middle point-- there's that integer division-- and split.
Split the list everything up to the middle point and do merge sort on that.
Split everything in the list from the middle point on.
Do merge sort on that.
And when I get back those two sorted lists, just merge them.
Again, I hope you can see what the order of growth should be here.
Cutting the problem down in half at each step.","42:12 merge isnt logn iterations its at least n iterations and at most 2n iterations. You can see at 39:49 that number of prints(iterations) isnt logn. 37:32 we arent cutting down the problem in half, its a tree 40:33. Its logn levels of iterations(not iterations themselves) where iterations at each level together have O(n) complexity(cost of each step(iteration) is O(n) but turns out that cost of steps at same level of a tree is also O(n) 41:24. He just mixes levels with steps 42:04 Normally what we do is we multiply number of steps(O(n->2n)) to their complexity(O(n)) but in this case we use the fact that at each LEVEL of a tree sum of conplexity of steps is O(n). So number of LEVELS in a tree (O(logn)) multipled with each levels complexity(O(n)) = O(nlogn)"
6LOwPhPDwVc,"Now you've already seen some of this, right? We did search where we said, we can do linear search.
Brute force.
Just walk down the list looking at everything till we either find the thing we're looking for or we get to the end of the list.
Sometimes also called British Museum algorithm or exhaustive enumeration.
I go through everything in the list.
Nice news is, the list doesn't have to be sorted.
It could be just in arbitrary order.
What we saw is that the expected-- sorry, not expected.
The worst case behavior is linear.
In the worst case, the element's not in the list.
I got to look at everything.",2:53 linear search (mentioned) is not the same as sequential search (what is meant here)
6stKGH6zI8g,"This doesn't seem right? Uh, [LAUGHTER] uh, [NOISE] and that is- that's a good intuition to have.
[NOISE] But, uh, in this case, we- now- are now moving from meta-training- from training set to the meta-train sets, and test sets to meta test sets.
So each of these tasks, the training set and the test sets for the task correspond to the meta-training dataset.
And then at meta-test time were given new tasks.
Uh, and we don't wanna train on the meta-test set.
Okay.
Um, so we have our meta-learning data, this corresponds to training and test sets for every task, uh, where each of the training set- datasets corresponds to K data points.
Each of the test data- datasets correspond to a new set of K data points.
Um, yeah.
Okay.
So the complete, kind of, optimization problem is that at test-time we're gonna be inferring a set of task specific parameters.
Which can be some, some function that takes as input the training dataset and outputs the task specific parameters.
Where the parameters of that function or the meta parameters are theta star.
Um, and we essentially wanna learn a set of meta parameters such that, this function is good for held-out data points, after being- after ge- getting the training dataset as input.
Okay.
Um, so essentially you can view theta star as optimizing this, uh, objective [NOISE] where we want to optimi- optimize the, the probability of the parameters, um, being effective at new data points.","Meta learning is not complicated in practice, but the notion of the datasets in the literature is confusing, like 1:12:30. Neither support and query sets, nor meta training and test sets are clear. The first is not commonly used in regular neural networks. The second is confusing as it is long and might be confused with the regular model dataset (optimizee). Perhaps keeping training and test sets for the optimizee dataset, while using training and test tasks for the meta."
6stKGH6zI8g,"[inaudible] Yeah exactly.
So basically the weights that are, uh, right after the Z_i, the kind of, if you have a fully connected layer right after Z_i concatenated with your features, the, the part of that matrix that corresponds to, uh, that is basically right after Z_i will have basically different, um, different components that, that are not shared for each of the tasks.
Other than that half of that matrix, all the other parameters are shared.
Yeah.
There's also kind of forces [inaudible] Yeah.
Yeah, exactly.
So in this case, we assume that all the inputs have the same size, the same, uh, dimensions.
One thing that you could do is, uh, if, if different tasks have different sizes, you can basically like, you would have some sort of, uh, recurrent neural network, or some sort of attention based model that basically aggregates over the variable dimens- like if you- if one of them is time for example, it aggregates over that, uh, whereas maybe some tasks have- are text and others image.
Images in that case, you would probably wanna have different, um, different first parts of that network to take in those different modalities of data.
And we'll show- we'll see like an explicit example of, of how that has been done in the past.
Yeah.
So [inaudible] So this is, yeah, this is a good question.
It's, it's a f- it's a fairly nuanced point.
So basically each, um, you can ba- you can view, uh, the first- a fully connected layer corresponding to, um, a weight matrix times a vector.
And when you, um, when you take the, the top part of that matrix will correspond to- or sorry the left, um, rows of that- the left columns of that matrix will correspond to the, the features and the right columns of that will correspond to the task, uh, task vector that's being processed in this, in this input.","question of 17:15, the prof mentioned ""the parameter are shared except"". After the concatenation, is that all the full connected layer afterwards would be shared across different tasks?"
7lQXYl_L28w,"And I put it into new.
That's simply taking all of the solutions of subsets of up to n minus 1 and creating a new set of subsets where n is included in every one of them.
And now I take this, and I take that.
I append them-- or concatenate them, rather.
I should say ""append them""-- concatenate them together and return them.
That's a crisp piece of code.
And I'm sorry, John, I have no idea why I put res up there.
I don't think I need that anywhere in this code.
And I won't blame it on John.
It was my recopying of the code.
AUDIENCE: [INAUDIBLE] .
ERIC GRIMSON: Sorry? AUDIENCE: Maybe.
ERIC GRIMSON: Maybe, right.
Look, I know I'm flaming at you.
I get to do it.
I'm tenured, as I've said multiple times in this course.
That's a cool piece of code.
Imagine trying to write it with a bunch of loops iterating over indices.
Good luck.
You can do it.
Maybe it'll be on the quiz.
Actually, no, it won't.
That's way too hard to ask.","35:52 , 37:29 It was a mistake."
7lQXYl_L28w,"And inside of the loop, this is just constant.
It doesn't depend on the size of the integer.
So how many times do I go through the loop? Well, how many times can I divide i by 10? And that's log of i, right? So it's not i itself.
It's not the size of the integer.
It's the number of digits in the integer.
And here's another nice example of log.
I'll point you, again, right here.
I'm reducing the size of the problem by a constant factor-- in this case, by 10-- each time-- nice characteristic of a logarithmic algorithm.
OK, we've got constant.
We've got log.
What about linear? We saw it last time, right? Something like searching a list in sequence was an example of something that was linear.
In fact, most of the examples we saw last time were things with iterative loops.
So for example, fact, written intuitively-- factorial, right-- n times n minus 1 times n minus 2 all the way down to 1.","23:09 Aren't string immutables? So this means it will reassign the string res = digits[i%10] + res (by creating a new string). Wouldnt that mean creating a new string log(n) times.Hence shouldn't the complexity be (1 + 2 + 3 ....... log(n)) = (1+log(n))*log(n)/2 == O(log(n)*log(n))?
At 23:14, i think the code should say ""return res"" instead of ""return result"""
7lQXYl_L28w,"And you can also see, the size of that set's doubling each time.
Because you get to 4, I'm going to add everything in to all of those pieces-- really nice recursive description.
Let's write some code.
So I'll also hand it out to you, but here's the code.
And I'm going to walk through it carefully.
And then we're going to analyze it.
But it's actually, for me, a beautiful piece of code.
I did not write it, by the way, John did.
But it's a beautiful piece of code.
I want to generate all the subsets with a power set of some list of elements.
Here's how I'm going to do it.
I'm going to set up some internal variable called res, OK? And then, what am I going to do? Actually, I don't know why I put res in there.
I don't need it.
But we'll come back to that.
If the list is empty, length of the list is 0, I'm going to just return that solution.
And this is not a typo.
What is that funky thing there? It is a list of one element, which is the empty list, which I need.
Because the solution in this case is a set with nothing in it.
So there is the thing I return in the base case.","35:52 , 37:29 It was a mistake."
7lQXYl_L28w,"Something that grows linearly is not bad.
Something that grows, as we've seen down here, exponentially tends to say, this is going to be painful.
And in fact, you can see that graphically.
I'll just remind you here.
Something that's constant says, if I draw out the amount of time it takes as a function of the size of the input, it doesn't change.
Logarithmic glows-- gah, sorry-- grows very slowly.
Linear will grow, obviously, in a linear way.
And I actually misspoke last time.
I know it's rare for a professor to ever admit they misspeak, but I did.
Because I said linear is, if you double the size of the input, it's going to double the amount of time it takes.
Actually, that's an incorrect statement.
Really, what I should have said was, the increment-- if I go from, say, 10 to 100, the increase in time-- is going to be the same as the increment if I go from 100 to 1,000.
Might be more than double depending on what the constant is.
But that growth is linear.
If you want to think of it, take the derivative of time with respect to input size.
It's just going to be a constant.
It's not going to change within.
And of course, when we get down to here, things like exponential, it grows really fast.
And just as a last recap, again, I want to be towards the top of that.
There was my little chart just showing you things that grow constant, log, linear, log-linear, quadratic, and exponential.
If I go from n of 10, to 100, to 1,000, to a million, you see why I want to be at the top of that chart.
Something up here that grows logarithmically, the amount of time grows very slowly as I increase the input.
Down here, well, like it says, good luck.
It's going to grow up really quickly as I move up in that scale.","5:30 how? If its linear like k*n+b O(n) then if input(n) is doubled then its 2k*n+b. The worst case is almost doubled. How can get much more larger. 5:51 He said it can be more than doubled
Around 5:30 he says ""... linear is, if you double the size of the input, it's going to double the amount of time it takes. Actually, that's an incorrect statement. Really, what I should have said was, the increment--if I go from, say, 10 to 100, the increase in time is going to be the same as the incremeent if I go from 100 to 1000. Might be more than double depending on what the constant is. But that growth is linear."" I don't get the difference... he's saying 10->100 = 10x, 100->1000 = 10x, right? And the earlier version was 1->2 = 2x and 2->4 = 2x. Aren't these the same? What am I missing here?"
7lQXYl_L28w,"When you're given a new problem, how do I get this into a linear algorithm if I can? Log-linear, if I can, would be really great.
But you know, if I can't, how do I stay away from exponential algorithms? And finally, what we're going to show later on is that, in fact, there are some problems that, as far as we know, are fundamentally exponential.
And they're expensive to compute.
The very last thing is, you might have decided I was cheating in a different way.
So I'm using a set of built-in Python functions.
I'm not going to go through all of these.
But this is just a list, for example, for lists, of what the complexity of those built-in functions are.
And if you look through the list, they kind of make sense.
Indexing, you can go straight to that point.
Computing the length, you compute it once, you've stored it.
Comparison-- order n, because I've got to compare all the elements of the list.
Similarly, to remove something from the list, I've got to find where it is in the list and remove it.
Worst case, that's going to be order n.
So you can see that these operations are typically linear in the size of a list.
These are constant.
For dictionaries, remember, dictionaries were this nice thing.
They weren't ordered.
It gave me a power in terms of storing them.
But as a consequence, some of the costs then go up.
For a list, indexing, going to a particular point, I just go to that spot and retrieve it.
Indexing into a dictionary, I have to find that point in the dictionary that has the key and get the value back.
So that's going to be linear, because I have to, in principle, walk all the way down it.
It's a slight misstatement, as we'll see later on.
A dictionary actually uses a clever indexing scheme called a hash.
But in the worst case, this is going to be linear.",I think the complexity of finding the dictionary length should be O(1). Why is it O(n)? 48:27
8C_T4iTzPCU,"All right, I'm on a roll not just with Frisbees.
I finished the proof with a finger to spare.
So f of S T equals c of S T.
All right, so that's exactly what we want.
We are saying f of S T is obviously a cardinality of f so I've shown this thing over here.
So that's why the Ford-Fulkerson algorithm works.
It's because of this analysis that the Ford-Fulkerson algorithm works.
So are we done? What are we missing in algorithm design, our algorithm analysis? Not you, yet.
AUDIENCE: [INAUDIBLE] PROFESSOR: Sorry? AUDIENCE: A runtime? PROFESSOR: Runtime, good runtime.","in the proof from 17:00-35:00 Isn't he just proofing this for the special cut he defined? But the first part of the theorem states |f| = c(S,T) for ANY S,T cut"
8C_T4iTzPCU,"And how many iterations do you really need if you did it right? Two.
So that's a billion factor slowdown.
So this is a pathological example, a simple pathological example, to just show you what the problem is.
But you can imagine that if you use depth-first search you might be a factor of five slower on average than if you use some of the technique.
And a factor of 5 is nothing to be scoffed at, especially if you're running for minutes.
And, you know, back in the day computers were horribly slow.
So how is this problem fixed? Well, any number of ways.
But the first real way that took into account asymptotitc complexity, did analysis, and did all of that was due to Edmonds and Karp, which is a few years after Ford-Fulkerson.
In fact, several years after Ford-Fulkerson.
And their contribution was not as much a new algorithm, though it is called Edmonds-Karp algorithm.","at 42:49, I didn't understand why are there 2 billion iterations? Doesn't it keep oscillating between the two paths?"
8C_T4iTzPCU,"And the proof, the max flow, min-cut theorem, which is going to show this key result that we require, which is that when we terminate in the Ford-Fulkerson algorithm, we're going to have a max flow.
And that's the reason why it's a maximum flow algorithm.
If you don't have that proof, you don't have a max flow algorithm.
So hopefully all of that is clear.
Pipe up if you have questions.
And let's write out the max flow min-cut theorem, which I mentioned of it last time but never really got to even stating, but today we're going to state it and prove it.
So this is an interesting theorem.
I mean it's a legendary theorem.
And it's not your usual theorem in the sense that it makes a simple statement.
It actually makes three statements.
It says the following are equivalent, and it's got three things which are the following.","in the proof from 17:00-35:00 Isn't he just proofing this for the special cut he defined? But the first part of the theorem states |f| = c(S,T) for ANY S,T cut"
8C_T4iTzPCU,"So it's simply the S to T edges and summing over the capacities.
And if you take a look, obviously S to T, you've got 4 plus 4 plus 4.
So you've got 4 plus 4 plus 4, corresponding to this one, this one, and that one.
Do I need to add this edge in here? Over here? Where does this edge go? From this to over there? That goes from T to S.
So that's good because that has a capacity of infinity.
That would cause trouble.
And so the other edges are I got 1, 5, and 7.","1:21:00 Can someone help me out? srini said that it has capacity infinite in that case it makes sense in not considering it in min cut value cuz it is limited by the source (s), but we are going from 'S' to 'T' for those flows but the infinite flow is from ""T"" to ""S"" (S and T are the partition that we obtained after cut) so it must be -infinite ....why don't we consider the flow through cut here as we did in (https://youtu.be/VYZGlgzr_As?t=3288)....... I understood the capacity of the middle region is infinite because they are limited by the incoming flow from source(so that will be adjusted accordingly)."
8C_T4iTzPCU,"What are these pairs? As you can imagine, these pairs correspond to the games that each of these teams that are inside the circle play against each other.
So 3 plays 4 a certain number of times.
According to that table it's 4 times.
So I'm going to put a 4 in here.
This is 4 as well.
4, 5, 7, 2.
OK? And the edges in between here are going to have capacities of infinity and how are these edges structured? So far I've just explained how the left-hand side works.
These edges have a capacity of infinity.
1 2 goes to 1, 1 2 goes to 2.
1 3 goes to 1, 1 3 goes to 3.
And that's pretty much it.
So that's where these edges are.
That's how these edges are introduced.
And all of these edges have a capacity of infinity.
2 to 4, 3 to 4, and that.
So far, it's pretty straightforward.
There's one last thing that we need to do, which is add capacities to these edges.
This is actually crucial.
It turns out that we have to add capacities such that this max flow is going to represent elimination.","At 1:08:31, how the capacities of edges are infinite?"
8EfqiUVBI9s,"Unless where- unless there's something in the numerator, unless the x minus x_i norm squared is also very small, and so the similar size to- to Rho squared.
And so what that means is that, um, when Rho is very small, this quantity is going to be 1 when x is close to x_i, and 0 almost everywhere else.
Um, and that means that this weighted sum that I'm going to get right here is going to be the nearest, the value of y at the nearest point x_i, the nearest i, the- the- the, uh, the nearest data point.
Um, so this reverts to the nearest neighbor predictor when Rho is small.
When Rho is large, it becomes a little different.
Let's look at what it becomes.
So I won't go through this code, but this code is- because it's very similar to the previous code.
Um, it, uh, uh just explicitly follows the mathematics again.
So here the- here is the, uh, uh, this- the, uh, soft nearest neighbors predictor.
Uh, this is the case where Rho is 2 on the left here.
And you can see it's really rather smooth.
Uh, when Rho is 1, it's becoming, uh, uh, a little bit more similar to the, uh, uh, the nearest neighbor predictor.
Here, we can see when Rho is 0.5.
Over here, we- we can see something that's really quite close to the- the nearest neighbor predictor.
And, uh, and the nearest neighbor predictor is here on the left, and we can really see that there's a correspondence between those two.","32:41 with the julia code, pho should be pho^2 (according to the formula of the notes)"
8LEuyYXGQjU,"Okay, so what we've said here so far is that we have this approximation where what we do is we just take our policy, we run it out phi m times, for each of those m times we get a whole sequence of states and actions and rewards.
And then we average.
And this is an unbiased estimate of the policy gradient but it's very noisy.
So, this is gonna be unbiased and noisy.
If you think about what we saw before for things like Monte Carlo methods, it should look vaguely familiar, same sort of spirit, right? We have, um, we're just running out our policy.
We're gonna get some sum of rewards just like what we got in Monte Carlo, um, estimates.
But, [NOISE] so, it'll be unbiased estimate of the gradient.
So, it's unbiased estimate of the gradient, estimate of gradient.
But noisy.
So, what can make this actually practical? Um, there's a number of different techniques for doing that.
Um, but some of the things we'll start to talk about today are temporal structure and baselines.
[NOISE] Okay.","If you pause around 57:47 and work on the math yourself you will actually find the equation on the top (first line) does not equal to the last equation (on the bottom). To verifty this, you can use a simple case as a test example, such as a three time step episode with reward r0, r1, r2. So do not her derivation for granted as she is missing lots of details here. Why there is this dissonance here? The reason is because ""the policy's choice at a particular time step t only affects rewards received in later steps of the episode, and has no effect on rewards received in previous time steps"" (https://web.stanford.edu/class/cs234/CS234Win2019/slides/lnotes8.pdf). So the original equation on the top actually does not take this into account. It is imo a very important point that she did not mention here."
8NYoQiRANpg,"And so, lifting Gamma up, maximizing Gamma has effective maximizing the worst-case examples geometric margin, which is, which is, which is how we define this optimization problem, okay? Um, and then the last one step to turn this problem into this one on the left, is this interesting observation that, um, you might remember when we talked about the functional margin, which is the numerator here, that, you know, the functional margin you can scale w and b by any number and the decision boundary stays the same, right? And so, you know, if- if your classifier is y, so this is g of w transpose x plus b, right? So if- let's see the example I want to use, uh, 2, 1.
If w was the vector 2, 1- [NOISE] Let's say that's the classifier, right? Then you can take W and B, and multiply it by any number you want.","A) He spends from 7:00 to 8:30 explaining that scaling the vector w and the real number b by the same constant does not change the line. You say it's ""wasteful to apply so much attention to that idea"", but it's not necessarily obvious to everyone and he's just going slowly so that everybody can follow and not spending that much time, really. B) The rest of the time in the introduction, he is talking about a SPECIFIC clever choice of scaling (which makes ||w|| = 1/gamma) which converts the problem from something that was previously difficult to solve, into something solvable. You entirely missed the point of the scaling talk, despite him repeatedly stating the point in lecture 6 and here in lecture 7. None of this has to do with ""simple application of elementary probability theory"", and the notation he uses is already as simple as it can be to express the idea. If anything, he is simplifying a lot of math involved and not deriving everything in detail (he signals extra resources such as lecture notes for those interested in reading further). You might benefit from rewatching the lectures again and paying more attention."
8NYoQiRANpg,"As well as how you make predictions is, um-uh, is expressed only in terms of inner products, okay? So we're now ready to apply kernels and sometimes in machine learning people sometimes we call this a kernel trick and let me just the other recipe for what this means, uh, step 1 is write your whole algorithm, [NOISE] um.
[NOISE] In terms of X_i, X_j, in terms of inner products.
Uh, and instead of carrying the superscript, you know X_i, X_j, I'm sometimes gonna write inner product between X and Z, right? Where X and Z are supposed to be proxies for two different training examples X_i and X_j but it simplifies the notation, uh, right a little bit.",12:00 discuss why \theta := \sum_{i=1}^n \beta_i \phi(x^{(i)}) is a reasonable assumption. 28:50 Kernel tricks 33:50 No free lunch theorem sucks
8NYoQiRANpg,"I can multiply this by 10, [NOISE] and this defines the same straight line, right? Um, so in particular, I think, uh, let's see with this 2 1x.
[NOISE] This actually defines the decision boundary that looks like that.
Uh, if this is X1 and this is X2, then this is the equation of the straight line where W transpose X plus B equals 0, right? Uh, that's uh, one, and two.
Uh, you can- you can verify it for yourself.
You plug in this point, then W transpose X plus B equals 0.
We plug in this point, W transpose X equals 0, um and so that's the decision boundary where the, uh- as yet we'll predict positive [NOISE] everywhere here and we'll predict [NOISE] negative everywhere to the lower left, and this straight line, you know, stays the same even when you multiply these parameters by any constant, okay? Um, and so, um, to simplify this, uh, notice that you could choose anything you want for the normal W, right? Just by scaling this by a factor of 10, you can increase it, or scaling it by a factor of 1 over 10, you can decrease it.
But you have the flexibility to scale the parameters W and B, you know, up or down by any fixed constant without changing the decision boundary, and so the trick to simplify this equation into that one is if you choose [NOISE] to scale the normal W to be equal to 1 over gamma.","A) He spends from 7:00 to 8:30 explaining that scaling the vector w and the real number b by the same constant does not change the line. You say it's ""wasteful to apply so much attention to that idea"", but it's not necessarily obvious to everyone and he's just going slowly so that everybody can follow and not spending that much time, really. B) The rest of the time in the introduction, he is talking about a SPECIFIC clever choice of scaling (which makes ||w|| = 1/gamma) which converts the problem from something that was previously difficult to solve, into something solvable. You entirely missed the point of the scaling talk, despite him repeatedly stating the point in lecture 6 and here in lecture 7. None of this has to do with ""simple application of elementary probability theory"", and the notation he uses is already as simple as it can be to express the idea. If anything, he is simplifying a lot of math involved and not deriving everything in detail (he signals extra resources such as lecture notes for those interested in reading further). You might benefit from rewatching the lectures again and paying more attention."
8NYoQiRANpg,"Right? So um, and it turns out that when X I is you know, 100 trillion dimensional, doing this will let us derive algorithms that work even in these  100 trillion or these infinite-dimensional feature spaces.
Now, I'm just deriving this uh, just as an assumption.
It turns out that there's a theorem called the representer theorem that shows that you can make this assumption without losing any performance.
Uh, the proof that represents the theorem is quite complicated.
I don't wanna do this in this class, uh, it is actually written out, the proof for why you can make this assumption is also written in the lecture notes, it's a pretty long and involved proof involving primal dual optimization.
Um, I don't wanna present the whole proof here but let me give you a flavor for why this is a reasonable assumption to make.
Okay? And when- just to- just to make things complicated later on uh, we actually do this.
Right? So Y I is always plus minus 1.",12:00 discuss why \theta := \sum_{i=1}^n \beta_i \phi(x^{(i)}) is a reasonable assumption. 28:50 Kernel tricks 33:50 No free lunch theorem sucks
8sOtXbQIOuE,"In general, the filtering distribution is the probability of the variables that you are considering conditioned on the evidence so far.
And suppose we have just two particles here, 0, 1, and 1, 2.
So now the propose step is going to take each of these particles, and I'm just going to sample a value for H3 the next variable-- given the transmission distribution.
Remember it was up and down with probability one quarter and the same with probability 1/2.
So that is going to produce these extended articles conditioned on the same evidence.
So for example, I'm going to take 0, 1.
I'm going to-- now that will produce this particle with probability 1/2 because I'm just keeping the value of the same year.
And I'm going to take this particle, and I'm going to extend it to 2.
And that's also going to happen with probability 1/2.
Now, this is a random algorithm.
So I could have sampled from the distribution.
I could have got 1 here.
I could have gotten 3 here, but let's just go with 1, 2.
So in the next step, I'm going to-- wait, so you should think about these particles as a guess as to what H3 is going to be.
But we need to fact-check this guess with evidence.
And so the weighting step is going to assign a weight to each article.
And that weight is going to be the probability of the new evidence I got conditioned on H3 here.
So this is going to produce a set of new particles, which are weighted representing the distribution-- the h1, h2, h3 conditioned on all the evidence so far.","at 10:07 example the probablities doesnt add up to 1, plz expln"
9TNI2wHmaeI,"And I'd like to talk a little bit about the relationship between NP-completeness and crypto.
Because we've made these assumptions about hardness.
Now, what's interesting here is that N composite is clearly in NP, but unknown if NP-complete.
So this is very interesting.
The tried and trusted algorithm for public key encryption relies on a computational assumption where the problem associated with that assumption is not even known to be NPC.
All right.
So that's kind of wild.
So how does this work? Or why does this work? Now, if you take other problems, like, is a graph 3-colorable? And so what does that mean? Well, you have three colors.
And you're not allowed to reuse the same color on two ends of an edge.
So if you put red over here, you can put red here, but you can't put red here and there.
And so that graph is 3-colorable.
But if you had a click, then this would not be 3-colorable.
Because you have all these edges.
You have three edges coming out.
And so clearly, the degree from a vertex is going to tell you what you have.
So if you have a 4-click over there, immediately it's not 3-colorable.
But checking whether a graphic is 3-colorable is NPC.
You can use a three set as a way of showing that.
So you can say, oh, wow, maybe I shouldn't be worried about RSA.","1:07:45 I don't think ""Is N composite?"" is the correct decision problem for integer factorization. Since ""PRIMES is in P"" via AKS, we can run AKS on an integer to answer if it's composite or not. -> Run AKS, return opposite of what it says. A correct decision problem for integer factorization would be something like: ""Given N, a, b, does N have a factor d such that a < d <= b?"" This is in NP because you can verify a factor in polynomial time (division algorithm). We can then factor an integer in polytime using a polytime oracle that solves this decision problem -- just binary search for factors starting from an interval (1, N-1] by consulting the oracle (we can restrict our search to prime factors using AKS), then divide out by prime factors as many times as possible whenever one is found. Return N as the only factor if the first oracle call returns `No`. This algorithm calls the oracle O(log N) times for each factor, and N has O(log N / log log N) unique factors (via a theorem of number theory), so all-in-all, this yields a polytime algorithm. Of course, you'd need a polytime oracle for this to actually be implemented, but I'll leave constructing such a thing to the experts. :)"
9g32v7bK3Co,"And again if you remember search problems, the solution to search problems was just a sequence of actions, said that's all I had, like a sequence of actions, a path that was a solution.
And the reason that was a good solution was like everything was deterministic, so I could just give you the path and then that was what you would follow.
But in the case of MDPs, the way we are defining a solution is by using this notion of a policy.
So a policy- let me actually write that here.
So we have defined an MDP but now I want to say well, what is a solution of an MDP? A solution of an Markov decision pro- process is a policy pi of S.","At 29:36, a policy is defined as a one-to-one mapping from the state space to the action space; for example, the policy when we are in station-4 is to walk. This definition is different compated to the one made in the classic RL book by Sutton and Barto; they define a policy as ""a mapping from states to probabilities of selecting each possible action."" For example, the policy when we are in station-4 is a 40% chance of walking and 60% chance of taking the train. The policy evaluation algorithm that is presented in this lecture also ends up being slightly different by not looping over the possible actions. It is nice of the instructor to highlight that point at 55:45"
9g32v7bK3Co,"Those are like, the only things I'm storing, because that allows me to compute and if I've converged then that kind of allows me to keep going because I only need my previous values to update my new values, right.
In terms of complexity, well this is going to take order of T times S times S prime.
Well, why is that? Because I'm iterating over T times step, and I'm iterating over all my states and I'm summing over all S primes, right.
So because of that- that's a complex idea yet, and one thing to notice here, is it- it doesn't depend on actions, right.
It doesn't depend on the size of actions.
And the reason it doesn't depend on the size of actions as you have given me the policy, you are telling me follow this policy.
So if you've given me the policy then I don't really need to worry about, like, the number of actions I have.
Okay.
All right.
Um, here is just another like the same example that we have seen.
So at iteration T equal to 1, in, is going to get 4, end is going to get 0, at iteration 2 it gets a slightly better value.","At 29:36, a policy is defined as a one-to-one mapping from the state space to the action space; for example, the policy when we are in station-4 is to walk. This definition is different compated to the one made in the classic RL book by Sutton and Barto; they define a policy as ""a mapping from states to probabilities of selecting each possible action."" For example, the policy when we are in station-4 is a 40% chance of walking and 60% chance of taking the train. The policy evaluation algorithm that is presented in this lecture also ends up being slightly different by not looping over the possible actions. It is nice of the instructor to highlight that point at 55:45"
A6Ud6oUCRak,"And these have to add up to 1.
So that would be 0.9, that would be 0.0, that would be 0.5, and this would be 0.0.
So because those are just 1 minus the numbers in these columns, I don't bother to write them down.
Well, we still have a couple more things to do.
The probability that we'll call the police depends only on the dog.
So we'll have a column for the dog, and then we'll have a probability of calling the police.
There's a probability for that being false and a probability for that being true.
So if the dog doesn't bark, there's really hardly any chance we'll call the police.
So make that 0, 0, 1.
If the dog is barking, if he barks vigorously enough, maybe 1 chance in 10.
Here, we have the trash can-- the final thing we have to think about.","I think , at 42:56 , the phantom probability are wrong, since they should add up to one..."
A6Ud6oUCRak,"So let's go from here over to here and say that the probability of a whole bunch of things-- x1 through x10-- is equal to some product of probabilities.
We'll let the index i run from n to 1.
Probability of x to the last one in the series, conditioned on all the other ones-- sorry, that's probability of i, i minus 1 down to x1 like so.
And for the first one in this product, i will be equal to n.
For the second one, i will be equal to n minus 1.
But you'll notice that as I go from n toward 1, these conditionals get smaller-- the number of things on condition get smaller, and none of these things are on the left.","I have a question: in 28:45 he explain the general formula for conditional probabilities. However, if we apply it on the case P(a,b,c), where x1 = a, x2 = b, and x3 = c, we do not obtain the same result as the professor as the probabilities seem to be shifted in the other way. In other words, we finish with P(a) as the last term and not P(c). Can someone explain me where is my mistake ? Thank you"
A6Ud6oUCRak,"What's that mean? That means that if you know that we're dealing with z, then the probability of a doesn't depend on b.
b doesn't matter anymore once you're restricted to being in z.
So you can look at that this way.
Here's a, and here's b, and here is z.
So what we're saying is that we're restricting the world to being in this part of the universe where z is.
So the probability of a given b and z is this piece in here.
a given b and z is that part there.
And the probability of a given z is this part here divided by all of z.",Is the region of z at 35:25 correct? I think it should cover (a and b)
AbhV49lfaWw,"There are three parts.
And we also covered regularization before we go into regularization, so we've just discussed the three components.
We still haven't spoken of any trade-offs between them.
But this- this is the mental model to have in, you know, in your mind to decompose the- the test error into three parts, right? Part due to noise in the test data, that's irreducible error, part due to noise in the training data, that's variance and bias.
Now, we also spoke about regularization and we will see why regularization, er, plays a role in, um, um, shortly, soon.
So regularization is a way in which we want to penalize our estimated parameters from having very large values.","At 11:50 timestamp, we talk about ""noise"" in the training data that results in variance when we test different models on the same test data point. These different models are built when we take different samples. The noise that we are referring to, is it just due to different samples or a combination of different samples + irreducible error of each y_train(because each y has its own distribution, and in the case of regression, its gaussian) from the training data?"
Amd_bNYzgUw,"And this follows as a trivial consequence of the inclusion-exclusion rule for two sets, because the probability of A union B is equal to this plus this minus some probability, namely, the probability of the intersection.
So you're taking away something non-negative from these two in order to equal that.
In particular, then, this must be less than or equal to that.
And the closely related phenomenon is [? basi-- ?] [AUDIO OUT] The probability that A or B happens is greater than or equal to the probability that A happens.","8:06 should it's caption be ""union bound""? or ""boole's inequality""?"
B5y47gWt3co,"So the way we write this in terms of, um, uh, GIN operator is to say, aha, we are taking the messages from the children, we aggregate- we transform them using an MLP, this is our function f, and we summed them up.
Um, and then we also add 1 plus epsilon, where epsilon is some small, uh, learnable scalar, our own message transformed by f and then add the two together and pass through another function, uh, phi.",What is the use of the term (1 + epsilon) at 23:15 (slide 70)?
BZTWXl9QNK8,"So they're on a scale of minutes.
So if you connect within the same minute, then you're in good shape.
And if you connect on the minute boundary, well, too bad.
Yet another problem with the scheme-- it's imperfect in many ways.
But most operating systems, including Linux, actually have ways of detecting if there's too many entries building up in this table that aren't being completed.
It switches to this other scheme instead to make sure it doesn't overflow this table.
Yeah.
AUDIENCE: So if the attacker has control of a lot of IP addresses, and they do this, and even if you switch it the same-- PROFESSOR: Yeah, so then actually there's not much you can do.
The reason that we were so worried about this scheme in the first place is because we wanted to filter out or somehow distinguish between the attacker and the good guys.
And if the attacker has more IP addresses and just controls more machines than the good guys, then he can just connect to our server and request lots of web pages or maintain connections.
And it's very hard then for the server to distinguish whether these are legitimate clients or just the attacker tying up resources of the server.
So you're absolutely right.
This only addresses the case where the attacker has a small number of IP addresses and wants to amplify his effect.","If the timestamp is part of the message, then why does it matter if ""you connect on the minute boundary""? (1:08:32)"
C1lhuz6pZC0,"And if we know that, what's the order? AUDIENCE: [INAUDIBLE].
JOHN GUTTAG: N log n plus n-- I guess is order n log n, right? So it's pretty efficient.
And we can do this for big numbers like a million.
Log of a million times a million is not a very big number.
So it's very efficient.
Here's some code that uses greedy.
Takes in the items, the constraint, in this case will be the weight, and just calls greedy, but with the keyfunction and prints what we have.
So we're going to test greedy.
I actually think I used 750 in the code, but we can use 800.
It doesn't matter.
And here's something we haven't seen before.
So used greedy by value to allocate and calls testGreedy with food, maxUnits and Food.getValue.
Notice it's passing the function.
That's why it's not-- no closed parentheses after it.
Used greedy to allocate.","28:42 what is ""item"" that used for ?"
C1lhuz6pZC0,"On the other hand, if we use greedy by cost, I get 318 happiness points and a different menu, the apple, the wine, the cola, the beer, and the donut.
I've lost the pizza and the burger.
I guess this is what I signed up for when I put my preferences on.
And here's another solution with 318, apple, wine-- yeah, all right.
So I actually got the same solution, but it just found them in a different order.
Why did it find them in a different order? Because the sort order was different because in this case I was sorting by density.
From this, we see an important point about greedy algorithms, right, that we used the algorithm and we got different answers.","36:48 donut should have 95 in calories instead of 195 showing in the result, and apple should be 150, not 95."
C1lhuz6pZC0,"So let's go look at the code that does this.
So here you have it or maybe you don't, because every time I switch applications Windows decides I don't want to show you the screen anyway.
This really shouldn't be necessary.
Keep changes.
Why it keeps forgetting, I don't know.
Anyway, so here's the code.
It's all the code we just looked at.
Now let's run it.
Well, what we see here is that we use greedy by value to allocate 750 calories, and it chooses a burger, the pizza, and the wine for a total of-- a value of 284 happiness points, if you will.","[36:00] I don't get why we get different answers in the greedy algorithms as long as we use the same items and the same key function It does local optimization, but it does not mean that local optimization is different each time we run the program given the same parameters"
C6EWVBNCxsc,"So each level, in fact, is going to be exactly n over b cost.
We should be a little careful about the bottom because the base case-- I mean, it happens that the base case matches this.
But it's always good practice to think about the leaf level separately.
But the leaf level is just m over b times n over m The m's cancel, so m over b times n over m.
This is n over b.
So every level is n over b.
The number of levels is log of n over m.
Cool.
So the number of memory transfers is just the product of those two things.","I am curious why in 45:42, the height is lgN - lgM rather than lg(N/B) - lg(M/B), although result is the same but a little confused."
CAKSh3M0y8k,"Now, I can also cancel k if it's relatively prime to n.
And the reason is that if I have ak equivalent to bk mod n and the gcd of k and n is 1, then I have this k prime that's an inverse of k.
So, I just multiply both sides by the inverse of k, namely k prime.
And I get that the left hand side is a times k, k inverse.
And the right hand side is b times k, k inverse.
And of course, that's a times 1 is equivalent to b times 1.",03:26 Where's the proof for associativity of modular multiplication then? ;>
CG4ihzTaGdM,"So y of minus 1 is x of minus 1 minus x of minus 2.
Since both of those are 0, it says that the output at time minus 1 is 0.
Trivial, right? Trivial.
And similarly, we can just iterate through the solution to the whole signal.
So y of 0 is x of 0 minus x of minus 1.
x of 0 is that special one, that is 1.
So now we get 1 minus 0, which is 1.
y of 1 is x of 1 minus x of 0.
Now the special one is on the other side of the minus sign, so the answer is minus 1.
y of 2 is x of 2 minus x of 1 -- they're both 0.
And in fact, all the answers from now on are going to be 0.
So what I just did is a trivial example of -- I use a difference equation to represent a system, and I figured out the output signal from the input signal.
That's the method that we call-- that's the representation for discrete time systems that we refer to as difference equations.","24:12 ??? Isn't x[2] = 2 Since on the x axis the numbers are: -1 for x[-1], 0 for x[0] and so on... If it isn't so, where do the x's come from? I don't get it, this is not trivial..."
CHhwJjR0mZA,"This is a bit circular.
I'm going to define an array in terms of the word RAM, which is defined in terms of arrays.
But I think you know the idea.
So we have a big memory which goes off to infinity, maybe.
It's divided into words.
Each word here is w bits long.
This is word 0, word 1, word 2.
And you can access this array randomly-- random access memory.
So I can give you the number 5 and get 0 1, 2, 3, 4, 5, the fifth word in this RAM.
That's how actual memories work.
You can access any of them equally quickly.
OK, so that's memory.
And so what we want to do is, when we say an array, we want this to be a consecutive chunk of memory.
Let me get color.
Let's say I have an array of size 4 and it lives here.
Jason can't spell, but I can't count.
So I think that's four.
We've got-- so the array starts here and it ends over here.
It's of size 4.
And it's consecutive, which means, if I want to access the array at position-- at index i, then this is the same thing as accessing my memory array at position-- wherever the array starts, which I'll call the address of the array-- in Python, this is ID of array-- plus i.
OK.
This is just simple offset arithmetic.
If I want to know the 0th item of the array, it's right here, where it starts.
The first item is one after that.
The second item is one after that.
So as long as I store my array consecutively in memory, I can access the array in constant time.
I can do get_at and set_at as quickly as I can randomly access the memory and get value-- or set a value-- which we're assuming is constant time.","If you cannot explain arrays without referring to physical structure, words and bits then you have failed. Even at 10:48 he gives the impression that array-elements are generic Words when an array really have elements of any size. My instructors 35 years ago did it better."
E-_ecpD5PkE,"So they're both important, they're doing slightly different things.
All right.
So let's talk about, ah, could we move this up, please? Thank you.
Okay.
So like we sort of started talking about before, um, well, let's- let's talk about first the baseline.
So how should we choose the baseline? Um, one thing that we can do for the baseline, is just to- like what that what we're seeing there, which is an empirical estimate of V_Pi i.
So we could say, in general, we wanna just have- use V_Pi i as a baseline.
That means we have to compute it somehow.
And the way we estimate that could be from Monte Carlo or it could be from TD methods.
All right.","1:03:06, why now the advantage function's definition becomes the subtration of two Q value functions? I though previously advantage function was defined as the subtraction of Q (state-action) and V (value) function?"
E3f2Camj0Is,"They don't have to be- This has- doesn't have to be anything to do with value iteration.
These are just two different value functions.
One could be, you know, 1,3,7,2 and the other one could be 5,6,9,8.
Okay.
So we just have two different vectors of value functions and then we re-express what they are after we apply the Bellman backup operator.
So there's that max a, the immediate reward plus the discounted sum of future rewards where we've plugged in our two different value functions.
And then what we say there is, well, if you get to pick that max a separately for those two, the distance between those is lower bounded than if you kind of try to maximize that difference there by putting that max a in.
And then you can cancel the rewards.
So that's what happens in the third line.
And then the next thing we can do is we can bound and say the difference between these two value functions is diff- is, um, bounded by the maximum of the distance between those two.
So you can pick the places at which those value functions most differ.
And then you can move it out of the sum.",Does anybody understand how did she get to 2nd step of the equation on 1:11:56?
E3f2Camj0Is,"Um, I in this case because we're thinking about processes that are infinite horizon, the value function is stationary, um, and it's fine if you have include self loops.
So, it's fine if some of the states that you might transition back to the same state there's no problem.
You do need that this matrix is well-defined.
That you can take that you can take the inverse of it.
Um, but for most processes that is.
Um, so, if we wanna solve this directly, um, this is nice it's analytic, um, but it requires taking a matrix inverse.
And if you have N states so let's say you have N states there's generally on the order of somewhere between N squared and N cubed depending on which matrix inversion you're using.
Yeah.
Is it ever actually possible for, uh, that matrix not to have an inverse or does like the property that like column sum to one or something make it not possible? Question was is it ever possible for this not to have an inverse? Um, it's a it's a good question.
Um, I think it's basically never possible for this not to have an inverse.
I'm trying to think whether or not that can be violated in some cases.
Um, if yeah sorry go ahead.
Okay.
[NOISE] Yeah.
So, I think there's a couple, um, if there's a- if this ends up being the zero matrix, um depending on how things are defined.
Um, but I'll double-check then send a note on a Piazza.
Yeah.
Well, actually I think the biggest side about the transition matrix [inaudible] Let me just double check so I don't say anything that's incorrect and then I'll just send a note on- on Piazza.","25:47 Conjecture: inverse exists if gamma in [0,1), and fails to exist if gamma=1. Easy to check for 1 or 2 state systems."
EC6bf8JCpDQ,"Ah.
But first of all, none of those expressions condition any of the variables on anything other than non-descendants, all right? That's just because of the way I've arranged the variables.
And I can always do that because are no loops.
I can always chew away at the bottom.
That ensures that whenever I write a variable, it's going to be conditioned on stuff other than its descendants.
So all of these variables in any of these conditional probabilities are non-descendants.
Oh wait.
When I drew this diagram, I asserted that no variable depends on any non-descendant given its parents.
So if I know the parents of a variable I know that the variable is independent of all other non-descendants.
All right? Now I can start scratching stuff out.
Well, let's see.
I know that C, from my diagram, has only one parent, D.
So given its parent, it's independent of all other non-descendants.
So I can scratch them out.
D he has two parents, B and R.
But given that, I can scratch out any other non-descendant.
B is conditional on T and R.
Ah, but B has no parent.
So it actually is independent of those two guys.
The trashcan, yeah, that's dependent on R.
And R over here, the final thing in the chain, that's just a probability.
So now I have a way of calculating any entry in that table because any entry in that table is going to be some combination of values for all those variables.","Thank you for this lecture! In the part where you're explaining the bottom-to-top approach starting minute 9:36 (chewing variables from the bottom), I noticed that you omitted the R variable from P(B | T , R) since B has no parents and is therefore not dependent on both T and R. Intuitively, this all makes sense to me, but there is the ""explaining away"" principle that links B to R since they both cause D. Given that D is correct, there is a relation between B and R. In other words, given that the dog barked, if there is a burglar, this explains away the theory of a Raccoon being present (and vice versa). My question is when and how is this ""explaining away"" principle used when modeling a system using belief networks? And if we are to use it, how is this relationship between B and R modeled? I would appreciate any input on this :)
The Markov blanket is used for something a bit stronger: we use it if we want conditional independence of a given node (A) to all other nodes. Then we must condition on the blanket of A. The blanket includes the nodes we saw in lecture 9:00 (parents and descendents of A), but it also includes the other parents of the descendents of A (if they exist). Intuitively, this is because a descendant of A surely depends on all of its parents. We better know the state of those parents if we want to claim how that descendant will behave with respect to A."
EK0sgHPLou8,"So here's one example for the Mixup.
We can generate some virtual examples between two classes.
So the first image in the left-hand side is for the cat.
And then next one is for dog.
And then we can combine them to generate some images in between.
So from cat and dog.
So this image has around the 70% probability to be classified as a cat and the 30% to be classified as a dog.
So this is a very common and a useful ways to do data augmentation.
Any questions about the process for the Mixup? So in domain generation, Mixup itself can improve the performance of domain generations.
So let's see.
We here, we want to do the tissue classification from Camelyon and also to do some learned type prediction.","48:54 how do we make sure that the final interpolated example is coherent, just use a low weight for the off class? or does that not matter much?"
EmSmaW-ud6A,"There's only one negative edge weight here.
What if I just added a large number, or in particular, the negative of the smallest edge in my graph to every edge in my graph? Then I'll have a graph with non-negative weights.
Fantastic.
Why is that not a good idea? Well, in particular, if I did that to this graph, if I added 2 to every edge, the weight of this path, which was the shortest path, changed from weight 3 to weight 9, because I added 2 for every edge.
But this path, which wasn't a shortest path in the original graph-- it had weight 4-- increased only by 2.
Now that is a shortest path.
Or it's a shorter path than this one, so this one can't be a shortest path.
So that transformation, sure, would make all the weights non-negative, but would not preserve shortest paths.","The telescope sum guarantees any path of ""a pair of vertices"" can be reweighted by the same value. But it doesn't maintain them in a global equally way to an entire graph. In other words, in a reweighted graph, the equality relation of edges could be changed. For example, A~D = 500 & B~Z = 300 may reweighted to A~D = 600 & B~Z = 700. So we cannot tell the edge relations rely on telescope-sum-reweighted graphs. This is the opposite of the intuitive add-weight-to-every-edge-till-non-negative method as at 22:22. That doesn't satisfying the reweighting requirement but it preserves the equality relation between edges. This is my blind spot, I was originally try to understand how it reweights while maintain the global relations."
EzeYI7p9MjU,"And then you just choose the line, the vertical line such that you've got a bunch of points that are on either side.
And then in terms of the merge operation, we have 2t n over 2 plus theta n.
People recognize this recurrence? It's the old merge sort recurrence.
So we did all of this in-- well, it's not merge sort.
Clearly the algorithm is not merge sort.
We got the same recurrence.
And so this is theta n log n-- so a lot better than theta nq.
And there's no convex hull algorithm that's in the general case better than this.
Even the gift wrapping algorithm that I mentioned to you, with the right data structures, it gets down to that in terms of theta n log n, but no better.
OK, so good.
That's pretty much what I had here.
Again, like I said, happy to answer questions about the correctness of this loop algorithm for merge later.
Any other questions associated with this? STUDENT: Question.
Yeah, back there.
STUDENT: If the input is recorded by x coordinates, can you do better than [INAUDIBLE]? PROFESSOR: No, you can't, because-- I mean, the n log n for the pre-sorting, I mean, there's another theta n log n for the sorting at the top level.
And we didn't actually use that, right? So the question was, can we do better if the input was pre sorted? And I actually did not even use the complexity of the sort.","44:00 Gift wrapping may be better than devide & conquer; it has O(nh) time complexity (not nlogn as the professor mentioned), where n is the number of points and h is the number of points on the convex hull. https://en.wikipedia.org/wiki/Gift_wrapping_algorithm"
EzeYI7p9MjU,"I mean, you could have defined it differently.
We're going to go with less than or equal to.
So in general, the rank, of course, is something that could be used very easily to find the median.
So if you want to find the element of rank n plus 1 divided by 2 floor, that's what we call the lower median.
And n plus 1 divided by 2 ceiling is the upper median.
And they may be the same if n is odd.
But that's what we want.","Did anyone find maybe the definition of rank at : 501 00:53:37,070 --> 00:53:53,880 And so in general, we're going to define, given a set of n numbers, define rank of x 502 00:53:53,880 -> 00:54:06,510 as the numbers in the set that are greater than- I'm sorry, less than or equal to x. 503 00:54:06,510 --> 00:54:09,270 I mean, you could have defined it differently. We're going to go with less than or equal 504 00:54:09,270 --> 00:54:10,750 to. is a typo? I checked the written note and find ""number of numbers in the set that are smaller than x"" makes more sense compared to rank defined on the black board in the video as ""numbers in the set that are smaller than x"" In short : ""number of numbers in the set"" versus ""numbers in the set """
EzeYI7p9MjU,"Let's say n is odd.
And it's floor of n over 2.
You can find that median.
Right, so it's pretty easy if you can do sorting.
But we're never satisfied with using a standard algorithm.
If we think that we can do better than that.
So the whole game here is going to be I'm going to find the median.
And I want to do it in better than theta n log n time.
OK, so that's what median finding is all about.
You're going to use divide and conquer for this.
And so in general, we're going to define, given a set of n numbers, define rank of x as the numbers in the set that are greater than-- I'm sorry, less than or equal to x.","Did anyone find maybe the definition of rank at : 501 00:53:37,070 --> 00:53:53,880 And so in general, we're going to define, given a set of n numbers, define rank of x 502 00:53:53,880 -> 00:54:06,510 as the numbers in the set that are greater than- I'm sorry, less than or equal to x. 503 00:54:06,510 --> 00:54:09,270 I mean, you could have defined it differently. We're going to go with less than or equal 504 00:54:09,270 --> 00:54:10,750 to. is a typo? I checked the written note and find ""number of numbers in the set that are smaller than x"" makes more sense compared to rank defined on the black board in the video as ""numbers in the set that are smaller than x"" In short : ""number of numbers in the set"" versus ""numbers in the set """
EzeYI7p9MjU,"PROFESSOR: O n-- exactly right.
So on test complexity-- and so we got over theta n cubed complexity, OK? So it makes sense to do divide and conquer if you can do better than this.
Because this is a really simple algorithm.
The good news is we will be able to do better than that.
And now that we have a particular algorithm-- I'm not quite ready to show you that yet.
Now that we have a particular algorithm, we can think about how we can improve things.
And of course we're going to use divide and conquer.
So let's go ahead and do that.
And so generally, the divide and conquer, as I mentioned before, in most cases, the division is pretty straightforward.
And that's the case here as well.
All the fun is going to be in the merge step.
Right, so what we're going to do, as you can imagine, is we're going to take these points.
And we're going to break them up.
And the way we're going to break them up is by dividing them into half lengths.
We're going to just draw a line.
And we're going to say everything to the left of the line is one sub problem, everything to the right of the line is another sub problem, go off and find the convex hull for each of the sub problems.
If you have two points, you're done, obviously.
It's trivial.
And at some point, you can say I'm just going to deal with brute force.
If we can go down to order n cubed, if n is small, I can just apply that algorithm.
So it doesn't even have to be the base case of n equals 1 or n equals 2.",why it is n3 at 20:45
EzeYI7p9MjU,"There's always a little bit of convenience thrown in here.
We will assume that the a has unique elements.
So there's nothing that's x, OK? Good.
So the recurrence, once you do that, is t of n equals-- we're going to just say it's order one for n less than or equal to 140.
Where did that come from? Well, like 140.
It's just a large number.
It came from the fact that you're going to see 10 minus 3, which is 7.
And then you want to multiply that by 2.
So some reasonably large number-- we're going to go off and we're going to assume that's a constant.
So you could sort those 140 numbers and find the median or whatever rank.
It's all constant time once you get down to the base case.
So you just want it to be large enough such that you could break it up and you have something interesting going on with respect to the number of columns.
So don't worry much about that number.
The key thing here is the recurrence, all right? And this is what we have spent the rest of our time on.
And I'll just write this out and explain where these numbers came from.
So that's our recurrence for n less than or equal to 140.
And else, you're going to do this.
So what is going on here? What are all of these components corresponding to this recurrence? Really quickly, this is simply something that says I'm finding the median of medians.",can somebody explain why is it T(n/5) and not 5T(n/5) in 1:17:29. Aren't we doing the recursion 5 times each step.
FgzM3zpZ55o,"So, what does planning involve? Involves optimization, often generalization and delayed consequences.
You might take a move and go early and it might not be immediately obvious if that was a good move until many steps later but it doesn't involve exploration.
The idea and planning is that you're given a model of how the world works.
So, your given the rules of the game, for example, and you know what the reward is.
Um, and the hard part is computing what you should do given the model of the world.
So, it doesn't require exploration.
And supervised machine learning versus reinforcement learning.
It often involves optimization and generalization but frequently it doesn't invo-, involve either exploration or delayed consequences.
So, it doesn't tend to involve exploration because typically in supervised learning you're given a data set.",11:41 I wonder why go game doesn't need exploration. In fact even human player would compute several steps after to see some possibilities.
FkfsmwAtDdY,"So now, I can start using these sets to make assertions about my database that can be useful to know.
So for example, if I want to say that every student is registered for some subject-- which, of course, they are-- what I would say is that D, the set of all students, is a subset of R inverse of J.
So this concise set theoretic containment statement-- d is a subset of R inverse of J-- is a slick way of writing the precise statement that says that all the students are registered for some subject.
Now, happens not to be true by the way.
Because if you look back at that example, Adam was not registered for a subject.","At 10:45, why not use D = R^(-1) (J) to indicate that every student is registered for some subject? It is against my intuition to use the ""is a subset of"" symbol because it is impossible for the domain of discourse to have fewer elements than the set of students who are registered for some subject."
FkfsmwAtDdY,"Which means that R of Jason is that set of two courses that he's associated with or that are associated with him-- that he's registered 6.042 and 6.012.
So at this point, we've applied R to one domain element-- one student Jason.
But the interesting case is when you apply R to a bunch of students.
So the general setup is that if x is a set of students-- a subset of the domain, which we've been showing in green-- then if I apply R to X, it gives me all the subjects that they're taking among them-- all the subjects that any one of them is taking.
Let's take a look at an example.
Well, another way to say it I guess is that R of X is everything in R that relates to things in X.
So if I look at Jason and Yihui and I want to know what do they connect to under R-- these are the subjects that Jason or Yihui is registered for.
The way I'd find that is by looking at the arrow diagram, and I'd find that Jason is taking 042 and 012.
And Yihui is taking 012 and 004.
So between them, they're taking three courses.
So R of Jason, Yihui is in fact 042, 012, and 004.
So another way to understand this idea of the image of a set R of X is that X is a set of points in the set that you're starting with called the domain.
And R of X is going to be all of the endpoints in the other set, the codomain, that start at X.
If I said that as a statement in formal logic or in set theory with logical notation, I would say that R of X is the set of j in subjects such that there is a d in X such that dRj.
So what that's exactly saying that dRj says that d is the starting point in the domain.
d is a student.
j is a subject.
dRj means there's an arrow that goes from student d to subject j.
And we're collecting the set of those j's that started some d.
So an arrow from X goes to j is what exists at d an X.
dRj means-- written in logic notation-- it's really talking about the endpoints of arrows, and that's a nice way to think about it.","Something is troubling me... At 6:53 the teacher uses the pipe symbol "" | "" and the dot symbol "" . "" but both are translated as "" such as "" Is there a difference between those two symbols ?"
FkfsmwAtDdY,"p for a professor.
Composition of R with V.
j for a subject holds if and only if professor p has an advisee registered in subject j.
Let's see how you figure that kind of thing out.
So I'm going to draw the V relation which goes from p professors to D students and then the R relation that goes from D students to J subjects.
And by showing them in this way, I can understand the composition of R and V as following two arrows.
You start off, say, at ARM, and you follow a V arrow from ARM to his advisee, Yihui.
Then you follow another arrow from Yihui to 6.012, and you discover, hey, ARM has an advisee in-- So now we can say that professor ARM is in the relation R composed with V with 6.012 because of this path ARM has Yihui as an advisee, and Yihui is registered for 6.012.
And this relation R o V, we figured out, is the relation that the professor has an advisee in the subject.
So in general, what we can say is that a professor p is in the R o V relation to j if and only if-- and here we're going to state it in formal logical notation, which really applies in general, not just to this particular example.",14:34 there is a missing arrow from TLP to Yihui
FlGjISF3l78,"So you can have multiple levels of inheritance.
What happens when you create an object that is of type something that's been-- of a type that's the child class of a child class of a child class, right? What happens when you call a method on that object? Well, Python's are going to say, does a method with that name exist in my current class definition? And if so, use that.
But if not, then, look to my parents.
Do my parents know how to do that, right? Do my parents have a method for whatever I want to do? If so, use that.
If not, look to their parents, and so on and so on.
So you're sort of tracing back up your ancestry to figure out if you can do this method or not.
So let's look at a slightly more complicated example.
We have a class named Person.
It's going to inherit from Animal.
Inside this person, I'm going to create my own-- I'm going to create an __init__ method.
And the __init__ method is going to do something different than what the animal's __init__ method is doing.
It's going to take in self, as usual.
And it's going to take in two parameters as opposed to one, a name and an age.
First thing the __init__ method's doing is it's calling the animal's __init__ method.
Why am I doing that? Well, I could theoretically initialize the name and the age data attributes that Animal initializes in this method.","why to write Animal__init__(self, age ) at all? in 28:54"
G7mqtB6npfE,"And in one of them, you keep all the incoming edges and the other one contains all the outgoing edges.
Does that transformation make sense intuitively? So what do you have here? So here you had-- let's say this graph had a Hamiltonian cycle.
So this graph had some cycle which went up here, did something, something, and came back.
So it would go like this.
It would do the cycle and come back.
So there was some cycle.
And since the cycle, it contains V, so now what you're doing is you're splitting apart V and disconnecting them.
So you'll still have-- if you look at the original path, it's still there, but it's been split up into a path now.
It's no longer a cycle.
Make sense? So now let's argue this more rigorously.
So what we want to say here is that let's say there was a cycle here.
If there was a cycle here, then is it clear that there is a path here? Because just take the same edges that you had before.
If you take the same edges, they will now form a path instead of a cycle.
So cycle implies path.
Does that makes sense? So the other way is a little more tricky.
So let's say you have a path.
So let's say you had a path.
So that means that-- let's redraw this so it's more clear.
So you have this new graph where you have two vertices, V dash and V double-dash.
This has a bunch of incoming edges and this is a bunch of outgoing edges.
So now let's say you have a Hamiltonian path in this graph.
So what does that mean? So where can the Hamiltonian path start? Can it start anywhere? Where can it start? AUDIENCE: V double-dash.
AMARTYA SHANKHA BISWAS: Right, because V double point doesn't have any incoming edges, so it can't be in the middle of the path.
So it has a start here.
So it starts.
It does something in there.
And where can it end? It can only end, similarly, in V dash.
Because V dash doesn't have any outgoing edges, so it can't be in the middle of the path.
So it has to end in V dash.
So now, if you have a path like that, and you go back to this graph-- so V dash and V double-dash are now on the same vertex.",11:48 its confusing cycle ->path is absolutely correct. but path->cycle is incorrect. http://www4.ncsu.edu/~uzgeorge/HamiltonCircuits7-19and22.pdf pg 6/89
G7mqtB6npfE,"We can use B to compute A, so then A must be easy.
But since we know that A is hard, that there's something wrong in our logic, so B must be hard.
Does that makes sense? Yes? Sort of? So let's move onto an actual problem.
So the first problem we're going to reduce is the Hamiltonian path.
So a well-known NP hard problem is the Hamiltonian cycle.
So here our A is-- so it's a Hamiltonian cycle.
So what's a Hamiltonian cycle? So what's a Hamiltonian cycle? So a Hamiltonian cycle-- so let's say you have a graph.
So we have this graph.
Let me draw this out.
That's it.
So a Hamiltonian cycle is a cycle in the graph which starts at some vertex, visits all the other vertices, and comes back to the starting vertex.
So in this case, we could do something like go here, and then take this vertex, take this vertex, take this vertex, and come back here.
So that is a valid Hamiltonian cycle.
So this graph is a Hamiltonian cycle.
So the decision problem is here that given the graph, does it have a Hamiltonian cycle? And that problem is NP-hard, so you can [INAUDIBLE] polynomial [INAUDIBLE].
So now the new polynomial shows NP-hard, which is B is Hamiltonian path.
So the Hamiltonian path is a very similar problem.
Instead of a cycle, you remove the requirement that you have to come back to the starting point.
You can just start anywhere and [INAUDIBLE] all the vertices and stop.
So for example, if you remove this edge, this graph no longer has Hamiltonian cycle, but it has a Hamiltonian path, which is just this line.
Simple.
So this is a simple reduction because the problems are very similar.
So the first step is, of course, showing that Hamiltonian path is an NP.
So that should be pretty clear because-- so what is our certificate here? So if someone says, OK, I have solved the Hamiltonian path and this is my Hamiltonian path.",5:40 how? Aren't we supposed to not repeat any vertex in Hamaltonion cycle?
GqmQg-cszw4,"In particular, lab one is going to rely on a lot of subtle details of C and Assembly code that we don't really teach in other classes here in as much detail.
So it's probably a good idea to start early.
And we'll try to get the TAs to hold office hours next week where we'll do some sort of a tutorial session where we can help you get started with understanding what a binary program looks like, how to disassemble it, how to figure out what's on the stack, and so on.
All right.
And I guess the one other thing, we're actually videotaping lectures this year.
So you might be able to watch these online.","Where can I learn ""what a binary program looks like, how to disassemble it, how to figure out whats on stack..."" (as mentioned in 4:20)? Also I wanted to know if it is possible to access the data stored in memory of one program using another program."
GqmQg-cszw4,"The system works.
But if I want to say that no one other than the TAs can access the grades file, this is a much harder problem to solve, because now I have to figure out what could all these non TA people in the world to try to get my grades file, right? They could try to just open it and read it.
Maybe my file system will disallow it.
But they might try all kinds of other attacks, like guessing the password for the TAs or stealing the TAs laptops or breaking into the room or who knows, right? This is all stuff that we have to really put into our threat model.
Probably for this class, I'm not that concerned about the grades file to worry about these guys' laptops being stolen from their dorm room.
Although maybe I should be.
I don't know.
It's hard to tell, right? And as a result, this security game is often not so clear cut as to what the right set of assumptions to make is.
And it's only after the fact that you often realize, well should have thought of that.
All right.","This video is a waste of time*, but let me help you waste a little less of your time with timestamps: fluff 5:42 - What is Security? more fluff 15:35 - Policy example of a company that got hacked example of a company that got hacked 22:30 - Threat Models example of a company that got hacked example of a company that got hacked 29:44 - Mechanisms example of a company that got hacked example of a company that got hacked example of a company that got hacked *This video is falsely titled because it's not about threat modeling. There's no threat modeling process presented in the video at all, no mention of STRIDE or DREAD or anything you hope to learn, just a quick verbal summary. Instead the instructor prefers to give tons of examples of hacks at big company and giggle at them. The video spends way more time on buffer overflow than threat modeling. Can't believe people are paying a fortune for this at MIT.
22:38 how threat models go wrong?
Is there a more detailed explanation of that atoi conversion that writes 0 1:01:00 ?"
GqmQg-cszw4,"We'll post them as soon as we get them ourselves from the video people.
And the last bit of administrivia is you should, if you have questions online, we're using Piazza, so I'm sure you've used this in other classes.
All right.
So before we dive into security, I need to tell you one thing.
There is a sort of rules that MIT has for accessing MIT's network when you're, especially, doing security research or playing with security problems, you should be aware that not everything you can technically do is legal.
And there's many things that you will learn in this class that are technically possible.
We'll understand how systems can be broken or compromised.
Doesn't mean you should go out and do this everywhere.
And there's this link in the lecture notes we'll post that has some rules that are good guidelines.
But in general, if you're in doubt, ask one of the lecturers or a TA as to what you should do.
And hopefully it's not too puzzling, what's going on.
All right.
So any questions about all this administrivia before we dive in? Feel free to ask questions.
OK.
So what is security? So we'll start with some basic stuff today.
And we'll look at just some general examples of why security is hard and what it means to try to build a secure system.
Because there's not really a paper, this will not have sort of deep intellectual content, maybe, but it'll give you some background and context for how to think about secure systems.","This video is a waste of time*, but let me help you waste a little less of your time with timestamps: fluff 5:42 - What is Security? more fluff 15:35 - Policy example of a company that got hacked example of a company that got hacked 22:30 - Threat Models example of a company that got hacked example of a company that got hacked 29:44 - Mechanisms example of a company that got hacked example of a company that got hacked example of a company that got hacked *This video is falsely titled because it's not about threat modeling. There's no threat modeling process presented in the video at all, no mention of STRIDE or DREAD or anything you hope to learn, just a quick verbal summary. Instead the instructor prefers to give tons of examples of hacks at big company and giggle at them. The video spends way more time on buffer overflow than threat modeling. Can't believe people are paying a fortune for this at MIT.
5:50 .... ""Secunty""?"
HpaHTfY52RQ,"Okay? So in reinforcement learning, you're always following some policy to get around the world right? Um, and that's generally called the exploration-policy or the control policy um, and then there's usually some other thing that you're trying to estimate, usually the- the value of a particular policy and that policy could be the same or it could be different.
So On-policy means that, uh, we're estimating the value of the policy that we're following, the data-generating policy.
Off-policy means that we're not.
Okay? So um, so in particular is, uh, model-free Monte Carlo, um, On-policy or Off-policy? It's On-policy because I'm estimating Q-pi not Q-opt.
Okay? That's On-policy.
Um, and Off-policy , uh, what about model-based Monte Carlo? [NOISE] I mean it's a little bit of a slightly weird question, but in model-based Monte Carlo, we're following some policy, maybe even a random policy, but we're estimating the transition then rewards, and from that we can compute the- the optimal policy.","I think there may be a typo at 28:27, it states that the Qpi is (4+8+16)/3 however I believe it should be (4+8+12)/3? Please correct me if I am wrong"
IM9ANAbufYM,"There are other methods that you should be able to figure out right now.
Even if you don't know class activation maps.
[NOISE] So to sum it up.
[NOISE] We have an image, [NOISE] input image, [NOISE] put it in your new network that is a binary classifier.
[NOISE] And the network says one.
You wanna figure out why the network says one, based on which pixels, what do you do? Visualize the weights.
[NOISE] Visualize the weights.
Uh, what do you visualize in the weights? The edges.
So I think visualizing the weights, uh, is not related to the input.
The weights are not gonna change based on the input.
So here you wanna know why this input led to one.","1:17:39 he says visualize the weights, which infact is true. Because dY/dX actually gives weight transpose (same shape as input image), so what we are actually doing is visualising the weight. what am I missing here?"
IPSaG9RRc-k,"Can someone tell me what this is, asymptotically? Yeah? STUDENT: n cubed-- JASON KU: n cubed-- why is that? Well, if we plug this stuff into that definition here, we have n factorial over 3 factorial n minus 3 factorial.
n factorial over n minus 3 factorial just leaves us with an n, an n minus 1, and an n minus 2 over 6.
And if you multiply all that out, the leading term is an n cubed, so this thing is asymptotically n cubed.
I skipped some steps, but hopefully you could follow that.
And then the last thing to remain is this one right there.
That one's a little tricky.
Anyone want to help me out here? What we can do is we can stick it into this formula, and then apply Sterling's approximation to replace the factorials.
That makes sense? OK, so what I'm going to do is-- let's do this in two steps.
This is going to be n factorial over-- what is this? n/2 factorial-- and then what is n minus n over 2? That's also n/2.
So this is going to be n/2 factorial squared.
Is that OK? Yeah? Now let's replace this stuff with Sterling's approximation and see if we can simplify.
So on the top, we have 2 pi n n/e to the n over-- and then we've got a square here, pi n.
I cancelled the 2-- n/2 over e to the n/2.
Did I do that right? OK.
I can't spell, and a lot of times, I make arithmetic errors, so catch me if I am doing one.
OK, so let's simplify this bottom here.
I'm not going to rewrite the top.
The bottom here-- we square this guy.
It's the pi times n.
And then this guy, n/2 squared-- that just stays is an n.
Then we have n/2 to the n/e-- something like that.
n/2 over e to the n-- that makes more-- me happier.
OK, so now we have this over this.
How do we simplify? Well, we can cancel out one of the root n's.
So we've got square root of pi n down here and square root of 2 up top.
And then what have we got? We've got n to the n down here and n to the n down-- up there, so those cancel.","why (n 3) = (n^3)? Can someone explain me this moment please? On 16:20
How does he get n+1 n and n-2 over 6 16:05, I get how that will give us n cubed just how he got that from that I don't follow"
IPSaG9RRc-k,"I'm remembering that.
Now, I don't care about what's stored in x.next, because I've stored it locally.
That makes sense.
All right, so now I am free to relink that next pointer to my previous guy.
And now I can essentially shift my perspective over, so the thing that I'm going to relink now is the next one.
So x previous and x now equals x, x_next.
Does that make sense? Just relinked things over-- so that's the end of step 2.
Now, as I got down this at the end of this for loop, where is x? What is x_p, x, and x_next-- or x_n? Really, I'm only keeping track of x and x_p here.
So what are x_p and x at the end of this loop? I've done this n times.
I started with b at x.
So what is x? Yeah? So we have a vote that x is c.
STUDENT: [INAUDIBLE] JASON KU: So this is a little interesting.
All right.
I will tell you that c is either x_p, x, or x_n.
So we have one vote for x.
Who says something else? Eric doesn't like x.
There are only two other choices.",Can someone pls explain line 11 a1 1:23:13? Thanks
IPSaG9RRc-k,"OK, so that's the first thing.
Otherwise, what do we do? We shift one thing over and then we make a recursive call.
Does that make sense? OK.
So we'll delete the first thing as a temporary variable-- delete first.
And then we'll insert last, x.
And then we need to do the recursive call.
So what's a recursive call look like? Yeah? STUDENT: Shift_left D, K minus 1-- JASON KU: Yeah.
So shift_left D, K minus 1-- OK? And then we can return.
This thing doesn't need to return anything.
It's just doing stuff to the thing.
Right? And whenever we get this K, we make a call, that gets down to 0, we will terminate because we will return.
We're in this range somewhere between we-- have an input after this line.
We know that K is somewhere between 1 and n minus 1.
And what we'll do is, every time through this recursion, we will subtract 1 from K.
So this is a nice, well-ordered sequence.
We do the correct thing obviously in the base case, and as long as this thing was correct for a smaller value of K, this thing also does the correct thing, because we're shifting over one, as we are asked, and we're letting this do the work of the rest.
I don't have to think about that.
I just have to think about this one loop, this one part of the thing that I'm doing.
Constant amount of work is done in this section.
And how many times do I call a function? STUDENT: [INAUDIBLE] JASON KU: Yeah.",Can anyone explain to me how the first element of a data structure gonna be the last element after applying the algorithm on 00:44:00?
Ih0cPR745fM,"Clearly, by the pigeonhole principle if nobody goes to 0, there must be two guys that collide.
So this problem is also total by the pigeonhole principal.
So it always has a solution, no matter what the circuit is.
And the class PPP is all problems in NP that are reducible to this problem.
Finally, the hierarchy of problems I defined is this.
P, FNP, there is total FNP somewhere here, which I don't show.
And these are the relationships of these problems.
I haven't shown that these arrows true, that this is a subclass, PPD's a subclass of these two subclasses.
This is easy.
This is a simple exercise we can think about.
This is basically my introduction to PPA, PPAD and related classes.
The final thing that I want to point out is answering a question that was asked after the previous lecture, which was why did you define these classes, and not just a TFNP complete problem.
Why did you have to pay special attention to precise existence argument that gives rise to the guarantee that your problems are total? And the reason for that is that actually TFNP is not what's called a syntactic class.
In other words, if I give you a problem with TFNP-- if I give you a Turing machine, you cannot decide whether that is computing a total problem, that no matter what the input to that machine is, there's always an output.
So I had to pay attention to the specialized existence arguments because for specialized existence arguments, I know a priori that the problem is total.
So in particular, no matter what circuit I give you here, I don't even have to check anything.
No matter what input you give me, I know there is a solution.
No matter what pairs of circuits you give me here, I don't need to check anything.
I know the answer to this problem-- there's always an answer to this problem, and so on, so forth.
No matter what you give to me as input, it is important to define complexity classes for which you can show hardness results to find complete problems.
Otherwise, you would have what is called promise classes, which are not amenable to showing the completeness results.","in 1:20:16, does the hierarchy imply that P is a subclass of PPAD? Is that true or the P should be FP?"
IiD3YZkkCmE,"But they're also codified things, like if you see that the rheumatoid factor in a lab test was negative, then-- actually, I don't know why that's-- oh, no, that counts against-- OK.
And then various exclusions.
So these were the things selected by our regularized logistic regression algorithm.
And I showed you the results before.
So we were able to get a positive predictive value of about 0.94.
Yeah? AUDIENCE: In a the previous slide, you said standardized regression coefficients.
So why did you standardize? Maybe I got the words wrong.
Just on the previous slide, the-- PETER SZOLOVITS: I think-- so the regression coefficients in a logistic regression are typically just odds ratios, right? So they tell you whether something makes a diagnosis more or less likely.
And where does it say standardized? AUDIENCE: [INAUDIBLE].
PETER SZOLOVITS: Oh, regression standardized.
I don't know why it says standardized.
Do you know why it says standardized? KATHERINE LIAO: Couple of things.
One is, when you run an algorithm right on your data set, you can't port it using the same coefficients because it's going to be different for each one.
So we didn't want people to feel like they can just add it on.
The other thing, when you standardize it, is you can see the relative weight of each coefficient.","The correct answer to the question at 16:20: Why did you standardize the predictors? Is that you should always standardize your data before feeding them to a regularized regression model (mentioned at 16:03). This is because a regularized model will penalize large coefficients; and because you don't want the magnitude of the coefficient to depend on the scale of measurement of the predictor, you standardize them."
J8Eh7RqggsU,"Uh, this should be outside of the recurse object.
Yeah.
Glad you guys are paying attention.
Um, otherwise, yeah, it would do basically nothing.
Any other mistakes? [LAUGHTER] Yeah.
Um, there is also function decorators that like implement memoizing for you.
In this class, are you okay if we use that or would you rather us like make our own in this case? Um, you can use the deco- you can be fancy if you want.
Okay.
Um, yeah.
But- but I think this is, you know, pretty transparent.
Easy for learning purposes.
Okay.
So let's run this.
So now it runs instantaneously as opposed to- I actually don't know how long it would have taken otherwise.
Okay.
And sanity check for t is probably the right answer because there's four was the original answer and multiply by 10.",I didn't understand how the cache works. Can someone explain please? 1:14:47
JDW82csukhE,"Uh, and also, um, another interesting side note is that, uh, we fix the pre-training strategy and the pre-training different GNNs models, uh, use different GNNs models for pre-training.
And what we found out is that the most expressive GNN model, namely GIN, that we've learned in the lecture, um, um, benefits most from pre-training.
Uh, as you can see here, the gain of pre-trained model versus non-pre-trained model is the- is the largest, um, in terms of accuracy.
And- and the intuition here is that the expressive GNN model can learn to capture more domain knowledge than less expressive model, especially learned from large amounts of, uh, data during pre-training.
So to summarize of our GNNs, we've, uh, said- learned that the GNNs have important application in scientific domains like molecular property prediction or, um, protein function prediction, but, uh, those application domains present the challenges of label scarcity and out-of-distribution prediction, and we argue that pre-training is a promising, uh, framework to tackle both of the challenges.
However, we found that naive pre-training strategy of this, uh, supervised graph level pre-training gives sub-optimal performance and even leads to negative transfer.
And our strategy is to, um, effective strategy is to pre-train both node and graph embeddings, and we found this strategy leads to a significant performance gain on diverse downstream tasks.
Um, yeah, thank you for listening.
","18:30 Seems weird that the GAT performs the worst, and benefits the least from pretraining. Isn't it more expressive than GCN and GraphSAGE in theory?"
KLBCUx1is2c,"Namely, we want to split up our problem into multiple subproblems, and if your input is a sequence-- that's the main case we've seen so far-- like the bowling problem, for example, then the natural subproblems to try are prefixes, suffixes, or substrings.
Prefixes and suffixes are nice, because there's few of them.
There's only a linear number of them.
In general, we want a polynomial number.
Sometimes you can get away with one of these.
They're usually about the same.
Sometimes you need substrings.
There's quadratically many of those.
Then, once you set up the subproblems, which-- it's easy to set up some problems, but hard to do it right-- to test whether you did it right is, can I write a recurrence relation that relates one subproblem solution to smaller subproblems solutions? And the general trick for doing this is to identify some feature of the solution you're looking for.","at 1:32 there is a typo on the board, suffixes of course should start at i and continue to the end, in previous lecture it was written down correctly https://youtu.be/r4-cftqTcdI?t=2624"
KlQiwkhLBg0,"But in the problem, in the homework problem, they were sneaky.
And they only said that f of T is big O of something.
So remember, what's the difference between big O and big theta? Well, intuitively, big theta says that my function really does look like this guy.
Somehow, it's bounded above and below as I go far enough out.
In big O, there's just a bound above, right? So this is somehow looser.
And so the way to apply master theorem in this case is say, well, at least f of n is upper bounded.
It looks like this.
So the best that I can do is to replace this guy also with an upper bound.
Yeah.","51:15 I think it should be 2^m-1 < k  2^m If 2^m-1  k then it wouldn't go to 2^m Correct me if I am wrong
48:30 This question about finding planets with the index K. Provided that the key indices are not duplicate can't we simply take left as 1 and right as K. since we know that the K key will never to at an index greater than K.
At 1:18:47, wouldn't the last house also be special because is is a house that has no easterly neighbor? Therefore, the sequence actually have two special houses instead of all but one. Maybe my understanding to all but one is wrong."
KsHOdr5UYZ0,"If you have tasks that are complex or require a lot of thinking and processing data, those are problems where AI can help.
If you have a problem with drug overdoses or with suicides, AI is probably not going to help.
And in fact, I've heard of cases where people collect data, and they do all of this quantitative analysis, but this is fundamentally a human problem.
And so it can be actually quite distracting.
So my point is simply that, at its best, AI can make the Air Force more efficient, more effective.
At its worst, it can distract us from actually solving difficult, challenging human and cultural problems.
So it's really important to be discerning about what kind of problem you're trying to solve and where AI can help.
But at this point, I hope, and I think that you have the tools to prevent that from happening, and to harness this very powerful new technology in support of building a more effective Air Force and pursuing America's interests.
So thank you all so much.
I'm going to stick around for questions, and I will leave you with this conclusion slide.
","39:30 ""If you have a problem with drug overdoses or with suicides AI is probably not gonna help""- I strongly disagree. Can someone tell me why he says that AI can't solve human problems? I mean I there is already proof that proves AI can help on those types of problems. I really don't know why he would say that."
KvtLWgCTwn4,"Sure enough, a minus b is a multiple of n.
And that takes care of that one.
The only if direction now goes from left to right.
So in the converse, I'm going to assume that n divides a minus b, where a and b are expressed in this form by the division algorithm or division theorem.
So if n divides a minus b, looking at a minus b in that form what we're seeing is that n divides this qa minus qb times n, plus the difference of the remainders.
That's what I get just by subtracting a and b.
But if you look at this n divides that term, the quotient times n.
And it therefore has to divide the other term as well.
Because the only way that n can divide a sum, when it divides one of the sum ands, is if it divides the other sum and.
So n divides ra minus the remainder of 8 divided by n from b divided by n.
But remember, these are remainders.
So that means that they're both in the interval from 0 to n minus 1 inclusive.
And the distance between them has got to be less than 1.
So if n divides a number that's between 0 and n minus 1, that number has to be 0.
Because it's the only number that n divides in there.","At 5:28, Meyer probably meant to say ""The distance between them has got to be less than n."" The second summand is the remainder (""distance"" or ""difference""). The remainder of any number divided by n is never larger than n. Since 1) This remainder must be divisible by n, and 2) This remainder is a number from 0 to (n-1) inclusive, The remainder is 0. Hence, rem(a, n) = rem(b, n)"
KzH1ovd4Ots,"So you take two vectors and construct a matrix out of them.
All right, by- um, pick the- pick the ith, um, element from this vector, jth element from this vector, multiply them, and that becomes the ij element of- of- of the matrix, right? So this- this, uh, for the outer product, you don't need the vectors to have the same dimension.
And a matrix that you construct from one row vector and one column vector is also called a rank one matrix, right? Why is it called rank one? Because one way to think of it is it is made of one pair of a row and a column vector, right? So that makes it, um, um, what's, uh, what's also called as a- a rank one matrix.",I think at 46:40 the explanation for a rank 1 matrix should be that the columns of the resulting matrix are just scaler multiples of the first column. Hence the dimension of the column space is 1.
KzH1ovd4Ots,"Uh, now, next we are going to limit ourselves to only square matrices, which means the input and output spaces have the same dimensions, and for the purpose of visualization, we're gonna only consider, let's say, a three-dimensional space.
Right? Now, in this diagram we have two different, um, um, um, two different pictures for the input space and the output space.
But now, because A is symmetric, I'm sorry, A is- is, um, a square matrix, we're gonna overlay the input and output space onto the same space, right? So here, the input space and output space are- are being overlaid here.
Now let's ask the question- let's, you know, A, you know, let's ask the question.
We saw what happened, uh, for, you know, pick, you know, choose some points, you know, run it through A, you get an output, uh, uh, output point.
Now, what happens if we take the unit sphere around the origin? By unit sphere, I mean, just to the points on the surface of the unit sphere.
Think of it as a soccer ball at the center on the origin.
And you take every point on the surface, run it through A, you get a corresponding output point for every, you know, input point of- of- of the soccer ball, right? How would the resulting shape look? Right? So that's gonna look as an ellipsoid.
It's- it's, you know, almost like an ellipse.
So that's gonna be- right? So what- what- what exactly happened here? We- it's- it's a three-dimensional input and output space.
We started with the input as- we didn't have one input, but we had a collection of points as inputs.
And that collection was precisely those points that live on the surface of a unit sphere.
And let's say we- we took this point in the input space, run it through A, we got a corresponding out point, and that point, say, this one.
Right? So every point on the input surface maps to some point on the output surface of that shape, right? We could have done this with any input shape, but, you know, sphere is easy to kind of analyze, right? Now, similarly, um, you do it for another point, um, let's say this point, and let's say that maps here, and let's say we pick- what color do we have? Green- green, and let's say we pick this point and that maps here.
Okay? Now, we saw what- what- what- what happens when you- when we, uh, take some shape, for example, a sphere and run it through A, instead of thinking of um, um, running a point through A.","Great Lecture. One question in last part 1:44:30, why we are getting only ellipsoid output for sphere shape input, not any other shape? Does it have to do something with the assumption of A being symmetric?"
L5uBeAGJV1k,"And if you want to skip the short discussion of the meta theorems, that's fine, because it's never going to come up again in this class.
So let's look at this phrase in English, where the poet says, ""all that glitters is not gold."" Well, a literal translation of that would be that, if we let G be glitters, and I can't use G again, so we'll say Au is gold, then this translated literally would say for every x, G of x, if x is gold implies that not gold of x.
So is that a sensible translation? Well, it's clearly false, because gold glitters like gold.
And you can't say that gold is not gold.
So this is not what's meant.
It's not a good translation.
It doesn't make sense.
Well, what is meant, well, when the poet says, ""all that glitters is not gold,"" he's really leaving out a key word to be understood from context.
All that glitters is not necessarily gold.
He was using poetic license.
You're supposed to fill in and understand its meaning.
And the proper translation would be that it is not true that everything that glitters is gold.
It is not the case that for all x, if x glitters, then x is gold.
So it's just an example where a literal translation without thinking about what the sentence means and what the poet who articulated this sentence intended will get you something that's nonsense.
It's one of the problems with machine translation from natural language into precise formal language.","At 2:46, is it equivalent to say ""There exists an x such that (G(x) AND NOT Au(x))""? I think the logic is essentially the same, but I wonder if it's subtly different in that it may preclude the possibility of an empty set?"
MEz1J9wY2iM,"And so we're going to have two phases here in this particular approximation scheme.
The first phase is find an optimal partition, A prime, B prime, of S1 through Sm.
And we're just going to assume that this exhaustive search, which looks at all possible subsets, and picks the best one.
OK? And how many subsets are there for a set of size m? It's 2 raised to m.
So this is going to be an exponential order, 2 raised to m algorithm.
OK? I'm just going to find the optimum partition through exhaustive search for m.
Right? m is less than n.
So I'm picking something that's a smaller problem.
I'm going to seed this.
So, the way this scheme works is, I'm seeding my actual algorithm-- my actual heuristic-- with an initial partial solution.","It feels like the prof made a mistake when explaining aprox partition. He first said (when he was building the algorithm at 1:03:52) m < n, and then when he was proving he said imagine that the second part never executed because m is large. However the only case in which m never executes is if m = n, since otherwise there will still be leftover elements that are not added to any partition."
MEz1J9wY2iM,"But you got a problem of size n.
And we're going to define an approximation ratio, row of n, for any input-- if for any input-- excuse me.
The algorithm produces a solution with cost C that satisfies this little property, which says that the max of C divided by Copt divided by-- oh, sorry-- and Copt divided by C is less than or equal to row n.
And the only reason you have two terms in here is because you haven't said whether it's a minimization problem or a maximization problem.
Right, so of it' a minimization problem, you don't want to be too much greater than the minimum.
If it's a maximization problem, you don't want to be too much smaller than the maximum.
And so you just stick those two things in there.
And you don't worry about whether it's min or max in terms of the objective function, and you want it to be a particular ratio.
OK? Now I did say row of n there.
So this could be a constant or it could be a function of no.
If it's a function of n, it's going to be an increasing function of n.
OK? Otherwise you could just bound it, and have a constant, obviously.
So, you might have something like-- and we'll see one of these-- log n approximation scheme, which says that you're going to be within logarithmic of the answer-- the minimum or maximum.
But if it's a million, then if you do log of base two, then you're within a factor of 20, which isn't that great.
But let's just say if you're happy with it, and if it goes to a billion, it's a factor 30, and so on and so forth.
Actually, it could grow.
So that's an algorithm.
If these terms are used interchangeably, we'll try and differentiate.
But we do have something that we call an approximation scheme.
And the big difference between approximation algorithms an approximation schemes is that I'm going to have a little knob in an approximation scheme that's going to let me do more work to get something better.","4:54 correction in caption ""rho of n"" instead of ""row of n"""
MEz1J9wY2iM,"I'm constantly taking stuff away.
When xk equals 0, I'm going to be done.
OK? And I'm going to move a little bit between discrete and continuous here.
It's all going to be fine.
But what I have is, if I just take that, I can turn this.
This is a recurrence.
I want to turn that into a series.
So I can say something like 1 minus 1 over t, raised to k, times n.
And this is the cardinality of x, which is the cardinality of x0.
So that's what I have up there.
And that's essentially what happens.
I constantly shrink as I go along.
And I have a constant rate of shrinkage here.
Which is the conservative part of it.
So keep that in mind.
But it doesn't matter from an analysis standpoint.
OK? So if you look at that, and you say, what happens here? Well, I can just say that this is less than or equal to e raised to minus-- you knew you were going to get an e, because you saw a natural algorithm here, right? And so, that's what we got.
And basically, that's it.
You can do a little bit of algebra.
I'll just write this out for you.
But I won't really explain it.
You're going to have Xk equals 0.
You're done.
The cost, of course, is k.
Right? The cost is k, because you've selected k subsets.
All right, so that's your cost, all right? So, when you get to the point, you're done.
And the cost is k.
So what you need is, you need to say that e raised to minus kt divided by n is strictly less than 1.
Because that is effectively when you have strictly less than 1 element.
It's discrete.
So that means you have zero elements left to cover.
That means you're done OK? So that's your condition for stopping.
So this done means that e raised to minus kt times n is strictly less than 1.","53:50, how do you get (1-1/t)^k <= e^(-k/t) ?"
MH4yvtgAR-4,"Basically here is the prediction of the label for a- for a- uh, for a given color whether it's toxic or not, this is whether it is truly toxic or not, um, and then the way you can think of this is y takes value one if it's toxic, and zero if it's not.
If the true value is zero, then this term is going to survive and it's basically one minus the lock predicted- prob- uh, one minus the predicted probability.
So here we want this to be the predicted probability- to be as small as possible so that one minus it becomes close to one because log of 1 is 0, so that this discrepancy is small.
And if the- if the, uh, class value is one, then this term is going to survive because 1 plus 1- 1 minus 1 is 0 and this- this goes away.
So here we want this term to be as close to one as possible.
Which again would say, if it's- uh, if it's toxic, we want the probability to be high.
If it's not toxic, we want the probability [NOISE] to be low- uh, the predicted probability, uh, of it being toxic.
And this is the cross, uh, entropy loss.
So this is the encoded input coming from node embeddings.
Uh, these are the classification rates, uh, for the final classification.
Uh and these are the, uh, node labels, is it basically toxic, uh, or not.",Shouldnt the loss function in 31:42 have a minus sign?
MH4yvtgAR-4,"Which means that perhaps, you know, for a given node we have some, uh, we have- we have some label about a node, maybe this is a drug-drug interaction network, and you know whether this drug is toxic, uh, or not, whether it's safe or toxic.
So we could then say, you know, given this neural network, predict at the end the label of the node.
Whether it's safe or toxic.
And now, we can backpropagate based on label.
So both- both are o- both are possible, either we directly train to predict the labels, or we can train based on the network similarity, where network similarity can be defined using random works, the same way as in, uh, node to work.
So for, uh, supervised training, uh, basically what we wanna do is, we wanna define, uh, the loss, uh, in terms of let's say classification, this is the cross entropy loss for a binary classification.","cross entropy loss in 30:15 should be minus the summation ( blah, blah, blah...)"
Mi8wnYc1m04,"Yeah, this over here should be small y.
Thank you.
Yeah, so you, um, um, so now we have, uh, something called the law of total expectation, right? So the law of total expectation tells us that, expectation of X can be written as the expectation of- expectation of X given Y, right? Now, this holds true for any X and any Y, right? Y could be completely independent of X, it could be dependent on X.
But the expectation of X can always be decomposed into this nested form where, um, you condition on Y, you get a new random variable, right? And you take the expectation of- of this random variable and you get back the expectation of X.","57:30, Is it true that E [X|Y] may or may not be normally distributed?"
MjbuarJ7SE0,"So I'm going to just temporarily hop out of the scope and see is there variable x outside of me? And it'll find this variable x here, and it's going to print out its values.
So that's OK.
This last example here is actually not allowed in Python-- similar to this one-- except that I'm trying to increment a value of x, but then I'm also trying to reassign it to the same value of x.
The problem with that is I never actually initialized x inside h.
So if I said-- if inside h, I said x is equal to 1, and then I did x plus equals to 1, then it would be this example here-- f of y.
But I didn't do that.
I just tried to access x and then incremented and then tried to reassign it.
And that's actually not allowed in Python.
There is a way around it using global variables.
But it's actually frowned upon to use global variables, though global variables are part of the readings for this lecture.
And the reason why it's not a great idea to use global variables is because global variables sort of give you this loophole around scopes, so it allows you to write code that can become very messy.","In the last example, where x is not defined in function g(x) should be giving an error (UnboundLocalError: local variable 'x' referenced before assignment ), isn't it? Like how the example shown in 32:42"
MjbuarJ7SE0,"So h returns None.
Back to whoever called it, which was this code inside g.
So that gets replaced with None-- the thing that I've-- this circled red h here.
As soon as h returns, we're going to get rid of that scope-- all the variables created within it-- and we're done with h.
So now we're back into g.
And we just finished executing this and this got replaced with None.
We're not printing it out, so this doesn't show up anywhere; it's just there.
So we're finished with that line.
And the next line is return x.
So x inside g is 4, so 4 gets returned back to whoever called it, which was in the global scope here.
So this gets replaced with 4.
So once we've returned x, we've completely exited out of the scope of g, and we've come back to whoever called us, which was global scope and we've replaced z is equal to g of x and that completely got replaced with 4-- the returned value.
So that's sort of showing nested functions.
All right just circling back to decomposition-abstraction.
This is the last slide.
You can see if you look at the code associated with today's lecture, there are some other examples where you can see just how powerful it is to use functions.
And you can write really clean and simple code if you define your own functions and then just use them later.","39:49 If the function h() had a return at the end, would the outer function have returned x as 'abc' instead of 4?"
MjbuarJ7SE0,"So inside func_b, y has the value 2, and I'm returning 2 back to whoever called me.
So this is the value 2 and I'm going to print 5 plus 2, which is 7.
Last one.
This is the trickiest.
Oop, that popped up.
If you think you've got it, try that exercise.
But otherwise follow along.
print func_c func_a.
So I see that I am going to enter func_c's scope.
So I'm going to look at what func_c does.
First thing I do is I'm mapping all the parameters.
Don't even worry about the fact that this is a function right now.
Just pretend it's x or something.
So you say func_a is going to get mapped to the variable z inside func_c.","29:03 ""so z is func_c"" should have been ""so z is func_a"""
MjbuarJ7SE0,"So using global variables, you can be inside a function and then modify a variable that's defined outside of your function.
And that sort of defeats the purpose of functions and using them in writing these coherent modules that are separate.
That said, it might sometimes be useful to use global variables, as you'll see in a couple lectures from now.
OK cool.
So let's go on to the last scope example.
OK this slide is here, and notice I've bolded, underlined, and italicized the Python Tutor, because I find it extremely helpful.
So the Python Tutor-- as I've mentioned in one of the assignments-- it was actually developed by a grad student here, or post-grad student slash post-doc here.",34:00 what is the difference between incrementing x and printing x? it is defined in the same place relative to g() and h()
N2lwsB1qfJw,"And I ask the question, how close is y hat_ i to y_i? And the performance metric is going to be the measure of how close y hat_i is to y_i.
And normally it's designed so that the smaller the metric, the better the prediction performance.
It's an error measure.
Some people call it prediction performance metric, the prediction error as a result.
So there are a few which are very commonly used.
The first one is the mean square error.
So this here, uh, is the two-norm.
Let me just mark it here.","Hi guys, in the slice of the time 2:30 demos different prediction metrcs. However a type of notation is confusing (may be a silly question): when prof said y_i or y hat_i, what is the subscript i? It seem an index symbol however which is supposed to put in the bottom right. What was shown is in the top right position of y. Much appreciate if any guy got an answer, thanks."
Nsc0Yluf2yc,"And the way we do that as I said is simply by taking the dark green representations of the output layer here and using them as inputs to subsequent blocks so they get attended to, and we proceed with the subsequent regularization and feed-forward steps just as before.
And when you work with these models in Hugging Face, if you ask for all of the hidden states, what you're getting is a grid of representations corresponding to these output blocks in green here.
And of course, just as a reminder I'm not indicating it here but there is actually multi-headed attention of each one of these blocks through each one of the layers.
So there are a lot of learned parameters in this model, especially if you have 12 or 24 attention heads.
At this point, I'm hoping that you can now fruitfully return to the original Vaswani, et al paper and look at their model diagram and get more out of it.
For me, it's kind of hyper compressed but now that we've done a deep dive into all the pieces, I think this serves as a kind of useful shorthand for how all the pieces fit together.","Is there a mistake at 09:50? ""You have 12 or 24 attention heads""; shouldn't that be 12 or 24 layers with as many attention heads as the length of tokens in the input / output sequence? Also, this is VERY well done lecture series! We will probably have our own NLU course at our university based on these materials! This is a huge service for the next generation of natural language related data scientists!"
Nu8YGneFCWE,"I want to generalize this to be a family of hash functions, which are this habk for some random choice of a, b in this larger range.
All right, this is a lot of notation here.
Essentially what this is saying is, I have a has family.
It's parameterized by the length of my hash function and some fixed large random prime that's bigger than u.
I'm going to pick some large prime number, and that's going to be fixed when I make the hash table.
And then, when I instantiate the hash table, I'm going to choose randomly one of these things by choosing a random a and a random b from this range.
Does that makes sense? AUDIENCE: [INAUDIBLE] JASON KU: This is a not equal to 0.
If I had 0 here, I lose the key information, and that's no good.",Universal hash function hash(k) = (((ak + b) mod p) mod m) at 41:31 How do we know the key value (k)? Can anyone explain?
Nu8YGneFCWE,"So basically, if I fix this location i, this is where this key goes.
Sorry.
This is the size of chain at h of Ki.
Sorry.
So I look at wherever Ki goes is hashed, and I see how many things collide with it.
I'm just summing over all of these things, because this is 1 if there's a collision and 0 if there's not.
Does that make sense? So this is the size of the chain at the index location mapped to by Ki.
So here's where your probability comes in.
What's the expected value of this chain length over my random choice? Expected value of choosing a hash function from this universal hash family of this chain length-- I can put in my definition here.
That's the expected value of the summation over j of xij.
What do I know about expectations and summations? If these variables are independent from each other-- AUDIENCE: [INAUDIBLE] JASON KU: Say what? AUDIENCE: [INAUDIBLE] JASON KU: Linearity of expectation-- basically, the expectation sum of these independent random variables is the same as the summation of their expectations.","48:50 As I learned in 6.042j 2010 from Prof. Leighton, Linearity of Expectation holds regardless of mutual independence property. Correct me if I misunderstood."
Nu8YGneFCWE,"So this is equal to the summation over j of the expectations of these individual ones.
One of these j's is the same as i.
j loops over all of the things from 0 to u minus 1.
One of them is i, so when xhi is hj, what is the expected value that they collide? 1-- so I'm going to refactor this as being this, where j does not equal i, plus 1.
Are people OK with that? Because if i equals-- if j and i are equal, they definitely collide.
They're the same key.
So I'm expected to have one guy there, which was the original key, xi.
But otherwise, we can use this universal property that says, if they're not equal and they collide-- which is exactly this case-- the probability that that happens is 1/m.
And since it's an indicator random variable, the expectation is there are outcomes times their probabilities-- so 1 times that probability plus 0 times 1 minus that probability, which is just 1/m.
So now we get the summation of 1/m for j not equal to i plus 1.
Oh, and this-- sorry.
I did this wrong.","Question: At 51:00 Jason says that chain length is constant if m is at least order n. Will 1+((n-1)/m) not be between 1 and 2 which is not constant. If m is massive and n is massive then 1+((n-1)/m) would be about 2. If massive and n is small, say m=10000000 and n=1, then 1+((n-1)/m) is about 1. I am not great at probability, which is probably why I don't understand. Thanks for any clarification.
At 50:51, shouldn't that be less than or equal to as probability 1/m is an upper bound to the probability of two keys having the same hash value?"
Nu8YGneFCWE,"So what I would need to do-- and if I was storing your keys as MIT IDs, I would need an array that has indices that span the tire space of nine-digit numbers.
That's like 10 to the-- 10 to the 9.
Thank you.
10 to the 9 is the size of a direct access road off to build to be able to use this technique to create a direct access array to search on your MIT IDs, when there's only really 300 of you in here.
So 300 or 400 is an n that's much smaller than the size of the numbers that I'm trying to store.
What I'm going to use as a variable to talk about the size of the numbers I'm storing-- I'm going to say u is the maximum size of any number that I'm storing.",Great video! Thank you MIT OCW :) 18:37...how is it 10^9? Can someone please explain?
Nu8YGneFCWE,"This isn't u.
This is n.
We're storing n keys.
OK, so now I'm looping over j-- this over all of those things.
How many things are there? n minus 1 things, right? So this should equal 1 plus n minus 1 over m.
So that's what universality gives us.
So as long as we choose m to be larger than n, or at least linear in n, then we're expected to have our chain lengths be constant, because this thing becomes a constant if m is at least order n.","51:20 shouldn't be n-2? instead n-1, the result of the sum, bc one of the n-1 elements is not counted and that's when j=i, can anyone explain? thanks!"
OgO1gpXSUzU,"A pretty small number.
But the probability of 26 consecutive reds when the previous 25 rolls were red is what? No, that.
AUDIENCE: Oh, I thought you meant it had been 26 times again.
JOHN GUTTAG: No, if you had 25 reds and then you spun the wheel once more, the probability of it having 26 reds is now 0.5, because these are independent events.
Unless of course the wheel is rigged, and we're assuming it's not.
People have a hard time accepting this, and I know it seems funny.
But I guarantee there will be some point in the next month or so when you will find yourself thinking this way, that something has to even out.","I feel like the slide at 22:00 is a good opportunity to introduce probability notation, since in English the second sentence sounds really misleading. The first sentence is P(26 consecutive reds). The second sentence is P(26 consecutive reds | the FIRST 25 are red). Strictly speaking the second sentence is grammatically incorrect, what the professor means is ""Probability of a single roll being red, given that the last 25 were red."" This makes it WAY easier to understand that rolls are not correlated. What is written on the slide makes it sound like there are 26+25 rolls taking place.
22:10 I understand (I think, I hope) that concept of independence, but then it is said we assume the wheel is not rigged. But is this assumption justified after 25 consecutive reds? Probably yes, things like this happen (and we don't forget them for decades), but on the other hand, at how many consecutive reds should we become suspicious?"
OgO1gpXSUzU,"All right, so we've got one flip, and it came up heads.
And now I can ask you the question-- if I were to flip the same coin an infinite number of times, how confident would you be about answering that all infinite flips would be heads? Or even if I were to flip it once more, how confident would you be that the next flip would be heads? And the answer is not very.
Well, suppose I flip the coin twice, and both times it came up heads.
And I'll ask you the same question-- do you think that the next flip is likely to be heads? Well, maybe you would be more inclined to say yes and having only seen one flip, but you wouldn't really jump to say, sure.
On the other hand, if I flipped it 100 times and all 100 flips came up heads, well, you might be suspicious that my coin only has a head on both sides, for example.
Or is weighted in some funny way that it mostly comes up heads.
And so a lot of people, maybe even me, if you said, I flipped it 100 times and it came up heads.
What do you think the next one will be? My best guess would be probably heads.
How about this one? So here I've simulated 100 flips, and we have 50 heads here, two heads here, And 48 tails.
And now if I said, do you think that the probability of the next flip coming up heads-- is it 52 out of 100? Well, if you had to guess, that should be the guess you make.
Based upon the available evidence, that's the best guess you should probably make.
You have no reason to believe it's a fair coin.","At 8:30 he misses implications of Bayes theorem - if you observe 52 heads from 100 flips, it is still much more likely that the coin is fair than biased. Because as he mentions, there are many many more fair coins and dice our there than weighted ones. The probably you have to assess is P(52 heads | coin is fair) * P(coin is fair) vs P(52 heads | coin is biased) * P(coin is biased). Far more likely that it is fair."
OgO1gpXSUzU,"And that gets us to what's in some sense the fundamental question of all computational statistics, is how many samples do we need to look at before we can have real, justifiable confidence in our answer? As we've just seen-- not just, a few minutes ago-- with the coins, our intuition tells us that it depends upon the variability in the underlying possibilities.
So let's look at that more carefully.
We have to look at the variation in the data.
So let's look at first something called variance.
So this is variance of x.
Think of x as just a list of data examples, data items.
And the variance is we first compute the average of value, that's mu.
So mu is for the mean.
For each little x and big X, we compare the difference of that and the mean.",32:31 what is the symbol | X | is it the same as the symbol n?
OgO1gpXSUzU,"And they see this confidence interval and say, what does that really mean? Most people don't know.
But it does have a very precise meaning, and this is it.
How do we compute confidence intervals? Most of the time we compute them using something called the empirical rule.
Under some assumptions, which I'll get to a little bit later, the empirical rule says that if I take the data, find the mean, compute the standard deviation as we've just seen, 68% of the data will be within one standard deviation in front of or behind the mean.
Within one standard deviation of the mean.
95% will be within 1.96 standard deviations.
And that's what people usually use.
Usually when people talk about confidence intervals, they're talking about the 95% confidence interval.
And they use this 1.6 number.
And 99.7% of the data will be within three standard deviations.
So you can see if you are outside the third standard deviation, you are a pretty rare bird, for better or worse depending upon which side.
All right, so let's apply the empirical rule to our roulette game.
So I've got my three roulette games as before.
I'm going to run a simple simulation.
And the key thing to notice is really this print statement here.","14:32 Either his understanding of the FairRoulette script is not good, or he is misspeaking. totPocket is the amount won, not the number you get right, betPocket does not return 0, it returns the negative bet playRoulette is missing the final parameter ""toPrint"" in the definition on the slide playRoulette's expected return does not correctly calculate because it does not use bet size 40:45 code is incomplete and looks wrong, too. :("
OgO1gpXSUzU,"But you wouldn't bet that it would have fewer than 5.
Because of this, if you now look at the average of the 20 spins, it will be closer to the mean of 50% reds than you got from the extreme first spins.
So that's why it's called regression to the mean.
The more samples you take, the more likely you'll get to the mean.
Yes? AUDIENCE: So, roulette wheel spins are supposed to be independent.
JOHN GUTTAG: Yes.
AUDIENCE: So it seems like the second 10-- JOHN GUTTAG: Pardon? AUDIENCE: It seems like the second 10 times that you spin it.
Like that shouldn't have to [INAUDIBLE]..
JOHN GUTTAG: Has nothing to do with the first one.
AUDIENCE: But you said it's likely [INAUDIBLE]..
JOHN GUTTAG: Right, because you have an extreme event, which was unlikely.
And now if you have another event, it's likely to be closer to the average than the extreme was to the average.
Precisely because it is independent.
That makes sense to everybody? Yeah? AUDIENCE: Isn't that the same as the gambler's fallacy, then? By saying that, because this was super unlikely, the next one [INAUDIBLE].
JOHN GUTTAG: No, the gambler's fallacy here-- and it's a good question, and indeed people often do get these things confused.
The gambler's fallacy would say that the second 10 spins would-- we would expect to have fewer than 5 reds, because you're trying to even out the unusual number of reds in the first Spin Whereas here we're not saying we would have fewer than 5.",what is he throwing at 26:22 and why? I noticed this already in the previous video... what is it it about?
OgO1gpXSUzU,"Here it says, ""in repeated independent tests with the same actual probability, the chance that the fraction of times the outcome differs from p converges to 0 as the number of trials goes to infinity."" So this says if I were to spin this fair roulette wheel an infinite number of times, the expected-- the return would be 0.
The real true probability from the mathematics.
Well, infinite is a lot, but a million is getting closer to infinite.
And what this says is the closer I get to infinite, the closer it will be to the true probability.
So that's why we did better with a million than with a hundred.
And if I did a 100 million, we'd do way better than I did with a million.
I want to take a minute to talk about a way this law is often misunderstood.
This is something called the gambler's fallacy.
And all you have to do is say, let's go watch a sporting event.
And you'll watch a batter strike out for the sixth consecutive time.","Prof. Guttag is a good teacher but generally misleading. Around 20:00, while describing the Gambler's fallacy, Prof Guttag falls for the Ludic Fallacy. https://www.logicallyfallacious.com/logicalfallacies/Ludic-Fallacy"
OgO1gpXSUzU,"I did so badly on the midterm, I will have to do better on the final.
That was mean, I'm sorry.
All right, speaking of means-- see? Professor Grimson not the only one who can make bad jokes.
There is something-- it's not the gambler's fallacy-- that's often confused with it, and that's called regression to the mean.
This term was coined in 1885 by Francis Galton in a paper, of which I've shown you a page from it here.
And the basic conclusion here was-- what this table says is if somebody's parents are both taller than average, it's likely that the child will be smaller than the parents.
Conversely, if the parents are shorter than average, it's likely that the child will be taller than average.
Now you can think about this in terms of genetics and stuff.
That's not what he did.
He just looked at a bunch of data, and the data actually supported this.
And this led him to this notion of regression to the mean.
And here's what it is, and here's the way in which it is subtly different from the gambler's fallacy.
What he said here is, following an extreme event-- parents being unusually tall-- the next random event is likely to be less extreme.
He didn't know much about genetics, and he kind of assumed the height of people were random.
But we'll ignore that.","Correction: at 23:36, professor meant to say taller, than THEIR PARENTS (not shouting) :D
small error at 23:10 ""If the parents are taller than average, it's likely that the child will be smaller than the parents"" -- CORRECT ""Conversely, if the parents are shorter than average, it is likely that the child will be taller than average"" -- INCORRECT That 2nd phrase should have been: ""Conversely, if the parents are shorter than average, it is likely that the child will be taller than its parents"".
Around 23:35 he says ""taller than average"" when he should have said ""taller than the parents"" (and shorter than average).
For those that may be confused, he misspoke at 23:36 ""taller than average"" should have been ""taller than the parents"". In the case that parents are shorter than average, it is expected that their children will be taller than them, not taller than average.
23:33 this should be corrected to --> if the parents are shorter than average, the children are likely to be taller than the parents ( not taller than average).
Great professor! A slight hiccup on 23:38; I believe he meant to say if the parents are both shorter than average it is likely that the child will be taller than their parents (not average).
at 23:37 shouldn't it be ""taller than parents"" instead of ""taller than average""?
At 23:37, he said ""it's likely that the child will be taller than average,"" but he should have said (and probably meant to say) ""it's likely that the child will be taller than the parents "". EDIT: Oops, I just noticed muhammed ali already made this point."
OgO1gpXSUzU,"OK, but the idea is here that it will be less extreme.
So let's look at it in roulette.
If I spin a fair roulette wheel 10 times and get 10 reds, that's an extreme event.
Right, here's a probability of basically 1.1024.
Now the gambler's fallacy says, if I were to spin it another 10 times, it would need to even out.
As in I should get more blacks than you would usually get to make up for these excess reds.
What regression to the mean says is different.
It says, it's likely that in the next 10 spins, you will get fewer than 10 reds.
You will get a less extreme event.
Now it doesn't have to be 10.
If I'd gotten 7 reds instead of 5, you'd consider that extreme, and you would bet that the next 10 would have fewer than 7.","The slide at 25:05 is wrong ! For a system without memory (like a roulette), the past has NO EFFECT on future events. Therefore, the probability of any event remains the same, even after the occurrence of an extreme event. That means ; after an extreme event the system is exactly in the same state as it was when we started the game. After a sequence o 10 reds, the probability of getting a red at the next trial is just 18 ou of 37. Some people lost a lot of money in Monte-Carlo the day ""red"" turned up 26 times in a row. When doing Monte-Carlo simulations be careful of so-called ""cyclic"" random number generators. From a mathematical point of view, be aware that variance on the estimated mean value tends to zero as the number of trials increases, but the variance on the number of events does not. Check any good book on probability."
OgO1gpXSUzU,"So here is play roulette.
I've made game the class a parameter, because later we'll look at other kinds of roulette games.
You tell it how many spins.
What pocket you want to bet on.
For simplicity, I'm going to bet on this same pocket all the time.
Pick your favorite lucky number and how much you want to bet, and then we'll have a simulation just like the ones we've already looked at.
So the number you get right starts at 0.
For I and range number of spins, we'll do a spin.
And then tote pocket plus equal game dot that pocket.
And it will come back either 0 if you've lost, or 35 if you've won.
And then we'll just print the results.","Good lecture overall but there is a bug in the code at 14:32 and 15:25 -- playRoulette should instead print 100 * totPocket / (numSpins * bet). The output in his example only looks correct because `bet` is 1. If `bet` were 2 and `numSpins` were 1, it either prints ""-200%"" or ""7200%"" (obviously you can't lose more than 100% or win more than 3600%).
14:32 Either his understanding of the FairRoulette script is not good, or he is misspeaking. totPocket is the amount won, not the number you get right, betPocket does not return 0, it returns the negative bet playRoulette is missing the final parameter ""toPrint"" in the definition on the slide playRoulette's expected return does not correctly calculate because it does not use bet size 40:45 code is incomplete and looks wrong, too. :("
OgO1gpXSUzU,"So we can do it.
In fact, let's run it.
So here it is.
I guess I'm doing a million games here, so quite a few.
Actually I'm going to do two.
What happens when you spin it 100 times? What happens when you spin it a million times? And we'll see what we get.
So what we see here is that we do 100 spins.
The first time I did it my expected return was minus 100%.
I lost everything I bet.
Not so unlikely, given that the odds are pretty long that you could do 100 times without winning.
Next time I did a 100, my return was a positive 44%, and then a positive 28%.
So you can see, for 100 spins it's highly variable what the expected return is.
That's one of the things that makes gambling attractive to people.
If you go to a casino, 100 spins would be a pretty long night at the table.
And maybe you'd won 44%, and you'd feel pretty good about it.
What about a million spins? Well people aren't interested in that, but the casino is, right? They don't really care what happens with 100 spins.
They care what happens with a million spins.
What happens when everybody comes every night to play.
And there what we see is-- you'll notice much less variance.
Happens to be minus 0.04 plus 0.6 plus 0.79.
So it's still not 0, but it's certainly, these are all closer to 0 than any of these are.
We know it should be 0, but it doesn't happen to be in these examples.","Good lecture overall but there is a bug in the code at 14:32 and 15:25 -- playRoulette should instead print 100 * totPocket / (numSpins * bet). The output in his example only looks correct because `bet` is 1. If `bet` were 2 and `numSpins` were 1, it either prints ""-200%"" or ""7200%"" (obviously you can't lose more than 100% or win more than 3600%).
16:17... ""a positive 28 percent"". Better check again. I see a negative 28 percent."
OgO1gpXSUzU,"The next time they come to the plate, the idiot announcer says, well he struck out six times in a row.
He's due for a hit this time, because he's usually a pretty good hitter.
Well that's nonsense.
It says, people somehow believe that if deviations from expected occur, they'll be evened out in the future.
And we'll see something similar to this that is true, but this is not true.
And there is a great story about it.
This is told in a book by Huff and Geis.
And this truly happened in Monte Carlo, with Roulette.
And you could either bet on black or red.
Black came up 26 times in a row.
Highly unlikely, right? 2 to the 26th is a giant number.
And what happened is, word got out on the casino floor that black had kept coming up way too often.
And people more or less panicked to rush to the table to bet on red, saying, well it can't keep coming up black.
Surely the next one will be red.
And as it happened when the casino totaled up its winnings, it was a record night for the casino.
Millions of francs got bet, because people were sure it would have to even out.
Well if we think about it, probability of 26 consecutive reds is that."," I don't understand why, isn't the whole purpose of ""Law of Large Numbers"" to have a ""regression to the mean""? In the casino example 20:48 ""black came 26 times in a row"", so it's likely that ""regression to the mean"" rule will begin to correct this ""anomaly"", right? So the next 26 dices will have a slight bias to have - not 13/13 REDS/BLACKS, but maybe smth like 15/11 (slightly correcting the anomaly), right? Or even 14/12, right? (otherwise there's no regression to mean). If yes then you should switch to RED, right? Or did i completely misunderstood the ""regression to the mean"" rule?"
OgO1gpXSUzU,"Today we would think of a few microseconds, but those machines were slow.
Hence was born Monte Carlo simulation, and then they actually used it in the design of the hydrogen bomb.
So it turned out to be not just useful for cards.
So what is Monte Carlo simulation? It's a method of estimating the values of an unknown quantity using what is called inferential statistics.
And we've been using inferential statistics for the last several lectures.
The key concepts-- and I want to be careful about these things will be coming back to them-- are the population.
So think of the population as the universe of possible examples.
So in the case of solitaire, it's a universe of all possible games of solitaire that you could possibly play.","This guy made a lot of errors. One of the earliest ones is at 3:25 when he attempts to define Monte Carlo simulation. He mentions an unknown quantity but that is incorrect. Suppose for example I wanted to simulate a known quantity, perhaps just to confirm that my programming skills are still good. So I would have someone good in math compute the exact answer using the ""paper and pencil"" method, and I would use a computer to simulate it to get an approximation. According to his definition, I would NOT be doing Monte Carlo simulation because the value is known. To take that concept even farther, suppose I try to simulate an ""unknown"" quantity but unbeknownst to me, someone else already has the correct exact answer. According to his definition, that is also NOT Monte Carlo simulation. He didn't specify who the quantity is unknown to in the definition, therefore it is not a good definition because it is ambiguous. If you Google Monte Carlo simulation, you will see MUCH better definitions of it then the crappy definition presented here in this lecture. Another big problem with his definition is he states the random sample tends to exhibit the same behavior as the population from which it is drawn. That is NOT true if the population has some super rare occurrence we are looking for such as 1 out of 1 quintillion. Now suppose 2 people, each with a computer, can simulate 1 trillion random samples. Since they are still 6 orders of magnitude below the full population size, it is VERY likely that they will both come up with 0 ""hits"" and possibly falsely concluding that there are no good ""hits"" (cases they are looking to count). Even if they reran the simulation 10 times each, then would still be 5 orders of magnitude below the population size which is 2/100,000 sampling which is not enough to get accurate results. Someone might say well 0 is very close to the real expected value (and that is true), but my point is by getting 0 ""hits"", no information is given about if a ""good"" event (the ones we are trying to count up exactly) even exist."
OgO1gpXSUzU,"We're saying we'd probably have fewer than 10.
That it'll be closer to the mean, not that it would be below the mean.
Whereas the gambler's fallacy would say it should be below that mean to quote, even out, the first 10.
Does that makes sense? OK, great questions.
Thank you.
All right, now you may not know this, but casinos are not in the business of being fair.
And the way they don't do that is in Europe, they're not all red and black.
They sneak in one green.
And so now if you bet red, well sometimes it isn't always red or black.
And furthermore, there is this 0.","27:30 But if we start counting from the beginning of the series, when we have 5 blacks in row, then the next black would change the series of 5 into the series of 6 ,which is more extreme. Can't I think this way ?"
OgO1gpXSUzU,"What does this mean? If I were to conduct an infinite number of trials of 10,000 bets each, my expected average return would indeed be minus 3.3%, and it would be between these values 95% of the time.
I've just subtracted and added this 3.5, saying nothing about what would happen in the other 5% of the time.
How far away I might be from this, this is totally silent on that subject.
Yes? AUDIENCE: I think you want 0.2 not 9.2.
JOHN GUTTAG: Oh, let's see.
Yep, I do.
Thank you.
We'll fix it on the spot.
This is why you have to come to lecture rather than just reading the slides, because I make mistakes.
Thank you, Eric.
All right, so it's telling me that, and that's all it means.
And it's amazing how often people don't quite know what this means.
For example, when they look at a political pole and they see how many votes somebody is expected to get.","Thank you for the great lecture. One question....at 39:00 I see it saying ""The return on betting a pocket 10k times in European roulette is -3.3%"". Was that based on the Monte Carlo sim? I ask because there are 37 pockets on a European roulette wheel. If you win it returns 35 to 1, plus your original wager, for 36 units returned on a win. 1/37 = 0.0270, for an expected return of -2.7%, or 97.3% (depending how you look at it) on European roulette. Thanks again for the awesome info...
At 38:02, I believe there is a typo: The confidence interval should be between -6.8% and +0.2% (not +9.2%). We get this because the avg return is -3.3% and adding +3.5% for the upper bound of the CI would yield 0.2%"
P3YoIxiz6to,"So in general, we have some problem A-- decision problem A and parameter k.
And we want to convert into some decision problem B with parameter k prime.
And of course, as usual, the set up is we're given an instance x.
This is going to look almost identical to NP Karp-style reductions, but then we're going to have one extra condition.
So instance x of A gets mapped to by a function f to an instance x prime of B.
X prime is f o x as usual.
This needs to be a polynomial time function just like for NP reductions, which means, in particular, x prime has polynomials size reduced back to x.
It should be answer preserving.
So x is yes instance for A if and only x prime is a yes instance for B.
So, so far, exactly NP reductions.
And then when we need one extra thing which is parameter preserving.
This is there's some function g, which I'll call the parameter blow up-- or, I guess you call it parameter growth for g-- such that the new parameter value for the converted instance is, at most, that function of the original parameter value of the original instance.
Question? AUDIENCE: Are there any limits on the amount of time that g can take to compute? PROFESSOR: g should be a computable function.
I think that's all we need.
Probably polynomial time is also a fine, but usually this is going to be like linear or polynomial or exponential.
It's rarely some insanely large thing, but computable would be nice.
Cool.
So that is our notion of parameterized reduction.
And the consequence, if this exists and B is fixed parameter tractable, then A is because we can take an instance of A converting it to B.
If the original parameter was founded by some k, this new parameter will be bounded by g of k.
New instance will be bounded of g of k.
So we run the FPT algorithm for B, and that gives us the answer to the original instance of A.
So if we don't care about what this function is, we are basically composing functions.
So there's some f dependence on k in this algorithm, and we're taking that function of g of k is our new function.
And we get a new dependence on k and the running time over FPT.
So what that means is if we believe it A does not have an FPT, then B does not have an FPT if we can do these reductions.","At around 21:00 you give the definition of a parameterized reduction. The classic definition usually states that a reduction is computable by some FPT algorithm (for example, in Flum & Grohe). Why do you opt to give this liberal definition of an FPT reduction, instead?"
PNKj529yY5c,"A small detail, not a particularly important one.
Now where are we.
We've got that guy there.
We've got our complete architecture.
We've got our solved problem.
And now we can start reflecting on what we've done.
We can say, for example, how good an integration program is this? And the answer is, it was pretty good.
This machine that Slagle was using was a machine that was over in building 26.
And we were so proud of it, that it was behind glass, and you could go there and watch the tape spin, it was really a delight.
32k of memory, that's 32k of memory.
It's amazing that he was able to do anything with a machine of that size.",How did he find functional composition depth at 34:45
PNKj529yY5c,"All good, I see some confused, worried, concerned looks.
Maybe I've made a mistake, perhaps I should use notes.
Well no, wait a minute.
For those of you who have a concerned look, remember that if x equals a sine y, then dx is equal to cosine y dy.
That's why it's cosine to the fourth not cosine to the fifth, as you were perhaps thinking it might be.
So now we've made some progress.
We look at this, we say, are there any safe transformations that apply? And the answer is, no.
Now we look for a heuristic transformation that might apply, and I say, what do you see? Which one? What's that? AUDIENCE: [INAUDIBLE].
SPEAKER 1: She said something unintelligible, but what she probably said is, that this looks like a pattern that might match with the heuristic transformation A, right? Because we have a function in which the variable is buried, universally in sines, or cosines, or tangents, or cotangents, or secants, or cosecants.
And we know we can rewrite that in one of three ways.
It's already written as a function of sine and cosine.
But we can also rewrite that in terms of tangent and cosecant.
Or cotangent and secant.
So when we do that, we can go this way, and we can get the integral of 1 over the cotangent of x dx.",Hey could someone explain me why dx = cos y at about 19:25? that would be very nice. I cant finde an explanation online. A good link would help me too. Thanks
PNKj529yY5c,"All right so that's progress, maybe.
But don't see this in any of the heuristic transformations, what do I do now? I didn't have to look in the heuristic transformations, because one of the safe transformations applies.
Because this thing is a rational function and the degree of the numerator is greater that the degree of the denominator, so I have to divide.
And when I divide, and that by the way is number four, I get what? Is anybody good high school algebra that can help me out with that? AUDIENCE: Y squared minus 2 plus negative 2 over 1 plus y squared SPEAKER 1: Exactly, y squared minus 1 plus 1 over 1 plus y squared, I think.",can anyone tell me at 28:50 how did he get that division result
PNKj529yY5c,"And now we have tangent to the fourth x dx.
Do I need the safe transformation supply? No.
Which of the-- you know something has to apply, otherwise it wouldn't be up here as an example.
So what of the heuristic transformation supply? Elliott.
AUDIENCE: [INAUDIBLE] SPEAKER 1: Yeah, B bravo.
Military background or something like that.
Maybe he flies airplanes.
OK so B says, it is in fact a function of the tangent.
And when we do that, we've got to make a substitution, that y is equal to the tangent.
So that means that this becomes the integral of y to the fourth over 1 plus y squared.
And that's by transformation B, and the transformation is y equals tangent of x.
The tangent-- I guess I've lost track of the fact that I've already transformed a y, but relabeling doesn't matter.",28:00 (tanx)^4 = y^4/(1 + y^2)^4 did he forget to put power 4 on the denominator
PNKj529yY5c,"But once we get a name for it, we'll get power over it.
And then we'll be able to deploy it, and it will become a skill.
We'll not just witness it, we'll not just understand it, we'll use it instinctively, as a skill.
So there you are, you've got that problem, there's your problem, and what do you do to solve it? I don't know, look it up in a table? You'll never find it in a table because of that minus sign and that 5.
So you're going to have to do something better than that.
So what you're going to do, is what you always do when you see a problem like that.
You try to apply a transform, and make it into a different problem that's easier to solve.
And eventually, what you hope is that you'll simplify it sufficiently, that the pieces that you've simplified to will be found in some small table of integrals.
So how long is this table? It's not the case that we're going to look at a table with 388 elements, because this is not a big table of integrals.
This is what a freshman might have in a freshman's head, after taking a course in integral calculus.
One of the interesting questions is, how many elements have to be in that table to get an A in the course? We're interested in how much knowledge is involved, that's one of the elements of catechism that I've listed over there, that will be part of the gold star ideas suite of the day.
So we'd like to take that problem, and find a way to make it into another problem that's more likely, or closer to being found in the table.
So what we're going to do is very simple, graphically.
We're going to take the problem we're given, and convert it into another problem that's simpler.
And we're going to give that process and name, and we're going to call it problem reduction.
And so, in the world of integral calculus, there are all sorts of simple methods, simple transformations, we can try that will take a hard problem and make it into an easier problem.
And some of these transformations are extremely simple and always safe.
Some of them are just, well let's try it and see what happens.
But some of them are safe, and I'd like to make a short list of safe transformations right now.
Now I'm going to be going into some detail.",what's problem reduction? 03:35
PNKj529yY5c,"Now what? Now we're really getting close to getting through this, because that is a sum.
And by virtue of the fact that it's a sum, that divides into three pieces, and the top piece is the integral of y squared, the middle piece is the integral of minus 1, and the bottom piece is the integral of 1 over 1 plus y squared dy in all cases.
Gosh, if I look this up, I've found it.
That's up there, that's letter B.
So I'm done with that.
This one I can transform again, by virtue of 1, and now I get the integral dy.
That's in there, that's B as well.
As this one, I don't know.
But I'd better keep track of what I'm doing here.
This is in the and node, so I've got to do all of those.
I can't give up on that last thing.
And that and transformation is transformation number 3.
So this is in the table, this is in the table, we still have this to do, but that's C, heuristic transformation C.
We have 1, plus y squared, then with the transformation C, with y-- this is y squared-- y equals tangent of z And then we get to the integral of dz and that's in the table and, we're done.",How did he get to dz from 1/(1+y^2) dy at 31:07 ?
PNKj529yY5c,"Or we can rewrite that as a function of tangent of x, and cosecant of x.
Or we can rewrite that as function of cotangent of x, and the secant of x.
So that's a transmission from trigonometric form, into another trigonometric form.
It's not always a good idea, sometimes it helps.
Well that's just part one of our suite of heuristic transformations.
Stop.
There are others that we need to have in our repertoire, in order to solve the problem.
One of them is a family of transformations, which I'll show you only one.
It goes like this, if you have the integral of a function, of the tangent of x, then you can rewrite that as the integral of a function of y over 1 plus y squared dy.
So that's a transformation from a trigonometric form into a polynomial form.
So it gets rid of all that trigonometric garbage we don't want to deal with.
And there's a whole family of things like that, just as there's a family of transformations like so, but this is enough to give you flavor.
Now there's a C that we need as well.
And that's going to be your proper knee-jerk reaction when you see something of the form 1 minus x squared.","Where the heuristic transformations after 15:30 come from, what is this?"
PNKj529yY5c,"We have no transformation that can take us further, so we need something else.
And what we need by way of something else, is some transformations that we will describe as-- perhaps we'll call them, heuristic transformations.
A funny word, meaning a method that often works isn't guaranteed to work.
It's not an algorithm in the usual sense that we talk about algorithms.
But rather, it's an attempt.
So these things I'm going to talk about now, are sometimes useful, not always useful.
Sometimes take you into a blind alley, don' always work.
But you can't get an A in calculus without knowing some of them.
So you said, some kind of trig substitution.
So here is some kind of trig substitution.
We'll call this heuristic transformation A.
You have a function sine x, cosine x, tangent of x, cotangent of x, secant of x, and cosecant of x.
And we all know from high school trigonometry, that we can rewrite that as a function of sine x, and cosine x.",Where can I find readings for the transformations mentioned at 14:30 ? I have taken calculus classes but I have never seen anything similar to that before and would like to learn.
QOtA76ga_fY,"One is that they assumed that encryption also provides authentication or integrity of messages.
So don't do that.
And Kerberos version 5 fixes this by explicitly authenticating messages.
Another thing they sort of had a problem with is the ability for arbitrary clients to guess people's passwords.
So any suggestions of how we could fix this? How do you prevent guessing attacks in a protocol like this? What could we try? Yeah.
STUDENT: Some sort of salting? I'm not sure.
PROFESSOR: Well, so salting would just means that the client has to hash the password in different ways, maybe.
But it still doesn't prevent them from trying lots of things.
So maybe it'll be more expensive to build a dictionary.","How to discover the password by brute force as the teacher asked at the minute 31:15? The password is hashed on Kc. But the attacker does not know the Kc, only the encrypted message: {{Tc,s}Ks, Kc,s}Kc. He can try brute force the encrypted message: {{Tc,s}Ks, Kc,s}Kc, but he has no idea what is the true value of {{Tc,s}Ks, Kc,s}Kc, he can try all Kc possibles, include the correct key, and he won't know if the result found is the correct one."
QUO-HQ44EDc,"So it means using the training data, we are going to train two classifiers.
For example, using a linear classifier, a neural network, or support vector machine or a decision tree that given a feature vector of a given node, it's going to predict its label.
And in phase 1, we are going to apply this classifier Phi_1, so that now every node in the network has some predicted, uh, label.
And then using the training data, we are also going to train this classifier Phi_2, that is going to use two inputs.
It's going to use the feature of the node of interest, as well as this summary vector z that captures or summarizes the labels of the nodes, um, in it's network neighborhood.
And this Phi_2 is going to predict the label of the node of interest v based on its features, as well as the summary z of its, uh- of the labels of its neighbors.
And then- right, so we have first applied Phi_1 then we also have trained Phi_2 so that now we will go into phase 2 where we are going to iterate and we are going to iterate until convergence, uh, the following- uh, the following, uh, iteration where on the, uh, test set, which means on the unlabeled nodes.
Uh, we are going to use this, uh, first, the classifier Phi_1 to assign initial, uh, labels.
We are going to compute the label summaries z, and then we are going to predict the labels with the second classifier, Phi_2.
And we are going now to repeat this process until it converges in a sense that we are going to update the vector z.","Thanks for the great lecture. I have a question. For Iterative classifier (about 16:05), in Phase 1 (i.e. training phase), we train 2 classifiers \phi_1 and \phi_2. For the label summary vector z_v required for the classifier \phi_2, is it derived from the output produced by \phi_1? Or are the classifiers trained separately, as is suggested by earlier comment? Because if we do train separately, the error from \phi_1 will be added to the error from \phi_2. Which means that \phi_2 was not trained to be somewhat robust to the possible noise (that is absent in training but present in test due to error from \phi_1) in z_v."
RN8qpSs8ozY,"Um, in online settings if you don't know that, you can also constantly be updating it with a time step.
It's a good question.
Yeah.
How is delta decided? How is what? Pardon.
How is delta, is like what is delta? Okay.
Good question.
Um, so, question is what is delta.
What we're gonna, I did not tell you, uh, in this ca- well, in this case it's telling us, um, it's specifying what is the probability this is holding like what this inequality is holding.
Later we're gonna pro- provide a regret bound that is high probability.
So we're gonna say we're gonna have a regret bound which is something that like with probability is 1 minus a function of delta, um, your regret will be sub-linear.
So that's how.
You can get expected regret bounds too and the UCB paper which, um, one of the original UCB papers provides an expected bound but I thought it was a little, the, this bound was a little bit easier to do in class.
So I thought I would do the high probability bound.
Yeah.
So before we were talking about regret, I didn't exactly understand how you use regret to update your estimate of the action value.
Oh, good question.
Um, so question is, do we use or how would we use the regret bound, the regret to update our estimate action, we don't.
Regret is just a tool to analyze our algorithm.
Great clarification.
So regret is a way for us to analyze whether or not an algorithm is gonna be good or bad in terms of how fast the regret gro- grows but it's not used in the algorithm itself.
The algorithm doesn't compute regret.
And it's not used in terms of the updating.
Excuse me.
Okay.","1:07:05 presumably, this is only true with the upper bound holds right?
I guess are you are referring to 1:02:29? Perhaps it was because in the beginning by the definition of upper bound, U(at)>Q(at) with HIGH PROBABILITY (I know it sounds vague and sloppy)?"
Rfkntma6ZUI,"So perhaps your GN- GCN, GNN may need to be a bit deeper, but you have reduced, uh, the number of parameters, uh, significantly, which will lead to, uh, faster training, perhaps more robust model, less overfitting, um, and so on.
So that's, uh, the technique of, uh, block diagonal matrices, uh, where basically you reduce the dimensionality of each W by assuming a block diagonal structure.
Uh, second idea how to, uh, make this, uh, RGCN more scalable and its learning more tractable is to- to build on the key insight which is we wanna share weights across different relations, right? We don't wanna consider relations as independent from each other, but we want relations to kind of share weights, uh, to also share some information.","In Basis Learning (18:42) it is unclear if the learned scalar a_rb are relation specific. which means the importance for each matrix V will be a vector rather than a scalar.
In Basis Learning (18:42), it is unclear how we get the basis matrices (V_b). Do we set them to be trainable just like the importance weight coefficients (a_rb)?"
RvRKT-jXvko,"So why would we want to use tuples? Tuples are actually useful in a couple of different scenarios.
So recall a few years ago, we looked at this code where we tried to swap the values of variables x and y.
And this first code actually didn't work, because you're overwriting the value for x.
So instead, what we ended up doing was creating this temporary variable where we stored the value of x, and then we overwrote it, and then we used the temporary variable.
Well, turns out this three liner code right here can actually be written in one line using tuples.
So you say x comma y is equal to y comma x.
And Python goes in and says, what's the value of y? And assigns it to x.
And then what's the value of x? And assigns it to y.
Extending on that, we can actually use tuples to return more than one value from a function.
So functions, you're only allowed to return one object.
So you're not allowed to return more than one object.",5:28 what are they useful for
RvRKT-jXvko,"We haven't created a new object in memory.
We're just modifying the same object in memory.
And you're going to see why this is important as we look at a few side effects that can happen when you have this.
So I've said this a couple of times before, but it'll make your life a lot easier if you try to think of-- when you want to iterate through a list if you try to think about iterating through the elements directly.
It's a lot more Pythonic.
I've used that word before.
So this is sort of a common pattern that you're going to see where you're iterating over the list elements directly.
We've done it over tuples.
We've done it over strings.
So these are identical codes.
They do the exact same thing, except on the left, you're going from-- you're going through 0, 1, 2, 3, and so on.
And then you're indexing into each one of these numbers to get the element value.
Whereas on the right, this loop variable i is going to have the element value itself.
So this code on the right is a lot cleaner.
OK.
So now let's look at some operations that we can do on lists.
So there's a lot more operations that we can do on lists, because of their mutability aspect than we can do on tuples or strings, for example.",at 19:19 a list was not created yet. Can someone explain how this works
SE4P7IVCunE,"Because this is just like we did in the very first program, when I'm checking 0, 1, 2, 3, 4, 5, 6, 7, 8, when I'm trying to find the cube root of 8.
Once I've reached 8, I'm going to stop.
And it's the same thing here.
So I just added this little clause that says, well, while I'm greater than or equal to epsilon and I'm still less than the actual cube, just keep searching.
But once I've reached the cube, then stop searching.
And with 10,000, you can see that I failed to actually find-- so that's what this part, here, does.
It tells me I've failed to find the cube root with those particular parameters.
The last thing we're going to look at is bisection search.
And to illustrate this, I'm going to need one volunteer.
And you're going to play a game with me in front of the whole class.","Nice tutorial and lesson. Just a small note, IMHO, on ~35:00 Anna made a small ""mistake"" when added ""and guess<= cube"" to the test condition. Epsilon is an error range, so the correct answer (in general) is << Cube-epsilon<=guess**3<=Cube+epsilon >>. Therefore the test condition in the example should be ""and guess<= cube+epsilon"" or test should be rewritten according like within <<...>>
Code at 35:33 does not work for negative cubes. Here some changes in order to let it work also for negative numbers. Hope it will be useful :). _________________________________________________________________________________ cube = 27 epsilon = 0.01 guess = 0.0 increment = 0.0001 num_guesses = 0 while abs(guess**3 - cube) >= epsilon and abs(guess**3) <= abs(cube): # the increment must be negative if cube < 0 and intitial guess = 0, otherwise (guess**3 - cube) will always increase as guess increases guess += cube/abs(cube)*increment num_guesses+=1 print('num_guesses =', num_guesses) if abs(guess**3 - cube) >= epsilon: print('Failed on cube root of', cube) else: print(guess, 'is close to the cube root of', cube)"
SE4P7IVCunE,"But it's an entirely different object that I've created here.
Again, it might not mean anything right now, but just keep this in the back of your mind, strings are immutable.
So the next thing I want to talk about is a little bit of recap on for loops.
And we're going to see how we can apply for loops, very easily, to write very nice, readable code when dealing with strings.
So remember that for loops had a loop variable.
My loop variable being this var, here, in this particular case.
It can be anything you want.
And this variable, in this particular case, iterates over this sequence of numbers, 0, 1, 2, 3, 4.
So the very first time through the loop, var has a value of 0.
It does the expressions in the loop.
As soon as they're done, var takes the value 1.
It does all the expressions in the loop.
And then var takes the value 2, and it does that all the way up until 0, 1, 2.","13:21 loop variable that iterates over a collection, not a set?"
SE4P7IVCunE,"It's the same as 0 to the length s going every step.
This one might actually be useful.
It reverses the string automatically for you.
So with this one little line here, you can get the inverse of your string.
And that's equivalent to that.
So the minus 1 represents starting from the end and going back every letter.
And then this one's a little bit more complicated but also not too bad.
So as we're doing these string slices, again, if you're unsure what something does, just type it into Spider.
And you might be surprised.
You might not be.
But it's a good way to check yourself, to make sure you're understanding what's happening.","OpenCourseWare Error at 10:09 It should evaluate to ""hgfedcba"", not ""hgfedbca"" . the ""b"" and ""c"" may be opposite. Please correct me if i am wrong. Everyone, please LIKE my comment to bring it up to MIT if you agree with me. Thank you!"
SE4P7IVCunE,"Log base 2 of 100 is 6.-something, I think.
So in fact, I could have said, if I don't guess it within seven guesses, you would have won as well.
So that's why the game was rigged.
So the guess, notice, it converges on the order of log base N instead of just linearly in terms of N.
So that's why it's so powerful.
One last thing I want to mention is the code I showed only works for positive cubes.
And that's because of the following.
So I have this 0 and 1.
Let's say I'm trying to find the cube root of 0.5.
When I first set my initial boundaries, my low is this one, and my high is this one.
But what's the cube root of 0.5? Is it within this boundary or is it outside this boundary? AUDIENCE: Outside the boundary.","43:51 She meant to say it only works for cubes greater than or equal to 1
Correct me if I'm wrong but in 43:50 when the professor says the code works only for positive cubes, she makes an example of 0.5 not working, but that is the positive cube of 0.79... So I guess the correct statement to make is that this algorithm works only for positive cubes ""bigger or equal to 1""."
SE4P7IVCunE,"So I want to start at the first character and go halfway into the string, or I want to take a few characters in between, or I want to skip every other letter or something like that in my string.
So if I want to do this slightly more complicated interaction with strings, we call that slicing, slicing into a string.
And this notation here should seem a little bit familiar, because we saw it last lecture when we did it with range.
We had a start, stop, and a step.
The notation was a little bit different, because, in range, we had open-close parentheses and commas in between.","Is there a typo at 6:56? s[: : -1] -> ""hgfed'BC'a"" doesn't make sense, unless it's supposed to be ""...cba"""
SE4P7IVCunE,"So as long as the guess cubed minus the cube-- so how far away are we from the actual answer-- is greater than some epsilon, keep guessing, because the solution is not good enough.
But once this is less than epsilon, then we've reached a good enough solution.
So two things to note with approximate solutions.
So you can get more accurate answers if your step size is really, really small.
If you're incrementing by 0.0001, you're going to get a really good approximate solution, but your program will be a lot slower.
Same sort of idea with epsilon, you can change epsilon.
If you change epsilon to be a bigger epsilon, you're sacrificing accuracy, but you're going to reach a solution a lot faster.
So here's the code for the approximate solution of a cube root.
It might look intimidating, but, look, almost half this code is just initializing variables.
So we're initializing, this is the cube we want to find the cube root of.
We pick an epsilon of this.
We start with a guess of 0.
We start with an increment of 0.0001.
And just for fun, let's keep track of the number of guesses that it takes us to get to the answer.",Can someone help me out - surely the code at 29:55 is wrong? The While loop should not run because the condition is not true i.e. guess**3 - cube is NOT greater than or equal to 0.1. Rather 0x0x0 - 27 is LESS than 0.1? So surely the While loop would not run in the first place?
SE4P7IVCunE,"So my cube is 8.
I'm going to have a for loop that says, I'm going to start from 0.
And I'm going to go all the way up to-- So I'm going to start from 0 and go all the way up to 8.
For every one of these numbers, I'm going to say, is my guess to the power of 3 equal to the cube 8? And if it is, I'm going to print out this message.
Pretty simple, however, this code is not very user friendly, right? If the user wants to find the cube root of 9, they're not going to get any output, because we never print anything in the case of the guess not being a perfect cube.
or the cube not being a perfect cube.
So we can modify the code a little bit to add two extra features.
The first is we're going to be able to deal with negative cubes, which is kind of cool.
And the second is we're going to tell the user, if the cube is not a perfect cube, hey, this cube is not a perfect cube.
So we're not going to silently just fail, because then the user has some sort of feedback on their input.
So let's step through this code.
We have, first of all, a for loop just like before.
And we're going to go through 0 to 8 in this case.
We're using the absolute value, because we might want to find the cube root of negative numbers.
First thing we're doing is doing this check here.
Instead of guessing whether the guess to the power of 3 is equal to the cube, we're going to check if it's greater or equal to, and we're going to do that for the following reason.",at 24:50 why is there a +1 in range of that for loop? To make the loop go till 8 instead of stopping at *7*?
STjW3eH0Cik,"And there are b to d of those.
How many are there at this next level up? Well, that must be b to the d minus 1.
How many fewer nodes are there at the second to the last, the penultimate level, relative to the final level? Well, 1 over b, right? So, if I'm concerned about not getting all the way through these calculations at the d level, I can give myself an insurance policy by calculating out what the answer would be if I only went down to the d minus 1th level.
Do you get that insurance policy? Let's say the branching factor is 10, how much does that insurance policy cost me? 10% of my competition.","I have a question: At 36:22 he says what if we don't have enough time and we went only till the (d-1) th level. And then he also suggests we can have a temporary answer at every level as we go down as we should have some approximate answer at any point of time. But!! How can we have any answer without going to the leaf nodes because it's only at the leaf nodes we can conclude who can win the game. Think this for tic-tac-toe game. At (d-1)th level we don't have enough information to decide if this series of moves till this node at (d-1) will win me or lose me the game. At higher levels say at (d-3) it's so blur! Everything is possible as we go down! Isn't it? So, if an algorithm decides to compute till (d-1) th level then all those path options are equal!! Nothing guarantees a win and nothing guarantees a lose at (d-1)th level because if I understand correctly wins and losses are calculated only at the leaf nodes. This is so true especially in pure MinMax algorithm. So how exactly are we going to have an 'approximate answer' at (d-1)th level or say (d-5)th level?"
STjW3eH0Cik,"I'll need some help, especially from any of you who are studying cosmology.
So, we'll start with how many atoms are there in the universe? Volunteers? 10 to the-- SPEAKER 2: 10 to the 38th? SPEAKER 1: No, no, 10 to the 38th has been offered.
That's why it's way too low.
The last time I looked, it was about 10 to the 80th atoms in the universe.
The next thing I'd like to know is how many seconds are there in a year? It's a good number have memorized.
That number is approximately pi times 10 to the seventh.
So, how many nanoseconds in a second? That gives us 10 to the ninth.
At last, how many years are there in the history of the universe? SPEAKER 3: [INAUDIBLE].
14.7 billion.
SPEAKER 1: She offers something on the order of 10 billion, maybe 14 billion.
But we'll say 10 billion to make our calculation simple.
That's 10 to the 10th years.
If we will add that up, 80, 90, plus 16, that's 10 to the 106th nanoseconds in the history of the universe.
Multiply it times the number of atoms in the universe.
So, if all of the atoms in the universe were doing static evaluations at nanosecond speeds since the beginning of the Big Bang, we'd still be 14 orders of magnitudes short.","Question - Prof Winston says (around 12:30) that there are PI * 10^7 seconds in a year. Can anyone tell me where the PI comes from in this result? Does this take into account leap years somehow? Otherwise, shouldn't it be 60 secs/min x 60 min/hr x 24 hrs/day x 365 days/year = 3.1536 which is close to PI but not equal. Thanks"
STjW3eH0Cik,"Is that a big number? We could do a little algebra on that and say that b to the d is a huge number.
So, that minus one doesn't count.
And B is probably 10 to 15.
So, b minus 1 is, essentially, equal to b.
So, that's approximately equal b to the d minus 1.
So, with an approximation factored in, the amount of computation needed to do insurance policies at every level is not much different from the amount of computation needed to get an insurance policy at just one level, the penultimate one.
So, this idea is called progressive deepening.
And now we can visit our gold star idea list and see how these things match up with that.
First of all, the dead horse principle comes to the fore when we talk about alpha-beta.
Because we know with alpha-beta that we can get rid of a whole lot of the tree and not do static evaluation, not even do move generation.","40:50: He misspelled ""martial arts"". Well at least he doesn't teach in the English dept, lol. He is awesome though, nonetheless."
STjW3eH0Cik,"So, let's carry on and see if we can complete this equal to or less than 8, equal to 8, equal to 8-- because the other branch doesn't even exist-- equal to or less than 8.
And we compare these two numbers, do we keep going? Yes, we keep going.
Because maybe the maximizer can go to the right and actually get to that 8.
So, we have to go over here and keep working away.
There's a nine, equal to or less than 9, another 9 equal to 9.
Push that number up equal to or greater than 9.
The minimizer gets an 8 going this way.
The maximizer is insured of getting a 9 going that way.
So, once again, we've got a cut off situation.
It's as if this doesn't exist.
Those static evaluations are not made.","29:34, for the deep cut, did he compare two Max nodes? or compared the bottom Min node with the root Max node?"
STjW3eH0Cik,"So, you have to see that.
This is the important essence of the notion the alpha-beta algorithm, which is a layering on top of minimax that cuts off large sections of the search tree.
So, one more time.
We've developed a situation so we know that the maximizer gets a 2 going down to the left, and he sees that if he goes down to the right, he can't do better than 1.
So, he says to himself, it's as if that branch doesn't exist and the overall score is 2.
And it doesn't matter what that static value is.
It can be 8, as it was, it can be plus 1,000.
It doesn't matter.
It can be minus 1,000.
Or it could be plus infinity or minus infinity.
It doesn't matter, because the maximizer will always go the other way.
So, that's the alpha-beta algorithm.
Can you guess why it's called the alpha-beta algorithm? Well, because in the algorithm there are two parameters, alpha and beta.
So, it's important to understand that alpha-beta is not an alternative to minimax.
It's minimax with a flourish.
It's something layered on top like we layered things on top of branch and bound to make it more efficient.
We layer stuff on top of minimax to make it more efficient.
As you say to me, well, that's a pretty easy example.
And it is.
So, let's try a little bit more complex one.
This is just to see if I can do it without screwing up.
The reason I do one that's complex is not just to show how tough I am in front of a large audience.
But, rather, there's certain points of interest that only occur in a tree of depth four or greater.","In the alpha-beta example, the branch that doesn't actually get created is just the right-most one that leads to the terminal node (not computed, because of the ""cut""). Is that right? If it's right, than the statement ""it's as if that branch doesn't exist"" (24:00) must be interpreted such that the algorithm will never choose the action that leads to the right-hand node (the one <= 1). Is this interpretation correct?"
STjW3eH0Cik,"That's level 0.
That's level 1.
This is level d minus 1.
And this is level d.
So, down here you have a situation that looks like this.
And I left all the game tree out in between .
So, how many leaf nodes are there down here? b to the d, right? Oh, I'm going to forget about alpha alpha-beta for a moment.
As we did when we looked at some of those optimal searches, we're going to add these things one at a time.
So, forget about alpha-beta, assume we're just doing straight minimax.
In that case, we would have to calculate all the static values down here at the bottom.",I have a problem understanding the insurance policy being taught at 35:50. Can anybody help?
STjW3eH0Cik,"This move generation is not made and computation is saved.
So, let's see if we can do better on this very example using this alpha-beta idea.
I'll slow it down a little bit and change the search type to minimax with alpha-beta.
We see two numbers on each of those nodes now, guess what they're called.
We already know.
They're alpha and beta.
So, what's going to happen is the algorithm proceeds through trees that those numbers are going to shrink wrap themselves around the situation.
So, we'll start that up.
Two static evaluations were not made.
Let's try a new tree.
Two different ones were not made.
A new tree, still again, two different ones not made.
Let's see what happens when we use the classroom example, the one I did up there.
Let's make sure that I didn't screw it up.
I'll slow that down to 1.
2, same answer.
So, you probably didn't realize it at the start.","At 30:25, shouldn't the root node be updated to >= 8 ?"
STjW3eH0Cik,"We want to pull out every trick we can find to get as far as possible.
Now, you remember when we talked about branching down, we knew that there were some things that we could do that would cut off whole portions of the search tree.
So, what we'd like to do is find something analogous to this world of games, so we cut off whole portions of this search tree, so we don't have to look at those static values.
What I want to do is I want to come back and redo this thing.
But this time, I'm going to compute the static values one at a time.","21:36 It's not ""branching down"", he says ""branch and bound"" from previous class."
SUJCgisKW1Y,"So we'll be looking at what's the best way? Something like y hat test log y tests.
I should have written this down before.
I think the cross entropy loss is actually the opposite of this.
Is this correct for cross entropy loss? Something like this.
And so then you'll be minimizing the parameters of basically our comparator function with respect to how accurate your predictions are.
So more specifically, what this will look like is this corresponds to something called matching networks.
And what we're going to do is we're going to encode our training examples into some embedding space.
And then for each of those embeddings, we will compute this function that you can think of this as potentially taking the dot product between your embedding for x test and you're betting for xk.",Does the cross entropy loss shown in 17:38 need a negative sign before it?
SXBG3RGr_Rc,"OK, it's L'Hopital.
L'Hopital's Rule.
You have to differentiate the-- I guess we differentiate this guy as a ratio or something, and see what happens when it goes to 0.
And what we get when we use L'Hopital's Rule is that, oh thank God, this is still zero.
So now we know that we have a point up there and a point down there.
So now we've got three points on the curve, and we can draw it.
It goes like that.
No, it doesn't go like that.
It's obviously a Gaussian, right? Because everything in a nature is a Gaussian.
Can you put that laptop away, please? Everything in nature is a Gaussian, so it looks like this.
That right? No, actually, not everything in nature is a Gaussian.
And in particular, this one isn't a Gaussian either.
It looks more like one of those metal things they used to call quonset huts.
That's what it looks like.
Boom, like so.
So that is the curve of interest.
Now, did God say that using this way of measuring disorder was the best way? No, Got has not indicated any choice here.
We use this because it's a convenient mechanism, it seems to make sense, but in contrast to the reason it's used information theory, it's not the result of some elegant mathematics.
It's just a borrowing of something that seems to work pretty well.
Any of those curves would work just about the same, because all we're doing with it is measuring how disordered a set is.","at 29:48 why do we need to use L'Hospital Rule?? N/T is 0 (not tending to 0). So if we multiply anything with 0 we get 0, right??"
Sdw8_0RDZuw,"And at this point, I'm done.
I'm left with a set of edges called covering edges, which have the property that the only way to get from one vertex to another is going to have to be to use a covering edge to the target for vertex.
Or more precisely, the only way to get, say, from a to b is going to be to use that covering edge.
If there was any other path that went from a elsewhere and got back to b without using this edge, then it wouldn't be a covering edge anymore.
The fact that it's a covering edge means that if you broke it, there's no way anymore to get from a to b.
So that's the definition of covering edges and you'll do a class problem about them, more precisely, in a minute.
So the other edges are unneeded to define the walk relation.
And all we need to keep are the covering relations to get the minimum representation of the walk relation in terms of a DAG.
","11:00 Edge(b,d) is not a covering edge? So should it not be deleted?"
T1AtlGrCoU8,"And having proved both for all x Q of x and for all y P of y, clearly the AND holds.
And I've just proved that the right-hand side of this implication is true given that the left-hand side is true.
Now, having called this proving validity, let me immediately clarify that this is not fair to call a proof because the rules of the game are really murky here.
This theorem, you could read it as saying that universal quantification distributes over AND is one of these basic valid formulas that is so fundamental and intelligible that it's hard to see what more basic things you are allowed to assume when you're proving it.","5:53 Now this I just couldn't grasp clearly..... What exactly u mean by this proposition I mean? think with z u mean a set of all possible value of z... right? ... and P(z) and Q(z) u say some property of those elements... right? So z define a domain or set (here) of elements of type z (as far as elements are concern) ...no? ... My question is what makes u think x and y elements are part of that set z? So how come that UG logic applies?.... I mean I got confused in all these jargons here otherwise was going fine till now. :) Can u define the problem with some concrete real world examples or set... whatever... so that I can comprehend what u actually driving at? I mean it can be just set of number or anything like lets say z is a set of some male persons.... and P(z) or Q(z) be some property of them.... No how come u assume that x and y be the male persons as well... more specifically male persons from that z set? ... Like we say P(z) and Q(z) means a person is black and taller than 6 ft... how come some random x be black or taller than 6ft even if he happens to be male by chance? or even a person at all. There is nothing said what x is... might be goat, cow or even rock. "
TDo3r5M1LNo,"It's like interval arithmetic.
You know, interval arithmetic? I want to know, what are the extremes I can get on the output of a division if I'm given that a number is in some interval here and some interval here? If the answer is always, use one of the extreme endpoints here and use one of the extreme endpoints here, then this algorithm will work.
Otherwise, all bets are off.
Cool.
So if you negate-- if you put a minus here, that will work fine, because it's negating this range.
And then it's just like sum.
But-- AUDIENCE: [INAUDIBLE] ERIK DEMAINE: Oh, a divider-- if you're careful about 0, yeah.
Actually, it doesn't work, because we care about how close this can get to 0 for division.
It might be enough to consider those.
It's like, instead of minimizing and-- instead of computing this entire interval, if this interval spans 0, maybe I need to know-- if 0 is here, I need to know how close to 0 I can get on the left side and how close to 0 I can get on the right side.
Still just four quantities I need to know.
I would guess, for division, that's enough.
Yeah.
Nice.
Solved a little problem.
Then, we would be multiplying the subproblem space, instead of by 2, by 4.
Hey, maybe we should put this on the final.
No, just kidding.
Now it's in lecture, so we can't use it.
But it's a cool set of problems, right? You can do a lot with dynamic programming.
You don't need to be that clever, just brute force anything that seems hard.
And when it works, it works great.
And this class is all about understanding when it works and when it doesn't work.
Of course, we will only give you problems where it works.
But it's important to understand when it doesn't work.
For example, DAG shortest paths-- that algorithm on a non-DAG, very bad.
Infinite time.
OK.
Our last example is piano fingering.",Can someone explain to me 45:12 How to use this algo for division 
TOb1tuEZ2X4,"And correspondingly, you can have either one or two keys in a node.
Make sense? AUDIENCE: Yeah.
PROFESSOR: Cool.
OK So coming back to this.
So the root does not have a lower bound.
The root can have one child in any tree.
So you have a B equal to 5 tree, the root can still have one child-- sorry.
Not one child, one key element, two children.
All right.
It's good.
Also it's completely balanced.
So all the leaves are the same depth.
So you can see it here, right? So you can't have a dangling node here.
This is not allowed.
You have to have a leaf.
You have to have something going down, and everything ends at the same level.","1:32 Is no one in the class paying attention? It isn't at all clear why the depth is logN. 8:04 Finally mentioned that all the leaves are at the same depth, and only then do you know that the depth is actually logN and the tree is balanced."
TOb1tuEZ2X4,"It's less than 30, it goes to the left.
It's between 10 and 17, it goes in the middle.
16.
And it's greater than 14, so we add 16 here.
All right.
That seems good.
All the properties fine.
This still has two elements, which is the maximum, but it's good.
It doesn't overflow.
Let's insert something else.
Let's insert 2.
So 2 goes to 30, goes down, goes down.
And we have a problem, because 2 has overflowed this node.
So we split.
And the way we split is we take the middle element.
So we split the node here.
And 3 goes up to the parent, so 3 goes here.
And all good, except for the parent has overflowed.
So what do we do with the parent? We split the parent again.
And this time, it's right down the middle, the 10 goes up.
So OK, let's get rid of this.
So now that we split the parent, the 10 goes up here.
And you're good.
It's a bit cluttered, so let me reposition the 17.
Did those two operations make sense? Questions? AUDIENCE: If your node size [INAUDIBLE] number of-- PROFESSOR: So just pick the-- first of all-- OK.","Guys, the b-tree in 20:08 is not valid, isn't it? We are having problems with the root node"
TOb1tuEZ2X4,"So that's the general node.
So before we go into more details of the properties and everything, the question is why use B-trees.
So if we do a quick depth analysis, we can see that the depth is to log n rate.
Is that clear to everyone sort of, why the depth is log n? Because you have branching just like in binary search trees.
In fact, you have more branching.
But in any case, depth is to log n.
But why use B-trees over binary search trees? Anyone have a reason why you would prefer to use B-trees or not? So all the operations are still log n.","1:32 Is no one in the class paying attention? It isn't at all clear why the depth is logN. 8:04 Finally mentioned that all the leaves are at the same depth, and only then do you know that the depth is actually logN and the tree is balanced."
TOb1tuEZ2X4,"You do a merge.
So what do you have? So here you have B minus 2, and here you have B minus 1.
And you get 2B minus 3.
Well, you've got another element.
You also take the parent.
So how do you do the merge.
I just want to show you the merge first.
So the way you do it is you move the parent down, and you merge these two.
Seems OK? So you move the parent node down and merge these two.
And, well, now this comes together, and this points into the new node.
Sort of clear what's going on? Questions? Yes? AUDIENCE: So now the parent is underfull? PROFESSOR: Well, so you have-- yeah, exactly.
So you have decreased the size of the parent, so it might be underfull.
So you propagate.
Anything else? AUDIENCE: So are these all different techniques for doing that? PROFESSOR: So there are two cases.
So either you have a sibling which has extra nodes to donate to you or you don't.
If you don't, then you have to do this.
AUDIENCE: But what about that case? Or is that just like-- PROFESSOR: No, that is moving it down to the leaf.
Once you move the deletion down to the leaf, so here we have something now.
And now you move it all the way back up.",Have a question... The nodes are not supposed to have more than b-1 keys.. But in his example (26:16 m) he got 5 keys in one leaf... how is that right?
TU0ankRcHmo,"And notice what I've wrote here is basically, p of t equals M times p of t.
So now, because we have this correspondence, this means that r is a stationary distribution of the random walk.
So basically, it means that this flow based equations can be interpreted based on the flow, or can also be interpreted as this intuition that there is a random walker walking around infinitely long over the graph, so that after some time, basically, it doesn't matter where the walker- random walker started, but it's really all about this distribution of where the random walker is, is going to converge to this stationary, uh, distribution.
And in order for you to compute the stationary distribution, is the same problem as it was before to solve that system of equations or to solve this recursive equation of r equals m times r.","I enjoyed the lecture. It was very nice to learn page rank with that level of detail! I had a question about the connection between random walks and the flow equation. At 20:07 we had an assumption that the random walk reaches a stationary state in a finite number of steps. Then it's clear that the stationary state is nothing but r. I didn't find how we address this assumption later in the lecture. What if it doesn't converge to a fixed state in a finite number of steps? I.e. just comes closer and closer forever. Will the limit still be r? In the lecture there was also a statement that the start of the random walk doesn't depend for the final converged state which is going to be r anyway. I think I've got a proof that addresses both questions in case M has n independent eigenvectors (n is the number of nodes). However, I'm not sure if this assumption is generic and therefore if this proof has any value. Anyway, I'd gladly discuss any related ideas."
TWVntUfXsKs,"It's 110 choose to times the variance of the M sub ij turns out to be about 16.37, which means that the standard deviation sigma is less than 4.
Now I can apply Chebyshev, because by the Chebyshev band the probability that 16.4 is within a 2 sigma, is further away than 2 sigma, is only one chance in four.
Which means the probability that it's within 2 sigma, that the actual number of measured pairs is within 2 sigma of the expected number 16.4 is greater than 1 minus 1/4, or 3/4.
There's a 3/4 chance that the number of pairs that we find is within 2 sigma of the expected number 16.4.
Sigma was about 4, so this is 8, which means that we're expecting, with 3/4 probability, somewhere between 8.4, meaning 9, and 24.4, meaning 25 pairs.
So 75% of the time, in a class of 110, we're going to find between 9 and 25 pairs of birthdays.
Did that actually happen? Well it did.
In our class of 110 for whom we had data, we actually found 21 pairs of matching birthdays.
Literally we found 12 pairs and three triples, but each triple counts as three matching pairs.
And there they are, the blues are triples.
And you can see whether your birthday is among those, and knowing that you have a classmate or two that have the same birthday that you do.
So there are 15 different birthdays, but they count as 21 pairs because it's 12 single pairs, and three triplets, each of which counts for three pairs.
","11:03 If anyone is confused at the formula, he meant to write Pr[16.4-2(sigma)<=|R-u<=16.4+2(sigma)] but simply replaced that with Pr[16.4+- 2 sigma]. For some reason i thought he plugged in 16.4 as |R-u| in Chebyshev's formula which makes no sense
10:33 why is standard deviation is less than 4?"
TjZBTDzGeGg,"And we're going to use some duct tape to help us think about just one piece of the wheel.
So I want you to just think about that piece of the wheel as the wheel comes flying over the top, and I blow on it like that.
What's going to happen to that one piece? It's going to go off that way, right? And the next piece is going to go off that way too.
So when it comes over, it has to go that way.
Let me do some ground f here just to be sure.
It's very powerful feeling.
Try it.
We need a demonstration.
I don't anybody think that I'm cheating, here.
So let's just twist it one way or the other.
So that's powerful pull, isn't it.
Alex is now never going to get the gyroscope wrong, because he's got the right representation.
So much of what you're going to accumulate in this subject is a suite of representations that will help you to build programs that are intelligent.
But I want to give you a second example, one a little bit more computational.
But one of which was very familiar to you by the time you went to first grade, in most cases.","I can't get the point at 6:00, could any one explain it for me? Thanks."
TjZBTDzGeGg,"So I like to reinforce that by giving you a little puzzle.
Let's see, who's here? I don't see [? Kambe, ?] but I'll bet he's from Africa.
Is anyone from Africa? No one's from Africa? No? Well so much the better-- because they would know the answer to the puzzle.
Here's the puzzle.
How many countries in Africa does the Equator cross? Would anybody be willing to stake their life on their answer? Probably not.
Well, now let me repeat the question.
How many countries in Africa does the Equator cross? Yeah, six.
What happened is a miracle.
The miracle is that I have communicated with you through language, and your language system commanded your visual system to execute a program that involves scanning across that line, counting as you go.
And then your vision system came back to your language system and said, six.
And that is a miracle.
And without understanding that miracle, we'll never have a full understanding of the nature of intelligence.
But that kind of problem solving is the kind of problem solving I wish we could teach you a lot about it.
But we can't teach you about stuff we don't understand.
We [INAUDIBLE] for that.
That's a little bit about the definition and some examples.
What's it for? We can deal with that very quickly.
If we're engineers, it's for building smarter programs.
It's about building a tool kit of representations and methods that make it possible to build smarter programs.","Please can someone explain to me what he meant by ""Well so much the better--"" at17:39??"
TjZBTDzGeGg,"So it would be two to the fourth she says aggressively and without hesitation.
Yes, two to the fourth, 16 possibilities.
So we could actually draw out the entire graph.
It's small enough.
There's another position over here with the farmer, fox, goose, and grain.
And in fact that's the one we want.
And if we draw out the entire graph, it looks like this.
This is a graph of the situations and the allowed connections between them.
Why are there not 16? Because the other-- how many have I got? Four? 10? The others are situations in which somebody gets eaten.
So we don't want to go to any of those places.
So having got the representation, something magical has happened.","Can someone justify why there are 2^4 scenarios in the farm, fox, goose, grain problem? 9:08"
Tl_p5pgBsyM,"Well, it's still gets added.
Remember? We're only going to kill it when it gets extended.
So who's the shortest? So you were a step ahead, whoever asked me that question.
Who's the shortest? H also goes to E.
Absolutely right.
H also goes to E.
I was getting ahead of myself here.
H also goes to E with a length of extra 16.
So we've got 20 plus 16 is 36 plus 56 is 92.
All right.
That's good.
So who's the shortest? I is the shortest.
Do we extend it? No.
It's on the extended list.
All right.
Who's the shortest now? It's hard to read, but the shortest is this 67 on the G.
So we're done.
We win.
Our path is SBDIG.
Yeah, for A*.
It gets the right answer, right? No.
Unfortunately not.","At 47:19, from I to C, shouldn't the value be 162 and not 127 ?"
Tw1k46ywN6E,"AUDIENCE: And you want to add all the weights.
PROFESSOR: And you want to add all the weights, right.
You're exactly right.
I have two more Frisbees, but I need to use them for something else.
So you get one next time.
Or do I have more than that? No, no.
These are precious Frisbees here.
Sorry, man.
And you corrected me, too.
Shoot.
This is sad.
So that needed to be a Wr.
But you also need to add up all of the nodes that are in here, because they're one more level.
And now you see why I made the mistake.
I don't usually make mistakes.
But what I really want-- actually it's more like-- I don't know, a few per lecture.
Constant order one mistakes.
I'm going to say this is Wi, j, where Wi, j is simply the sum of all of the weights from i to j.
And this makes perfect sense, because the nice thing is that I don't even need to put an r in there.
I could choose a particular r.
Wr will be in there.
But all of the other nodes are going to be in there anyway.
So it doesn't really matter what the r selection is corresponding to this term here.
I will put it inside the minimization.
This bracket here is closed with that, but you can pull that out, because that doesn't depend on r.",why we need to add all of the node (51:40) i though we need add wr (i<= r <= j)
Tw1k46ywN6E,"And we'll talk about that, as to why the greedy algorithm doesn't quite work.
So it's kind of similar to the interval scheduling and the weighted interval scheduling problem that we had back in February, where the regular interval scheduling problem, greedy worked, earliest finish time worked.
But when it came to weights, we had to graduate to dynamic programming.
So here's our second problem, optimal BSTs.
So what is an optimal BST? We have a bunch of keys that we want to store in the BST, K1 through Kn.
And we'll assume that the way this is set up is that K1 is less than K2, da, da, da, less than Kn.","First example question: the pseudocode shown at 22:33 only works with contiguous palindromes, am I wrong? Whereas the brain teasers at the start werent necessarily contiguous e.g. rotator out of turboventilator I do not see how the code can ever figure out rotator if you start it with L(0,end) Also I dont see why nr. Of subproblems is n squared"
U1JYwHcFfso,"It may sound difficult, but in fact, there's a pretty simple way, which are called AVL trees, that maintain balance in a particular way called height balance.
This is if we take the height of node.left-- actually, I'd prefer to-- node.right, minus height of node.left, so this thing is called skew of the node.
I want this to always be minus 1, 0, or plus 1.
So this is saying that if I have any node, and I look if its left subtree and its right subtree, I measure their heights-- remember, that's downward distance, maximum distance to a leaf-- I measure the height of this tree-- maximum height-- and I measure the maximum height of this subtree, I want these to be within 1 of each other.","I'm sort of confused on the definition of skew. I tried using the formula that he wrote on the blackboard at 35:03 { skew(node) = height(node.right) - height(node.left) }. For example if I have this tree and I calculate the skew with the formula, then this is what I would get: Z (height: 2, skew: 1 - 0 = 1) \ Y (height: 1, skew: 0 - 0 = 0) \ X (height: 0, skew: 0 - 0 = 0) and that doesn't really make sense to me because that would imply it's balanced since all of its node have a skew within {-1, 0, 1}. I would think this tree should be unbalanced and skewed to the right by 2? Just by looking at it, I would intuitively think the skews should be something like this: Z (height: 2, skew: 2) \ Y (height: 1, skew: 1) \ X (height: 0, skew: 0) Which picture is right? Because if its the case that the 2nd picture is right, then I'm really not sure whats the formula for it. Or what am I doing incorrectly here?"
U1JYwHcFfso,"Now, both x and y are height balanced.
That's case one.
In case two, the skew of y is flat, which means that this is a k, and this is a k, and this is a k plus 1, and this is a k plus 2.
But still, all the nodes are balanced-- height balanced.
They're still plus or minus 1.
So those are the easy cases.
Unfortunately, there is a hard case-- case three.
But there's only one, and it's not that much harder.
So it's when skew of y is minus 1.
In this case, we need to look at the left child of y.
And to be alphabetical, I'm going to rename this to z.
So this one, again, is double right arrow.
This one is now left arrow.
And this is letter y.
And so we have A, B, C, and D potential subtrees hanging off of them.
And I'm going to label the heights of these things.
These are each k minus 1 or k minus 2.
This one's k minus 1.
And now, compute the inside.
So this is going to height k for this to be left leaning.
So this is k plus 1, and this is k plus 2.
But the problem is this is 2 higher than this.
The height of z is 2 higher than the height of A.
This case, if I do this rotation, things get worse, actually.
I'll just tell you the right thing to do is-- this is the one thing you need to memorize.
And let me draw the results.
You can also just think of it as redrawing the tree like this.
But it's easier from an analysis perspective to think about it as two rotations.
Then we can just reduce.
As long as we know how rotations work, then we know that this thing works-- ""works"" meaning it preserves traversal order and we can maintain all the augmentations.
So now, if I copy over these labels-- the height labels-- I have k minus 1.
I have, for these two guys, k minus 1 or k minus 2.
The biggest one is k minus 1.
This is k minus 1.
And so this will be k.
This will be k.
This will be k plus 1.",51:54 maybe it should be skew(z) = -1 instead of skew(y) = -1 after the renaming
UHBmv7qCey4,"And to get the weights for the next generation, all I have to do is scale them so that they equal half.
This was not noticed by the people who developed this stuff.
This was noticed by Luis Ortiz, who was a 6.034 instructor a few years ago.
The sum of those weights is going to be a scaled version of what they were before.
So you take all the weights for which this new classifier-- this one you selected to give you the minimum weight on the re-weighted stuff-- you take the ones that it gives a correct answer for, and you take all of those weights, and you just scale them so they add up to 1/2.
So do you have to compute any logarithms? No.
Do you have to compute any exponentials? No.
Do you have to calculate z? No.
Do you have to calculate alpha to get the new weights? No.
All you have to do is scale them.
And that's a pretty good thank-god hole.
So that's thank-god hole number one.
Now, for thank-god hole number two, we need to go back and think about the fact that were going to give you problems in probability that involve decision tree stumps.
And there are a lot of decision tree stumps that you might have to pick from.
So we need a thank-god hole for deciding how to deal with that.
Where can I find some room? How about right here.
Suppose you've got a space that looks like this.","While telling the advantages of Thank God hole number 1 around 46:00, The professor mentioned that we don't need to compute logarithms and also we don't need to compute alphas. I don't understand why alphas are not required since we will need alphas to get the final answer since H(x) is a weighted sum of h(x) and the weights are alphas so I think we need to compute alphas anyhow. Can someone please tell me where I am missing ?"
UHBmv7qCey4,"Here's one-- h.
And it produces either a minus 1 or a plus 1.
So that's how the classification is done.
If it's coffee, plus 1.
If it's tea, minus 1.
Is this chalk, plus one.
If it's a hand grenade, minus 1.
So that's how the classification works.
Now, too bad for us, normally the world doesn't give us very good classifiers.
So if we look at the error rate of this classifier or any other classifier, that error rate will range from 0 to 1 in terms of the fraction of the cases got wrong on a sample set.
So you'd like your error rate to be way down here.
You're dead if it's over there.
But what about in the middle? What if it's, say, right there.
Just a little bit better than flipping a coin.
If it's just a little bit better than flipping a coin, that's a weak classifier.","At 4:00 he said if error rate is towards 1 , we are dead...actually not true. It would mean that every classification is wrong and simply inverting that terrible classifier will make it an awesome classifier. But Amazing video, learnt so much, FILLED with Aha moments ! :D"
UHBmv7qCey4,"It would happen to be wrong, but it's a valid test with a valid outcome.
So that's how we double the number of test that we have lines for.
And you know what? can even have a kind of test out here that says everything is plus, or everything is wrong.
So for each dimension, the number of decision tree stumps is the number of lines I can put in times 2.
And then I've got two dimensions here, that's how I got to twelve.
So there are three lines.
I can have the pluses on either the left or the right side.
So that's six.
And then I've got two dimensions, so that gives me 12.
So that's the decision tree stump idea.
And here are the other decision tree boundaries, obviously just like that.
So that's one way can generate a batch of tests to try out with this idea of using a lot of tests to help you get the job done.
STUDENT: Couldn't you also have a decision tree on the right side? PATRICK WINSTON: The question is, can you also have a test on the right side? See, this is just a stand-in for saying, everything's plus or everything's minus.
So it doesn't matter where you put the line.
It can be on the right side, or the left side, or the bottom, or the top.
Or you don't have to put the line anywhere.
It's just an extra test, an additional to the ones you put between the samples.
So this whole idea of boosting, the main idea of the day.
Does it depend on using decision tree stumps? The answer is no.
Do not be confused.
You can use boosting with any kind of classifier.
so why do I use decision tree stumps today? Because it makes my life easy.
We can look at it, we can see what it's doing.
But we could put bunch of neural nets in there.
We could put a bunch of real decision trees in there.","At 16:25 doesnt the orange line at the bottom symbolize the exact same thing as the orange line at the very left? Both say ""Everything is +"" or ""Everything is -"". And then we dont have 12 classifiers but only 10."
UHBmv7qCey4,"So I want the alpha over here.
That'll be sort of a byproduct of picking that test.
And with all that stuff in hand, maybe that will be enough to calculate Wt plus 1.
So we're going to use that classifier that we just picked to get some revised weights, and then we're going to go around that loop until this classifier produces a perfect set of conclusions on all the sample data.
So that's going to be our overall strategy.
Maybe we've got, if we're going to number these things, that's the fourth big idea.
And this arrangement here is the fifth big idea.
Then we've got the sixth big idea.
And the sixth big idea says this.
Suppose that the weight on it ith sample at time t plus 1 is equal to the weight at time t on that same sample, divided by some normalizing factor, times e to the minus alpha at time t, times h at time t, times some function y which is a function of x, But not a function of time.",How did that formula 27:30 come from?
UroprmQHTLc,"This defense is good against that virus.
All right, so each virus, I have a defense for.
So an example would be-- these are, by the way, dated viruses, but that's when the slides were made.
So against the Mydoom virus, you could use Defender, Microsoft Defender.
Against the ILOVEYOU virus, you could use Norton.
Against the Bablas virus, you could use ZoneAlarm.
Well, is that what we want? It's expensive.
It means that for every different virus, I need a different defense.
I have to spend a fortune on software.
This is not what we want.
So that's when for every virus, there's a defense, but the quantifiers are in the wrong order.
Let's reverse them.
Suppose I tell you that there's one defense that's good for all viruses.
There is a defense such that for every virus, d protects against v.
For example, if d is MITviruscan, then it would be wonderful if it was true that d protects against all viruses, there's one defense good against every attack.
That's what we want because it's a lot cheaper.
All right, let's start looking at a concrete mathematical example.","At 6:48, he said for every virus there needs to be a different defense. Why does it need to be a different defense? It says ""for all viruses there exist some defense such that the defense protects against the virus"". Couldn't the defense be the same for all the viruses? Ex: The viruses MYDOOM, ILOVEYOU, BABLAS will get protected by NORTON."
UroprmQHTLc,"Well, let's shift again, the point being here both we're looking at alternate quantifiers, and we're understanding that the truth of a proposition with quantifies depends crucially on the domain of discourse.
If we let the domain of discourse be the negative reals, then what this is saying is that for every negative real, there's a bigger negative real.
And that, of course, is true.
Because if you give me a negative real r, then r over 2, because it's negative, is actually bigger than r.
And it will not be positive if r isn't positive.
So sure enough, G in this case is true.
All right, let's reverse the qualifiers and see what happens.
It's worth thinking about.
So let's call H the assertion that for every y-- sorry, that there exists a y such that for every x, x is less than y.",10:32 how r over 2 will not be positive?
VJzv6WJTtNc,"And it's not clear with all that common basis for returning what value they have.
It's not immediately obvious that they're independent, but as a matter of fact they are.
And again this is absolutely something that you should check out.
If you don't stop the video now to work it out, you should definitely do it afterward.
It's an important little exercise and it's easy to check.
All you have to do is check that the probabilities of the event of odd number of heads in the event all match are independent as events.
Or you could use the random variable definition and check that these two random variables were independent by checking 4 equations because this can have values 0 and 1.",11:51 Pr(O)=1/2 while Pr(O|M)=1!=Pr(O) since M means 111. So I conclude that these two events are not independent. Am I wrong? Is there a mistake?
VYZGlgzr_As,"And then we've got flow conservation.
And the important thing here is that I don't have it for all V, but I do have it for vertices, V, that are not the source or the sink.
And I'm going to require f(u,v) equals 0, right? And the last one, which I haven't talked about, but becomes easy to talk about, given this constraint, is skew symmetry.
So if you take-- this doesn't have to be an edge between u and v.
Now, I'm talking about the flow, f, between u and v.
And so the u could be s, which is the source.
v could be t, which is the sink.
So in general, I'm not talking about a flow.",How does the sum at 28:40 even work? The sum of flow values outgoing from all vertices except source and sink equals zero?
VYZGlgzr_As,"SRINIVAS DEVADAS: d, b is going out.
That's exactly right, d, b.
And the plus 2, it would be d, T, right? And so you have to do the enumeration.
It's worthwhile doing once.
And then it gets kind of boring.
We won't to do it again.
But you have to realize that you have to absolutely look at every pair of vertices.
And you have to use skew symmetry and ensure that, even though there's actually no edge going out, if there's an edge coming in, you've got to count that.
And that's going to get a negative.
Whatever is coming in, you've got to subtract, OK? So it's not that complicated.
Yeah, go ahead.
STUDENT: Do we not consider S, c? SRINIVAS DEVADAS: I'm sorry? STUDENT: Do we not consider S, c? SRINIVAS DEVADAS: So the beauty of this is that, when you don't have a particular edge from S to c, you can use skew symmetry to argue that S, c and c, S cancel out each other, all right? So that's the good part, right? And thanks for asking the question.
That's a good question.
All right.
Here you go.
So you can do that by just looking at the edges.
And you can add up the numbers, all right? And so I don't think this is going to be absolutely crucial to understand the rest of the lecture.
Keep this in mind, that there's a process by which you define the value of a cut.
And we're going to get back to this, when we prove the max-flow min-cut theorem next time.","at 56:14 , one student asked a question about why don't we consider path s -> c. Sirinivas said when you don't have a particular edge from s to c, you can use skew symmetry to argue s->c and c->s cancelled out each other. I don't get his answer. We are considering f(S, T), why does c->s get involved and cancel s -> c?"
VYZGlgzr_As,"So what I've done here is invoke, essentially, this, except it's not exactly that, in the sense that it's written a little bit differently.
But if you see what's going on here, what I've done is look at this s, and I've said, think of this s as being cap V minus s.
Right? So that gives you s.
And those are clearly disjoint, right? Those are clearly disjoint sets.
There is this one and this one are disjoint sets.
That's what I mean to say.
I mean, these two aren't disjoint, but this and that are disjoint.
And that's what you need, in order to invoke the little property that you have here.
And so that all make sense? You see why I did that? OK? What can I say about either of these two quantities? Can I say something about either of these two quantities? Yeah? STUDENT: f(V,V) is 0.","at 42:14 why f(s,V)=f(V,V)-f(V-s,V)?"
VYZGlgzr_As,"So you can see that, pretty quickly, it gets kind of confusing, if we end up having these cycles that are such simple cycles, especially the ones where you have Su and Us, OK? And so we're going to disallow cycles of two kinds, right? The first cycle we're going to disallow is this simple one where we say, if I have a, then no self-loop edges allowed.
So that would involve accumulation at a particular node.
And it's going to make things really confusing.
And CLRS disallows that.
And most flow network algorithms assume that you're just going to discard these cycles.
This particular transformation that I'm going to describe here is something that is going to be forced on you.
And this is for your benefit in over 6 lectures and sections.
But it's not actually something that CLRS follows.","What was the point of disallowing self edges and cycles (u,v), (v,u) at 22:58? I'm not really sure I understand the difference between net-flow and positive-flow and the impact of the professor's assumption on the following definitions/constraints and therefore proof."
VYZGlgzr_As,"You know, dump the source on the left, and dump the sink on the right.
And you compute the flow the way we've defined it.
That's the flow.
It doesn't matter how you partition these vertices, as long as you've got the source on the left and the sink on the right, OK? And so we can prove this using implicit summation notation.
We'll do that.
And that'll give you a really good sense of why this statement is true, because we know that, for any given cut, the flow that cut is bounded by the capacity of that cut, right? You know that.
But to show this, here's how we could show that, f(S, T) is f(S, V) minus f(S, S), OK? So I'm playing around, just like I did before.
I had taken the cap T-- I know that S union T is cap V, right? This is a partition.
So I know that S union T is cap V.
So I can put a V here and an S here, right? And that's a subtraction over there, of course, right? So put that up here and finish this, a couple more lines.",The proof at 1:03:00 seems a bit odd. he has broken up the RHS term in the parenthesis into 2 parts( but the rules he gave earlier break the LHS term in the parenthesis )
V_TulH374hw,"Destination becomes source.
Source becomes destination.
And I'll add that into the graph.
Nice and easy, straightforward to do.
And this is kind of nice because, in a graph, I don't have any directionality associated with the edge.
I can go in either direction.
I just created something like that.
And you might say, well, wait a minute.
Why did I pick graph to be a subclass of digraph? Why not the other way around? Reasonable question, and you actually know the answer.
You've seen this before.
One of the things I'd like to have is the property that if the client code works correctly using an instance of the bigger type, it should also work correctly when it is using an instance of the subtype substituted in, which is another way of saying anything that works for a digraph will also work for a graph, but not vice versa.","I think at 29:00 it should say, ""anything that works for a graph will also work for a digraph"", not the other way round."
VrMHA3yX_QI,"If we rerun that, we see that it trains itself up very fast.
So we seem to be still close enough to a solution we can do without one of the neurons in each column.
Let's do it again.
Now it goes up a little bit, but it quickly falls down to a solution.
Try again.
Quickly falls down to a solution.
Oh, my god, how much of this am I going to do? Each time I knock something out and retrain, it finds its solution very fast.
Whoa, I got it all the way down to two neurons in each column, and it still has a solution.
It's interesting, don't you think? But let's repeat the experiment, but this time we're going to do it a little differently.
We're going to take our five layers, and before we do any training I'm going to knock out all but two neurons in each column.
Now, I know that with two neurons in each column, I've got a solution.
I just showed it.
I just showed one.
But let's run it this way.
It looks like increasingly bad news.
What's happened is that this sucker's got itself into a local maximum.
So now you can see why there's been a breakthrough in this neural net learning stuff.
And it's because when you widen the net, you turn local maxima into saddle points.
So now it's got a way of crawling its way through this vast space without getting stuck on a local maximum, as suggested by this.
All right.
So those are some, I think, interesting things to look at by way of these demonstrations.
But now I'd like to go back to my slide set and show you some examples that will address the question of whether these things are seeing like we see.
So you can try these examples online.
There are a variety of websites that allow you to put in your own picture.
And there's a cottage industry of producing papers in journals that fool neural nets.
So in this case, a very small number of pixels have been changed.
You don't see the difference, but it's enough to take this particular neural net from a high confidence that it's looking at a school bus to thinking that it's not a school bus.",So I get that at 44:25 the left picture is a school bus but can someone explain to me what's in the right picture?
VrMHA3yX_QI,"Let's see, it might look like this.
There's a [? summer.
?] There's a minus 1 up here.
No.
Let's see, there's a minus 1 up-- [INAUDIBLE].
There's a minus 1 up there.
There's a multiplier here.
And there's a threshold value there.
Now, likewise, there's some other input values here.
Let me call this one x, and it gets multiplied by some weight.
And then that goes into the [? summer ?] as well.
And that, in turn, goes into a sigmoid that looks like so.
And finally, you get an output, which we'll z.","Can someone explain what he is doing with -1 and the threshold value at 25:49? I watched his previous lecture 12a, but I still don't really understand how he can get rid of thresholds by doing that."
WO6vQJ6Rhm8,"You start saying, aha, that sentence sounds like recursion.
And that's right.
That's how we're going to solve this problem.
OK? So, in particular, here's going to be our thing.
I'm going to do a slightly different one in the solution-- so you guys should all be vigilant-- which is I'm going to write x ij to be the min work to convert.
I'm not a Python programmer, but hopefully I got this right.
i colon is going to be everything from i to the end of the file.
So in other words, this is the suffix version of our problem-- and into B j colon, like that.
OK.","If I had a document A, and I create document B as a copy of A with the first line moved to the end, won't continuous swaps and incrementing line index i, j by one give me all swaps? Here both the lines after swap would not match as indicated in condition 3. Edit: This was cleared around 51:10"
WPSeyjX1-4s,"And here's the way to think about it recursively.
I want to move a tower of size n, I'm going to assume I can move smaller towers and then it's really easy.
What do I do, I take a stack of size n minus 1, I move it onto the spare one, I move the bottom one over, and then I move a stack of size n minus 1 to there, beautiful, recursive solution.
And how do I move the smaller stack? Just the same way, I just unwind it, simple, and, in fact, the code follows exactly that.
OK, I do a little [INAUDIBLE] domain up here to try and get your attention, but notice by doing that what did I do? I asked you to think about it recursively, the recursive solution, when you see it, is in fact very straightforward, and there's the code.",Does anyone else have the same trouble understanding the Hanoi code at 23:13?
WPSeyjX1-4s,"And you can see now it says, if either x is equal to 0 or x is equal to 1 I'm going to return 1, otherwise, reduce it to two simpler versions of the problem but with different arguments, and I add them up.
OK, and if we go look at this, we can actually run this, if I can find my code.
Which is right there, and I'm just going to, so we can, for example, check it by saying fib of 0.
I just hit a bug which I don't see.
Let me try it again.
I'll try it one more time with fib of 0.
Darn, it's wrong, let me try it.
I've got two different versions of fib in here, that's what I've got going on.","If you're quick - at around 28:46 you can see on line 165 that he has a case where n == 1. You can't see the code below, but it will be as his slide shows, computing n - 1 and n - 2. So, as he enters 0 (zero), and as his base case of n ==1 doesn't match, he keeps subtracting from n going negative and recurses to death. What I have learned though is that you can have two functions with the exact same signature in a module (who knew!), and that in this case, Python chooses to call the latter one!"
WPSeyjX1-4s,"But notice, every other thing I do here, I've actually computed those values.
I'm wasting measures, or wasting time, it's not so bad with fib of 5, but if this is fib of 20, almost everything on the right hand side of this tree I've already computed once.
That means fibs very inefficient.
I can improve it by using a dictionary, very handy tool.
I'm going to call fib not only with a value of n, but a dictionary which initially I'm going to initialized to the base cases.
And notice what I do, I'm going to say if I've already computed this, just return the value in the dictionary.
If I haven't, go ahead and do the computation, store it in the dictionary at that point, and return the answer.","I loved that the recursion Fibonacci code(inefficient one) has some flaws for its optimization but I couldn't understand how the modified dictionary becomes the argument for the fibo_efficient(n, d) I mean we modified 'd' but was already given to the fibo_efficient(n, d) as it's an argument? 46:40
Thanks! This lecture about recursion was great. Its purpose was well defined and elaborated. Did anyone spot the ""code error"" regarding the base -case at 46:58 by the way?
There is one mistake. Shouldn't Fib(2) be equal to 1. That is at 46:58 the d should be d = {1:1, 2:1} instead of d = {1:1, 2:2}. Please check.
46:58 so what if you want fib_efficient(0, d)?"
WPSeyjX1-4s,"I say I need to prove that if it's true for an arbitrary value of n, I'm just going to prove that it's also then true for n plus 1.
And if I can do those two things I can then conclude for an infinite number of values of n it's always true.
Then we'll relate it back to programming in a second, but let me show you a simple example of this, one that you may have seen.
If I had the integers from 0 up to n, or even from 1 up to n, I claim that's the same as n times n plus 1 over 2.
So 1, 2, 3, that's 6, right.
And that's exactly right, 3 times 4, which is divided by 2, which gives me out 6.
How would I prove this? Well, by induction? I need to do the simple cases if n is equal to 0, well then this side is just 0.
And that's 0 times 1, which is 0 divided by true.
So 0 equals 0, it's true.
Now the inductive step.
I'm going to assume it's true for some k, I should have picked n, but for some k, and then what I need to show is it's true for k plus 1.
Well, there's the left hand side, and I want to show that this is equal to that.
And I'm going do it by using exactly this recursive idea, because what do I know, I know that this sum, in here, I'm assuming is true.
And so that says that the left hand side, the first portion of it, is just k times k plus 1 over 2, that's the definition of the thing I'm assuming is true.
To that I'm going to add k plus 1.
Well, you can do the algebra, right? That's k plus 1 all times k over 2 plus 1, which is k plus 2 over 2.
Oh cool, it's exactly that.
Having done that, I can now conclude this is true for all values of n.",Why did Prof. Grimson add k+1 to k(k+1)/2 at 19:07 ?
WPSeyjX1-4s,"If it's either 0 or 1 long, it's a palindrome.
Otherwise you could think about having an index at each end of this thing and sort of counting into the middle, but it's much easier to say take the two at the end, if they're the same, then check to see what's left in the middle is a palindrome, and if those two properties are true, I'm done.
And notice what I just did I nicely reduced a bigger problem to a slightly smaller problem.
It's exactly what I want to do.
OK? So it says to check is this, I'm going to reduce it to just the string of characters, and then I'm going to check if that's a palindrome by pulling those two off and checking to see they're the same, and then checking to see if the middle is itself a palindrome.
How would I write it? I'm going to create a procedure up here, isPalindrome.
I'm going to have inside of it two internal procedures that do the work for me.
The first one is simply going to reduce this to all lowercase with no spaces.
And notice what I can do because s is a string of characters.
I can use the built in string method lower, so there's that dot notation, s.lower.
It says.
apply the method lower to a string.
I need an open and close per end to actually call that procedure, and that will mutate s to just be all lowercase.
And then I'm going to run a little loop, I'll set up answer or ans to be an empty string, and then, for everything inside that mutated string, I'll simply say, if it's inside this string, if it's a letter, add it into answer.
If it's a space or comma or something else I'll ignore it, and when I'm done just return answer, strips it down to lowercase.
And then I'm going to pass that into isPal which simply says, if this is either 0 or 1 long, it's a palindrome, returned true.
Otherwise, check to see that the first and last element of the string are the same, notice the indexing to get into the last element, and similarly just slice into the string, ignoring the first and last element, and ask is that a palindrome.
And then just call it, and that will do it.
And again there's a nice example of that in the code I'm not going to run it, I'll let you just go look at it, but it will actually pull out something that checks, is this a palindrome.
Notice again, what I'm doing here.
I'm doing divide-and-conquer.
I'm taking a problem reducing it, I keep saying this, to a simpler version of the same problem.","typo at 31:07, he has leba in [-4:] instead of elba.
at 32:30 at the one before the last line shouldn't there be ""and isPal(s[1:-2])"" , -2 instead of -1 ?
Small typo in slide 38 at 31:00 ; ""leba"" instead of ""elba"" was written in the condensed string."
WQHOImO0pX0,"And I'm going to say that s HALTS-- the string has this property called halting or HALTS-- if and only if this procedure P that s describes returns successfully when it's applied to s.
This is where we're really doing a diagonal argument.
We're taking the sth object-- the procedure that's described by s and applying it to s.
And that's kind of going down the diagonal of s applied to s, where the n-th element of the n-th row in a pictorial diagonal argument.
That's the idea that we're going here.
But let's to go back to the definition.
A string is said to HALT if when you interpret it as the description of a procedure that takes a string argument and you apply that string procedure to that very same thing, s, you successfully return.","The assumptions given throughout this lecture (listed below) seem non-sensical. Someone please help! #1. An infinite string is computable. (1:02) - The only definition provided for ""computable"" pertains to portions infinite strings. Most computing involves finite objects, and even when ""computing"" involves irrational numbers, we never need to compute all of the digits. Why does this definition of ""computable"" make sense? #2. There is an (countably) infinite number of procedures. (2:10) - In the world we program in, with restrictions on size, how can there be an infinite number of procedures? A big finite number, yes, but infinite? #3. There are uncountably many infinite binary strings that are non-computable. (2:56) - Yes, there are uncountably many irrational numbers. Most un-""computable"" per previous definition. What does that have to do with real-world computers halting? #4. HALT is described by a program returning when it is given it's own source code as input. (Slide 7, 5:26) - ?? This is, again, a very strange definition for HALT. Why is this input the only case considered? #5. A type-checking program C exists based only on program text s. (10:18) - Is there any perfect type-checking procedure that can exist without knowing anything about the inputs to s? These explanations seem very imprecise and contrived. What am I missing?"
WQHOImO0pX0,"It's impossible to write a procedure that determines of strings whether they describe a procedure that HALTS when applied to itself.
OK.
That at least gives us some concrete problem that we can say is not something that a computer can do.
Even though it's a very well defined clear question, it's just not possible to get a computing procedure that will on an arbitrary string, figure out the right answer.
Any program that applied to strings is trying to do this job, either it will give the wrong answer.
Or if it never gives a wrong answer, it means it doesn't give an answer at all on some strings.
All right.","The assumptions given throughout this lecture (listed below) seem non-sensical. Someone please help! #1. An infinite string is computable. (1:02) - The only definition provided for ""computable"" pertains to portions infinite strings. Most computing involves finite objects, and even when ""computing"" involves irrational numbers, we never need to compute all of the digits. Why does this definition of ""computable"" make sense? #2. There is an (countably) infinite number of procedures. (2:10) - In the world we program in, with restrictions on size, how can there be an infinite number of procedures? A big finite number, yes, but infinite? #3. There are uncountably many infinite binary strings that are non-computable. (2:56) - Yes, there are uncountably many irrational numbers. Most un-""computable"" per previous definition. What does that have to do with real-world computers halting? #4. HALT is described by a program returning when it is given it's own source code as input. (Slide 7, 5:26) - ?? This is, again, a very strange definition for HALT. Why is this input the only case considered? #5. A type-checking program C exists based only on program text s. (10:18) - Is there any perfect type-checking procedure that can exist without knowing anything about the inputs to s? These explanations seem very imprecise and contrived. What am I missing?"
WQHOImO0pX0,"ROFESSOR: [? Diagonal ?] arguments are elegant, and infinite sets-- some people think-- are romantic.
But you could legitimately ask what is all this weird infinite stuff doing in a course that's math for computer science? And the reason is that diagonal arguments turn out to play a fundamental role in the theory of computing.
And what we're going to talk about now is the application of diagonal arguments to show that there are noncomputable sets and examine a particular one.
So let's look at the class of infinite binary strings.
Now, we've seen that there are an uncountable number of infinite binary strings, and that's because there was a simple bijection between the infinite binary strings and the subsets of the natural numbers-- that is the power set of n.
Let's look at the infinite binary strings that we might think of and call computable strings.
And what I mean by a computable string is that there's simply a procedure that will tell me what its digits are.","The assumptions given throughout this lecture (listed below) seem non-sensical. Someone please help! #1. An infinite string is computable. (1:02) - The only definition provided for ""computable"" pertains to portions infinite strings. Most computing involves finite objects, and even when ""computing"" involves irrational numbers, we never need to compute all of the digits. Why does this definition of ""computable"" make sense? #2. There is an (countably) infinite number of procedures. (2:10) - In the world we program in, with restrictions on size, how can there be an infinite number of procedures? A big finite number, yes, but infinite? #3. There are uncountably many infinite binary strings that are non-computable. (2:56) - Yes, there are uncountably many irrational numbers. Most un-""computable"" per previous definition. What does that have to do with real-world computers halting? #4. HALT is described by a program returning when it is given it's own source code as input. (Slide 7, 5:26) - ?? This is, again, a very strange definition for HALT. Why is this input the only case considered? #5. A type-checking program C exists based only on program text s. (10:18) - Is there any perfect type-checking procedure that can exist without knowing anything about the inputs to s? These explanations seem very imprecise and contrived. What am I missing?"
WQHOImO0pX0,"So what I mean is that the procedure applied to argument n will return the n-th digit of the string s.
That's the definition of what I mean by saying s is computable.
I can compute its digits, whichever digits are needed.
Now, we saw that there were only a countable number of finite binary sequences, and I mention that now because I want to think about sequences over the slightly larger alphabet instead of 0 1, the 256 ASCII characters.
And by the same argument, the set of finite ASCII strings is also countable.
You just list them in order of length-- same argument that we used for the binary strings.
Now, the point of looking at the ASCII strings-- the 256 keyboard characters-- is that every procedure that we enter into a computer, we type in an ASCII string.
Every procedure can be represented by an ASCII string.
And since they're only countably many finite ASCII strings, it follows that there are only countably many computable procedures.
Now, since in order to be a computable infinite string, there has to be a procedure which computes is digits, we can immediately conclude that there are only countably many infinite binary sequences that are computable-- only countably many computable infinite binary sequences.
But I already said there are an uncountable number of those infinite binary sequences.
So it has to be that there are noncomputable sequences, noncomputable infinite binary strings, that exist.
So we can conclude that as a matter of fact, since the set of infinite binary strings is uncountable and the computable ones are a countable subset, there have to be an uncountable number of noncomputable infinite binary sequences.
Most infinite binary sequences are actually noncomputable.
OK.
That's kind of abstract thing to know.","The assumptions given throughout this lecture (listed below) seem non-sensical. Someone please help! #1. An infinite string is computable. (1:02) - The only definition provided for ""computable"" pertains to portions infinite strings. Most computing involves finite objects, and even when ""computing"" involves irrational numbers, we never need to compute all of the digits. Why does this definition of ""computable"" make sense? #2. There is an (countably) infinite number of procedures. (2:10) - In the world we program in, with restrictions on size, how can there be an infinite number of procedures? A big finite number, yes, but infinite? #3. There are uncountably many infinite binary strings that are non-computable. (2:56) - Yes, there are uncountably many irrational numbers. Most un-""computable"" per previous definition. What does that have to do with real-world computers halting? #4. HALT is described by a program returning when it is given it's own source code as input. (Slide 7, 5:26) - ?? This is, again, a very strange definition for HALT. Why is this input the only case considered? #5. A type-checking program C exists based only on program text s. (10:18) - Is there any perfect type-checking procedure that can exist without knowing anything about the inputs to s? These explanations seem very imprecise and contrived. What am I missing?"
WcaMiqJR09s,"Or I guess the loss is maybe 1.5.
Sorry, the margin is 0.5, but the loss is 1.5.
So now we can also compute the gradients here.
So the loss on the first point is 0 because the loss is 0.
And generally, not always, if the loss is 0, then the gradient will be 0 as well.
And on the second one, the loss is not 0, so we have a non-zero gradient, which is minus phi of xy.
So it's this part times this minus sign.
And the third point also has positive loss, so it has a positive gradient, 1, minus 1.","i guess there is an error at 21:31. hinge loss on [1,-1] calculates 0.5 i believe"
WcaMiqJR09s,"So therefore, we expect low loss.
Whereas this point over here-- minus 2, 0-- is labeled as positive, but it's on the other side of the decision boundary.
And therefore, it's classified incorrectly.
On this point-- 1, minus 1, minus 1 is over here.
And it's labeled in the training data as a minus and is on this side of the decision boundary.
So it's predicted as minus.
Therefore, it is labeled incorrectly as well.
So to formalize this, we're going to find something called the zero-one loss.
And just like any loss function, it takes in a particular example and a weight vector.
And it looks at the prediction and the target and says, do they disagree? And if they disagree, then this indicator function will return 1.
And if they agree, then the indicator function returns 0.
So this is a zero-one loss.
So mathematically, you can walk through these calculations.
You plug in the first point and you look at the sign.
The sign here is going to be 2.
The dot product of 2 is 1.
They don't disagree, so that's 0.
The second point, they do disagree, so the loss is 1.
And the third point, they also don't disagree.","It might just be me, but there's an error around 8:54. Prof. Liang states that (1, -1) was classified incorrectly, even though it was on the correct side of the decision boundary."
X0Jw4kgaFlg,"So if i doesn't equal j we're going to get a 0, and if i does equal j, well, then we're going to get the regular one variable derivative of the logistic function, which if I remember correctly, you were asked to compute-- now I can't remember whether it's assignment, 1 or assignment 2, but one of the two asked you to compute it.
So our Jacobian for this case looks like this.
We have a diagonal matrix with the derivatives of each element element along the diagonal and everything else is 0.
OK, so let's look at a couple of other Jacobians.
So if we are asking, if we've got this Wx plus b basic neural network layer and we're asking for the gradient with respect to x, then what are we going to have coming out is that that's actually going to be the matrix W.","at 23:29 the reason for non diagonal of jacobian =0 are a little unclear. The derivative at non diagonal positions are zero because h1 is output for input z1 and no other input. Hence h1 would change dh1/dz1 for change in z1 and 0 for change in others i.e. z2, z3"
X0Jw4kgaFlg,"Two other things on what's coming up.
Actually for our Thursday's lecture, we make a big change.
And Thursday's lecture is probably the most linguistic lecture of the whole class where we go through the details of dependency grammar and dependency parsing.
Some people find that tough as well, but at least it will be tough in a different way.
And then one other really good opportunity is this Friday, we have our second tutorial at 10:00 AM, which is an introduction to PyTorch, which is the deep learning framework that we'll be using for the rest of the class once we've gone through these first two assignments where you do things by yourself.
So this is a great chance to get an intro to PyTorch, it will be really useful for later in the class.",may i get the tutorial of pyTorch mentioned at 4:35 from somewhere?
XROTP1RiNaA,"And we'll start with versions of SAT because we like SAT.
So I'm just going to tell you that sharp 3SAT is hard.
First sharp SAT is hard, and the usual proof of SAT hardness shows sharp P completeness for sharp SAT.
And if you're careful about the conversion from SAT to 3CNF you can get sharp three satisfied.
It's not terribly interesting and tedious.
So I will skip that one.
So what about Planar 3SAT? I stared at this diagram many times for a while.
This is lecture seven for replacing a crossing in a 3SAT thing with this picture.
And all of this argument and this table in particular, was concluding that the variables are forced in this scenario.",Where is the lecture containing the problems and reductions that you begin referring to from around 17:00? Thanks.
XhyOAX6oSX4,"And if you take a different sample, the hypothesis might look very different.
And it still doesn't fit your data at all.
It's just capturing the noise, but maybe hasn't captured the signal.
So it's weighs a lot depending on the sample that you get.
But still it doesn't go through it so that it- it is kind of overfitting and underfitting at the same time.
But these are just heuristics, right? So what- what you, um, what you want to do in general, um, what- what we've discussed so far is- is- is mostly for you to build a mental model of what's going on under the covers, okay? But what you actually do in practice, the actions that you take, is what we're going to discuss next, and that's called cross-validation.","At 1:07:10 can we say it is underfitting the signal and overfitting the noise? Also, if it is underfitting the noise and overfitting the signal is that a good model?"
XhyOAX6oSX4,"And that, that's called bootstrap.
However, bootstrap is generally used for um, bootstrap is- is generally used to obtain um- what -what we saw previously as the sampling distribution for getting an -an- uh- a distribution over your parameters, right? And uh, it's not commonly used in machine learning where the goal is to do prediction on unseen examples.
Uh- it's, it's more of a technique to get um- uncertainty estimates or confidence intervals on your model parameters.
And that's more commonly used in- in- in more classical statistics settings where you want to get confidence intervals on your parameters.
Um, it doesn't- it doesn't help us a lot in terms of getting generalization error estimates.
All right.
Regularization.
So first let's, uh- we gonna have a quick, um- a quick motivation for regularization.","At 1:32:00 you said bootstrap is not used a lot in MAchine learning. It forms the basis for Random forests and the tree based algorithms, right?"
Xm5VrxZYhu4,"So it will be like two symmetric relations rather than a inverse relation.
So this is where, uh, DistMult fails [NOISE].
Um, and then the last place, uh, where, uh, DistMult fails is, uh, composition, uh, relations, right? And the problem is that DistMult defines hyperplane for, uh, each, uh, head, er, relation pair and the union of hyperplanes induced by this, er, by this composition of relations cannot be expressed by a single, uh, hyperplane.
So kind of, you know, a union or an intersection of hyperplanes is not a hyperplane, uh, anymore.
So that's why- th- that's the intuition why composition relations, uh, are also not possible.","Thank you for lecture! I think the proof of that DistMul can't model inverse relations is not correct. We have <h, r2, t> = <t, r1, h> for a specific h and t. Thus, the equation says that r1-r2 shoud belong to a specific hyperplane, however it doesn't mean r1 = r2. 26:51"
Xnpt8US31cQ,"And then again, you say, Aha, now I'm here, I wanna go deeper.
And you keep descending until you hit, uh, the, uh, individual cell, and this is when you, uh, when you stop and you put an edge.
Um, this means that you are going to land in a given, um, in a given cell exactly with the probability, um, uh, according to that cell.
The only difference is that you may get a couple of edges colliding, right? You may land to the same cell multiple times.
If that happens, just ignore it, uh, and try again.
And this gives you now a very fast way to generate a- a Kronecker graph, right? So, uh, basically, this, uh, edge-dropping or ball-dropping mechanism basically says, um, take the initia- initial, uh, matrix, whatever are the entries, uh, normalize them and then keep dropping in, um, in- keep descending in until you hit an individual cell and put an edge there, right? Put a value 1 there, and this would mean that you have connected nodes, uh, i and j that are in a column, uh, in the row i and column j.","13:40 Thanks for sharing. Fast Kronecker generator algorithm does not reduce the number of simulation required, rather it saves the memory of storing a huge probability matrix, by recursively descending the level from the initial probability matrix? So, it is about saving memory storage, rather than speed of simulation? It is only 'fast' when we stop at middle level, use single simulation as an approximation for the 'edge' or sub-matrix? Not sure if my understanding is correct. Thanks again for sharing."
Xv0wRy66Big,"And intuitively, the idea is that rather than summing here over all the nodes, uh, in the network, we are only going to sum over a subset of the nodes.
So essentially, we are going to sample a couple of negative examples and sum, uh, over them.
So the way the approximation works out is that, um, we, um, we can- we can view this as an approximation to the, um, to the- to the softmax function, where we can, uh, approximate it using, uh, the following, uh, expression.
We are going to take log sigmoid function of the dot product between u and v.
Uh, this is for the, uh, for the theorem here.
And then we are going to say minus sum of i going from one to k, so this is our k negative examples, logarithm, again, of the sigmoid function between the, uh, st- starting node u, and the negative, um, negative sample, uh, i, where this negative samples, this negative nodes will be, uh, sampled at random, but not at ra- at uniform random, but random in a biased way.
So the idea here is that, instead of normalizing with respect to all nodes in the network, we are going to normalize softmax against k random negative samples, so negative nodes, uh, from the network.
And this negative samples will be carefully chosen.
So how do we choose negative samples? We- we sample k negative nodes, each with probabil- probability proportional to its degrees.","Can you tell me why ""Higher k in sampling corresponds to higher bias on negative events""? at around 12:30"
ZA-tUyM_y7s,"But in this class, generally what we mean is polynomial.
And as you get down this thing, things are more and more efficient.
There's one class I'm going to talk to you about over here, which is something like-- let's do this-- 2 to the theta of n, exponential time.
This is some constant to a function of n that's, let's say, super linear, that's going to be pretty bad.
Why is it pretty bad? If I were to plot some of these things as a function of n-- let's say I plot values of up to 1,000 on my n scale here.",34:35: Y U NO properly label your axes!?
ZA-tUyM_y7s,"I could check those possibilities, but there's an even easier base case.
Yeah? There's an even easier base case than 1.
STUDENT: 0-- JASON KU: 0, right? After interviewing 0 students, I haven't done any work, right? Certainly, the first 0 can't have a match.
This inductive hypothesis this is true just because this initial predicate is false.
So I can say, base case 0-- check.
Definitely, this predicate holds for that.
OK.
Now we got to go for the meat of this thing.
Assume the inductive hypothesis true for K equals, say, some K prime.
And we're considering K prime plus 1.
Then we have two cases.
One of the nice things about abduction is that it isolates our problem to not consider everything all at once, but break it down into a smaller interface so I can do less work at each step.
So there are two cases.
Either the first K already had a match-- in which case, by our inductive hypothesis, we've already returned a correct answer.
The other case is the-- it doesn't have a match, and we interview the K plus 1th student-- the K prime plus 1th student.
If there is a match in the first K prime plus 1 students, then it will include K plus-- the student K prime plus 1, because otherwise, there would have been a match in the things before it.
So there are two cases.
If K contains match, K prime.
If first K contains match-- already returned by induction.
Else, if K prime plus 1 student's contains match, the algorithm checks all of the possibilities-- K prime checks against all students, essentially by brute force.
It's a case analysis.
I check all of the possibilities.
Check if birthday is in record-- I haven't told you how to do that yet, but if I'm able to do that, I'm going to check if it's in the record.
If it's in the record, then there will be a match, and I can return it.
Otherwise, I have-- re-establish the inductive hypothesis for the K prime plus 1 students.","22:18 it literally says in the subtitles ""one the nice things about abduction"" instead of induction XD"
ZA-tUyM_y7s,"That would take forever, right? Usually what we do when defining a problem is specify some kind of predicate, saying that, oh, we can check-- if I give you an input and an output, I can check whether that output is correct or not.
That's usually how we define a problem is, if I am checking for whether this index contains a 5, I can just go to that array, look at index 5, and-- or the index you gave me, and see if it equals 5.
So usually, we're putting it in terms of predicates because, in general, we don't really want to talk about small instances of problems.","6:07 He says, ""Probably if [there is more than 365 people] the answer is yes"", it's actually logically guaranteed that 100% of the time a group of more than 365 people have at least one shared birthdays because there are no other unique days to have been born on! It bugs me that he said ""probably"""
ZA-tUyM_y7s,"What does constant look like? Maybe this is 1,000 up here.
What does a constant look like? Looks like a line-- it looks like a line over here somewhere.
It could be as high as I want, but eventually, anything that's an increasing function will get bigger than this.
And on this scale, if I use log base 2 or some reasonable small constant, what does log look like? Well, let's do an easier one.
What does linear look like? Yeah, this-- that's what I saw what a lot of you doing.
That's linear.
That's the kind of base that we're comparing everything against.
What does log look like? Like this-- OK, but at this scale, really, it's much closer to constant than linear.",34:55 one of the axes in the graph is n what about the other one?
ZUZ8VbX1YNQ,"Now, the odds of finding an m that's not relatively prime to n are basically negligible because if you'd find such an m, it would enable you to factor them.
And we believe factoring is very hard.
But in fact, it actually works for all m, which is a nice theoretical results.
And you'll work this out in class problem.
OK.
That's how it works.
The receiver publishers e and n, keeps a secret key d.
The sender exponentiates the message to the power e.
The receiver simply decodes by raising the received message to the power d and reads off what the original was.
OK.
So we need to think about the feasibility of all of this because we believe that it's impossible to decrypt, but there's a lot of other stuff going on there that the players have to be able perform.
And let's examine what their responsibilities and abilities have to be.
So the receiver to begin with has to be able to find large primes.
And how on earth do they do that? Well, without going into too much detail, we can make the remark that there are lots of primes.","At 11:07, why all m can work. If gcd(m,n) = m, what should we do?"
ZUZ8VbX1YNQ,"That is to say by appealing to the prime number theorem, we know that among the n digit numbers, about log n of them are going to be primes so that you don't have to go too long before you stumble upon a random prime.
That is, if you're dealing with a 200 digit n and you're searching for a prime of around that size, you're not going to have to search more than a few hundred numbers before you're likely to stumble on a prime.
And of course, how do you know that you stumbled on a prime? Well, you need to be able to check whether a number is prime or not-- and efficientlY-- in order for this whole thing to be feasible.","at 11:55 isn't it more correct to say that for a range of x integers, x/ln(x) are prime. I think there was a mistake there...."
_3WDzxt5p8c,"You can check that P if and only if is true exactly when the complement of P XOR Q is true.
So now we come to two crucial properties of formulas called satisfiability and validity, and let's examine what those are.
So a formula is satisfiable if and only if it's true in some environment.
That is, it's satisfiable if there is some way to set the values of the variables P and Q to be truth values in such a way that the formula comes out to be true.
And a related idea is that a formula is valid-- it's also called a tautology-- if and only if it's true in all environments.
No matter what you set the variables to, it's going to come out to be true.
Let's look at some examples to solidify those two concepts.
So the formula P all by itself is satisfiable because it can be true if P is true, but it's not always true because P might be false.
Symmetrically, NOT P is also satisfiable because it could be true if P is false, but it's not always true.
It's not valid because P might be true, in which case not P would be false.
A formula that's not satisfiable, a formula that means that there is no truth value that makes it true, which is the same as saying that it's always false, is the formula P and NOT P.
It's probably the simplest not satisfiable formula or unsatisfiable formula.
So this is clearly false because either P or NOT P has got to be false and the [? and ?] will then will definitely come out to be false.
There is no value for P that makes this formula true.
It's unsatisfiable.
A valid formula, actually by De Morgan's law applied to the P and NOT P, is P or NOT P is going to be valid because no matter what truth value P has, it comes out to be true.","At roughly 6:30, Is this ""iff"" same as that in general mathematics. IE need to prove ""if p is true then q is true"" and then to prove ""if q is true then p is true"". Or are we at the stage where this is only mentioned and use the ""definition""?
6:24 there is a mistake on the truth table- F T = F but should've been T. if only the Professor was talking about implication-"
_PwhiWxHK8o,"And now, I'll just convert that whole works into a double sum over both i and j of alpha i times alpha j times y sub i times y sub j times x sub i dotted with x of j.
We sure went through a lot of trouble to get there, but now, we've got it.
And we know that what we're trying to do is we're trying to find a maximum of that expression.
And that's the one we're going to had off to the numerical analysts.
So if we're going to had this off to the numerical analysts anyway, why did I go to all this trouble? Good question.
Do you have any idea why I went to all this trouble? Because I wanted to find out the dependence of this expression.",He might have forgotten a transpose at Xi at 35:45
_PwhiWxHK8o,"And now, we've got to have a summation over all the constraints.
And each or those constraints is going to have a multiplier, alpha sub i.
And then, we write down the constraint.
And when we write down a constraint, there it is up there.
And I've got to be hyper careful here, because, otherwise, I'll get lost in the algebra.
So the constraint is y sub i times vector, w, dotted with vector x sub i plus b, and now, I've got a closing parenthesis, a minus 1.
That's the end of my constraint, like so.
I sure hope I've got that right, because I'll be in deep trouble if that's wrong.
Anybody see any bugs in that? That looks right.
doesn't it? We've got the original thing we're trying to work with.
Now, we've got Lagrange multipliers all multiplied.
It's back to that constraint up there, where each constraint is constrained to be 0.
Well, there's a little bit of mathematical slight of hand here, because in the end, the ones that are going to be 0, the Lagrange multipliers here.
The ones that are going to be non 0 are going to be the ones connected with vectors that lie in the gutter.
The rest are going to be 0.
But in any event, we can pretend that this is what we're doing.
I don't care whether it's a maximum or minimum.","At 25:26 why does the Patrick Winston say that only those Lagrange Multipliers , which are connected with vectors that lie in the gutter are going to be non zero whereas the rest will be zero?"
_PwhiWxHK8o,"And that's x sub i.
Those are the quantities I need in order to do it.
So that means that if I have a function, let's call it k of x sub i and x sub j, that's equal to phi of x sub i dotted with phi of x sub j.
Then, I'm done.
This is what I need.
I don't actually need this.
All I need is that function, k, which happens to be called a kernel function, which provides me with the dot product of those two vectors in another space.
I don't have to know the transformation into the other space.
And that's the reason that this stuff is a miracle.
So what are some of the kernels that are popular? One is the linear kernel that says that u dotted with v plus 1 to the n-th is such a kernel, because it's got u in it and v in it, the two vectors.","Very good lecture, clear explanation and good pace :) One correction: 44:30 (u*v+1)^n is a polynomial kernel, not a linear kernel."
_PwhiWxHK8o,"And this is what the dot product is in the other space.
So that's one choice.
Another choice is a kernel that looks like this, e to the minus.
Let's take the dot product of the difference of those two guys.
Let's take the magnitude of that and divide it by some sigma.
That's a second kind of kernel that we can use.
So let's go back and see if we can solve this problem by transforming it into another space where we have another perspective.
So that's it.
That's another kernel.
And so sure, we can.
And that's the answer when transformed back into the original space.
We can also try doing that with a so-called radial basis kernel.
That's the one with the exponential in it.
We can learn on that one.
Boom.
No problem.
So we've got a general method that's convex and guaranteed to produce a global solution.","44:36 quote the dot product is in another space. How? Why dot product in another space? Thank you if anyone could advice!
Solution found at 45:14 doesn't seem to apply the concept of adding an extra dimension (41:50). Instead, it seems to apply a non-linear ""road""; which looks great because it's simpler, but I'm confused."
_PwhiWxHK8o,"But if you're a positive sample, we're going to insist that this decision function gives the value of one or greater.
Likewise, if w thought it was some negative sample is provided to us, then we're going to say that has to be equal to or less than minus 1.
All right.
So if you're a minus sample, like one of these two guys or any minus sample that may lie down here, this function that gives us the decision rule must return minus 1 or less.
So there's a separation of distance here.
Minus 1 to plus 1 for all of the samples.
So that's cool.
But we're not quite done, because carrying around two equations like this, it's a pain.
So what we're going to do is we're going to introduce another variable to make like a little easier.
Like many things that we do, and when we develop this kind of stuff, introducing this variable is not something that God says has to be done.
What is it? We introduced this additional stuff to do what? To make the mathematics more convenient, so mathematical convenience.
So what we're going to do is we're going to introduce a variable, y sub i, such that y sub i is equal to plus 1 for plus samples and minus 1 for negative samples.
All right.
So for each sample, we're going to have a value for this new quantity we've introduced, y.
And the value of y is going to be determined by whether it's a positive sample or negative sample.
If it's a positive sample it's got to be plus 1 for this situation up here, and it's going to be minus 1 for this situation down here.","Why only 1 and why not 2...@9:07
9:10 why 1 or greater? I mean, why 1? Why does it have to be 1 to be beyond the margin?
could someone please explain to me why 8:40 to 9:30 is the way it is? I'm so lost on this step... why did it turn from >=0 to >= 1"
_PwhiWxHK8o,"I've lost track.
But what we're going to do is we're going to try to find an extremum of that.
So what do we do? What does 1801 teach us about? Finding the maximum-- well, we've got to find the derivatives and set them to 0.
And then, after we've done that, a little bit of that manipulation, we're going to see a wonderful song start to emerge.
So let's see if we can do it.
Let's take the partial of L, the Lagrangian, with respect to the vector, w.
Oh my God, how do you differentiate with respect to a vector? It turns out that it has a form that looks exactly like differentiating with respect to a scalar.
And the way you prove that to yourself is you just expand everything in terms of all of the vector's components.",can someone explain to me about the stuff he does around 26:00 with the lagrange thingy? or guide me to some easy literature around it?
_PwhiWxHK8o,"Is everybody relaxed, taking deep breath? Actually, this is the easiest part.
This is just doing a little bit of the algebra.
So the think we're trying to maximize or minimize is equal to 1/2.
And now, we've got to have this vector here in there twice.
Right? Because we're multiplying the two together.
So let's see.
We've got from that expression up there, one of those w's will just be the sum of the alpha i times y sub i times the vector x sub i.
And then, we've got the other one, too.
So that's just going to be the sum of alpha.
Now, I'm going to, actually, eventually, squish those two sums together into a double summation, so I have to keep the indexes straight.
So I'm just going to write that as alpha sub j, y sub j, x sub j.
So those are my two vectors and I'm going to take the dot product of those.","Amazing lecture! At 32:25, could any help me understand why the magnitude of w is the dot product of w1 and w2? That's is not very clear for me (maybe I'm missing something)
Can someone explain what happens at 32:52? Where is j coming from?
Seems like every-body is understanding everything, well that's really good. So, if you are about to comment here to praise the lecturer that means you are very clear about what he said, if so, please answer this two questions, 1. it is being said by the lecturer that, x is projecting on w, but as I know if i want to take projection of x onto w then it will be x.w/|w| (without the direction of w), not only w.x . Am i right ? 2. in min 32:38 he says ""and then we got the other one"" which then he adds the sum(a_j y_j x_j ) which one he is referring to? What is the indice j ? why w^2 is wi.wj, and not wi.wi? Thank you very much for your time. And ya, the lecture is very good compare to other tutorials.
Hello, in min 32:38 he says ""and then we got the other one"" which then he adds the sum(a_j y_j x_j ) which one he is referring to? What is the indice j ?
1. You are right, but the point is: the decision rule only depends on w.x (vector). The magnitude is just a constant, and you can put it in the constant b, right? 2. On 32:38, the professor is referring to w (vector). The i and j are just identifiers; the two expressions are the same."
_PwhiWxHK8o,"Is it got something to do with Legendre? Has it got something to do with Laplace? Or does it have something to do with Lagrange? She says Lagrange.
Actually, all three were said to be on Fourier's Doctoral Defense Committee-- must have been quite an example.
But we want to talk about Lagrange, because we've got a situation here.
Is this 1801? 1802? 1802.
We learned in 1802 that if we going to find the extremum of a function with constraints, then we're going to have to use Lagrange multipliers.
That would give us a new expression, which we can maximize or minimize without thinking about the constraints anymore.
That's how Lagrange multipliers work.
So this brings us to miracle number four, developmental piece number four.
And it works like this.
We're going to say that L-- the thing we're going to try to maximize in order to maximize the width of the street-- is equal to 1/2 times the magnitude of that vector, w, squared minus.","24:00 so I looked up lagrange multipliers n all but I'm still not quite sure how we apply it here so if any1 got a good explanation to that then plz share
24:09 why did he subtract of that term?"
_PwhiWxHK8o,"It's always got to be equal to or greater than 0.
But what I'm going to say is if we're for x sub i in a gutter.
So there's always going to be greater than 0, but we're going to add the additional constraint that it's going to be exactly 0 for all the samples that end up in the gutters here of the street.
So the value of that expression is going to be exactly 0 for that sample, 0 for this sample and this sample, not 0 for that sample.
It's got to be greater than 1.
All right? So that's step number two.
And this is step number one.","13:44 ""zero In a gutter"" - for the bottom gutter or for the both ??? I didn't get it. Then I didn't get the part 21:10 when he takes the half of 'w' and squares it and instead of explain (why it needs) he uses the magic phrase ""mathematical convenient"" (without explanation)"
_PwhiWxHK8o,"Let's try this guy.
Oh.
What do you think? What happened here? Well, we're screwed, right? Because it's linearly inseparable-- bad news.
So in situations where it's linearly inseparable, the mechanism struggles, and eventually, it will just slow down and you truncate it, because it's not making any progress.
And you see the red dots there are ones that it got wrong.
So you say, well, too bad for our side-- doesn't look like it's all that good anyway.
But then, a powerful idea comes to the rescue, when stuck switch to another perspective.
So if we don't like the space that we're in, because it gives examples that are not linearly separable, then we can say, oh, shoot.
Here's our space.
Here are two points.
Here are two other points.
We can't separate them.
But if we could somehow get them into another space, maybe we can separate them, because they look like this in the other space, and they're easy to separate.
So what we need, then, is a transformation that will take us from the space we're in into a space where things are more convenient, so we're going to call that transformation phi with a vector, x.
That's the transformation.
And now, here's the reason for all the magic.
I said, that the maximization only depends on dot products.","Solution found at 45:14 doesn't seem to apply the concept of adding an extra dimension (41:50). Instead, it seems to apply a non-linear ""road""; which looks great because it's simpler, but I'm confused."
_PwhiWxHK8o,"OK? Well, we can do the same trick with x minus.
If we've got a negative sample, then y sub i is negative.
That gives us our negative w times dot over x sub i.
But now, we take this stuff back over to the right side, and we get 1 plus b.
So that all licenses to rewrite this thing as 2 over the magnitude of w.
How did I get there? Well, I decided I was going to enforce this constraint.
I noted that the width of the street has got to be this difference vector times a unit vector.
Then, I used the constraint to plug back some values here.
And I discovered to my delight and amazement that the width of the street is 2 over the magnitude of w.
Yes, Brett? STUDENT: So your first x plus is minus b, and x minus is 1 plus b.
PATRICK WINSTON: Yeah.
STUDENT: So you're subtracting it? PATRICK WINSTON: Let's see.
If I've got a minus here, then that makes that minus, and then, the b is minus, and when I take the b over to the other side it becomes plus.","18:52 (1-b) - (1+b) = -2b not 2 right?????????????????????????
Hi, could anyone explain at 19:11, why the vector w disappeared? I think it should be 2*w/||w||, rather than 2/||w||
At 18:48, why is w vector set equal to one? (expression = 2 / ||w vector||)
If anyone was also confused at 19:00, xPlus should actually be -1-b. And so when you substitute it becomes 1-b-(-1-b) = 2. I don't understand where the w vector disappears to though if someone could explain that.
For everyone watching, note that there is a mistake on the board. At 19:20 , a student asked a correct clarification. w dot xPlus should be (1-b) , whereas w dot xMinus should be (-1-b), then you can get the 2/norm(w) equation.
Excellent video, by the way, at 18:52, it should be 1-b and -1-b so you got 2
1. Not really. |w| is going to affect the equation at 19:00. If we keep it as y_i(x_i . w/|w| + b) - 1 = 0 then when plugging those values into the width equation will result in 2, not 2/|w|. I'm still not clear on how omitting |w| is okay.
Hoenir Bhullar Are you saying w in the first equation is a unit vector (so ), and expands to w/||w||? That's the only expansion I can think of. But if so, then how can he replace the w.x in equation 19:00 with .x?
Question at 19:20. y_i ( w . x_i - b ) - 1 = 0 -1 (w . x_i - b) - 1 = 0 -w . x_i - b - 1 = 0 -w . x_i = 1 + b w . x_i = -1 - b Then: (1-b) - (-1-b) = 1 - b + 1 + b = 2"
_PwhiWxHK8o,"STUDENT: Yeah, so if you subtract the left with the right [INAUDIBLE].
PATRICK WINSTON: No.
No, sorry.
This expression here is 1 plus b.
Trust me it works.
I haven't got my legs all tangled up like last Friday, well, not yet, anyway.
It's possible.
There's going to be a lot of algebra here eventually.
So this quantity here, this is miracle number three.
This quantity here is the width of the street.
And what we're trying to do is we're trying to maximize that, right? So we want to maximize 2 over the magnitude of w if we're to get the widest street under the constraints that we've decided that we're going to work with.",sorry~ 19:48 why minus x- become 1+b
_PwhiWxHK8o,"Should we take a break? Should we get coffee? Too bad, we can't do that in this kind of situation.
But we would if we could.
And I'm sure when Vapnik got to this point, he went out for coffee.
So now, we back up, and we say, well, let's let these expressions start developing into a song.
Not like that, that's vapid, speaking of Vapnik.
What song is it going to sing? We've got an expression here that we'd like to find the minimum of, the extremum of.
And we've got some constraints here that we would like to honor.
What are we going to do? Let me put what we're going to do to you in the form of a puzzle.","I got confused by the lecturer insisting that y_i(x_i * w + b) - 1 = 0 equation is ""the constraints"" (e.g. 22:38). This cannot be constraints - it would mean ""the solution (w) should be such that all training samples are in the gutter"", which cannot be satisfied by any solution. I think the actual constraints is the inequality y_i(x_i * w + b) - 1 >= 0, i.e. ""the solution (w) should be such that all training samples are either in the gutter or outside of the street"". Then we look for a vector w which gives the widest possible street while satisfying these constraints. The y_i(x_i * w + b) - 1 = 0 equation is satisfied by the training samples in the gutter (only), so it can be used to show that the formula for the street width is 2/||w|| (17:18) but that's all there is to it - this is not constraints."
_PwhiWxHK8o,"So now, if I only had a unit normal that's normal to the median line of the street, if it's a unit normal, then I could just take the dot product or that unit normal and this difference vector, and that would be the width of the street, right? So in other words, if I had a unit vector in that direction, then I could just dot the two together, and that would be the width of the street.
So let me write that down before I forget.
So the width is equal to x plus minus x minus.
OK.
That's the difference vector.
And now, I've got to multiple it by unit vector.
But wait a minute.
I said that that w is a normal, right? The w is a normal.
So what I can do is I can multiply this times w, and then, we'll divide by the magnitude of w, and that will make it a unit vector.",Can someone clarify how the calculation of the unit normal and the difference vector calculates the width at 16:16?
_PwhiWxHK8o,"So now, my decision rule with this expression for w is going to be w plugged into that thing.
So the decision rule is going to look like the sum of alpha i times y sub i times x sub i dotted with the unknown vector, like so.
And we're going to, I guess, add b.
And we're going to say, if that's greater than or equal to 0, then plus.
So you see why the math is beginning to sing to us now.
Because now, we discover that the decision rule, also, depends only on the dot product of those sample vectors and the unknown.
So the total of dependence of all of the math on the dot products.
All right.
And now, I hear a whisper.
Someone is saying, I don't believe that mathematicians can do it.
I don't think those numerical analysts can find the optimization.
I want to be sure of it.
Give me ocular proof.
So I'd like to run a demonstration of it.
OK.
There's our sample problem.
The one I started the hour out with.
Now, if the optimization algorithm doesn't get stuck in a local maximum or something, it should find a nice, straight line separating those two guys to finding the widest street between the minuses and the pluses.
So in just a couple of steps, you can see down there in step 11.
It's decided that it's done as much as it can on the optimization.
And it's got three alphas.
And you can see that the two negative samples both figure into the solution, the weights on the Lagrangian multipliers are given by those little yellow bars.
So the two negatives participate in the solution as one of the positives, but the other positive doesn't.","There is something unclear for me. At 37:57 How do you know which samples are support vectors, in other words, I mean, how do you know which alpha is zero which alpha is not? Maybe I need some knowledge of Lagrange multiplier to figure out this issue?
The guy seating on the right side at 38:49: how did he come up with this?
Is there anyone who knows how to derive 'alpha' and 'b' in Decision Function(37:35)?"
_PwhiWxHK8o,"So that dot product, not a product, that dot product is, in fact, a scalar, and it's the width of the street.
It doesn't do as much good, because it doesn't look like we get much out of it.
Oh, but I don't know.
Let's see, what can we get out of it? Oh gee, we've got this equation over here, this equation that constrains the samples that lie in the gutter.
So if we have a positive sample, for example, then this is plus 1, and we have this equation.
So it says that x plus times w is equal to, oh, 1 minus b.
See, I'm just taking this part here, this vector here, and I'm dotting it with x plus.
So that's this piece right here.
y is 1 for this kind of sample.
So I'll just take the 1 and the b back over to the other side, and I've got 1 minus b.","I can't understand much at 17:16 , what's with the unit vector W and why do we use it as a unit vector? I thought just projecting (X+ - X-) on to the normal W vector as seen before, would be enough, with no need for length/norm
I got confused by the lecturer insisting that y_i(x_i * w + b) - 1 = 0 equation is ""the constraints"" (e.g. 22:38). This cannot be constraints - it would mean ""the solution (w) should be such that all training samples are in the gutter"", which cannot be satisfied by any solution. I think the actual constraints is the inequality y_i(x_i * w + b) - 1 >= 0, i.e. ""the solution (w) should be such that all training samples are either in the gutter or outside of the street"". Then we look for a vector w which gives the widest possible street while satisfying these constraints. The y_i(x_i * w + b) - 1 = 0 equation is satisfied by the training samples in the gutter (only), so it can be used to show that the formula for the street width is 2/||w|| (17:18) but that's all there is to it - this is not constraints."
_PwhiWxHK8o,"So that's minus sub of alpha i times y sub i times b.
And then, to finish it off, we have plus the sum of alpha sub i minus 1 up there, minus 1 in front of the summation, such as the sum of the alphas.
Are you with me so far? Just a little algebra.
It looks good.
I think I haven't mucked it, yet.
Let's see.
alpha i times y sub i times b.
b is a constant.
So pull that out there, and then, I just got the sum of alpha sub i times y sub i.
Oh, that's good.
That's 0.
Now, so for every one of these terms, we dot it with this whole expression.
So that's just like taking this thing here and dotting those two things together, right? Oh, but that's just the same thing we've got here.
So now, what we can do is we can say that we can rewrite this Lagrangian as-- we've got that sum of alpha i.
That's the positive element.
And then, we've got one of these and half of these.
So that's minus 1/2.","35:00 the whole equation was with sub i byfar, so why do we assume that some summations are different so sub j?"
_PwhiWxHK8o,"So what we're going to do with this first equation is we're going to multiply it by y sub i, and that is now x of i, plus b is equal to or greater than 1.
And then, you know what we're going to do? We're going to multiply the left side of this equation by y sub i, as well.
So the second equation becomes y sub i times x sub i plus b.
And now, what does that do over here? We multiplied this guy times minus 1.
So it used to be the case that that was less than minus 1.
So if we multiply it by minus 1, then it has to be greater than plus 1.
The two equations are the same, because that introduces this little mathematical convenience.
So now, we can say that y sub i times x sub i plus b.
Well, what we're going to do-- Brett? STUDENT: What happened to the w? PATRICK WINSTON: Oh, did I leave out a w? I'm sorry.
Thank you.
Yeah, I wouldn't have gotten very far with that.
So that's dot it with w, dot it with w.
Thank you, Brett.
Those are all vectors.
I'll pretty soon forget to put the little vector marks on there, but you know what I mean.
So that's w plus b.
And now, let me bring that 1 over to the left side, and that's equal to or greater than 0.
All right.
With Brett's correction, I think everything's OK.
But we're going to take one more step, and we're going to say that y sub i times x sub i times w plus b minus 1.","12:00 How does multiplying by yi gives that equation? Can anyone explain?
at 13:00 professor equated the two equations but he didn't multiply the left side of eqn with -1......im really confused
Hi, I have a question about the step he does at 13:29 : He goes for [...] >= 0 to [...] = 0 and then specifies that the second equation verifies the point which are in the gutter. Yet the multiple possible projections of \vec{x} on \vec{w} could give out different scalars depending on \vec{x} and it's position realive to the center of the road. If you assume b to be a constant, how can all of the projection of vectors from inside the gutter be = 0? Shouldn't the equation be valid only for \vec{x} on the border of the gutter?"
_PwhiWxHK8o,"So what we're going to say is this, that if we look at this quantity that we're checking out to be greater than or less than 0 to make our decision, then, what we're going to do is we're going to say that if we take that vector w, and we take the dot product of that with some x plus, some positive sample, now.
This is not an unknown.
This is a positive sample.
If we take the dot product of those two vectors, and we had b just like in our decision rule, we're going to want that to be equal to or greater than 1.
So in other words, you can be an unknown anywhere in this street and be just a little bit greater or just a little bit less than 0.","can someone plz explain why at 8:32, wx+b >=1 for positive samples?
could someone please explain to me why 8:40 to 9:30 is the way it is? I'm so lost on this step... why did it turn from >=0 to >= 1
At 8:58, why is it set to be >=1 and and <= -1, and not 2, -2, or 10, -10? What is special about 1?
Hi guys. I have a question... at 8:52, why is it +1 or -1 when the sample is positive or negative? Shouldn't it be some quantity like +delta? (what I mean by delta here is a small positive real number).
At 9:00 why do we choose the quantity to be greater that 1? Is there a motivation for that or it can be any number and the derivation would still be valid?"
_PwhiWxHK8o,"So what's w multiplied by? Well, it's multiplied by x and y sub i and alpha sub i.
All right.
So that means that this expression, this derivative of the Lagrangian, with respect to w is going to be equal to w minus the sum of alpha sub i, y sub i, x sub i, and that's got to be set to 0.
And that implies that w is equal to the sum of some alpha i, some scalars, times this minus 1 or plus 1 variable times x sub i over i.
And now, the math is beginning to sing.
Because it tells us that the vector w is a linear sum of the samples, all the samples or some of the sample.
It didn't have to be that way.
It could have been raised to a power.
It could have been a logarithm.
All sorts of horrible things could have happened when we did this.
But when we did this, we discovered that w is going to be equal to a linear some of these vectors here.",27:50 xi should be a vector. but he caught up at 28:45. Beautifully explained.
_PwhiWxHK8o,"Some of the vectors in the sample set, and I say some, because for some alpha will be 0.
All right.
So this is something that we want to take note of as something important.
Now, of course, we've got to differentiate L with respect to anything else it might vary, so we've got to differentiate L with respect to b, as well.
So what's that going to be equal to? Well, there's no b in here, so that makes no contribution.
This part here doesn't have a b in it, so that makes no contribution.
There's no b over here, so that makes no contribution.
So we've got alpha i times y sub i times b.","29:25 oh b is a variable? How? Isn't b a constant value past which samples are positive? Why does he derivate with respect to that?
29:18 the perpendicular vector w seems like the ""mean"" of all the data point ""vectors"", is that the eigen vector ?? just a dummy guess
27:50 xi should be a vector. but he caught up at 28:45. Beautifully explained."
_PwhiWxHK8o,"We don't know anything about it's length, yet.
Then, we also have some unknown, say, right here.
And we have a vector that points to it by excel.
So now, what we're really interested in is whether or not that unknown is on the right side of the street or on the left side of the street.
So what we'd what to do is want to project that vector, u, down on to one that's perpendicular to the street.
Because then, we'll have the distance in this direction or a number that's proportional to this in this direction.
And the further out we go, the closer we'll get to being on the right side of the street, where the right side of the street is not the correct side but actually the right side of the street.
So what we can do is we can say, let's take w and dot it with u and measure whether or not that number is equal to or greater than some constant, c.
So remember that the dot product has taken the projection onto w.
And the bigger that projection is, the further out along this line the projection will lie.","5:36 Sir, is that what is called an eigenvector?"
_PwhiWxHK8o,"You differentiate those with respect to what you're differentiating with respect to, and everything turns out the same.
So what you get when you differentiate this with respect to the vector, w, is 2 comes down, and we have just magnitude of w.
Was it the magnitude of w? Yeah, like so.
Was it the magnitude of w? Oh, it's not the magnitude of w.
It's just w, like so, no magnitude involved.
Then, we've got a w over here, so we've got to differentiate this part with respect to w, as well.
But that part's a lot easier, because all we have there is a w.
There's no magnitude.
It's not raised to any power.","In 27:10, why was ||w|| transformed to the w vector?"
a1RaIqkdG0c,"And then, I just say the best time to end the party is at this time.
Time, which was discovered here, and the number of celebrities who are going to be attending is max count.
OK.
So this is essentially the code for the algorithm.
And this thing here is doing the slightly more relaxed check that I alluded to, which was like when you have 6:00, you're not just looking for 6:00 here, you're looking for 6:00 within this interval that looks like this.
So 6:00 is definitely within this, but it would not be-- 6:00 is not inside 5:00 and 6:00 as I described to you before, right? So that's simply the check.
That's why you have a less than equal to here and a strictly greater than over here, just to take care of that closed and open part of it.
So again, if you don't understand every nuance in this code, it's not that big a deal.
But hopefully you have the overall picture, right? Makes sense, right? So as you can imagine, there's a much better way.
There's a much better way that isn't as exhaustive as this one.
And do people have a sense of what a better way would be, or would you like a hint? And anyone want to conjecture a different way of solving this problem that's-- yeah, go ahead.
AUDIENCE: The exhaustiveness of this algorithm? SRINI DEVADAS: Ah, so the exhaustiveness of this algorithm is as follows.
So basically what I'm saying is-- what I'm saying is I'm going to go look at the range of times.
And all of these are per hour, right? Everything is per hour.
I'm going to look at the range of times, and I'm going to go ahead and include 6:00 through 12:00 here.
And I'm going to go 6:00, and I'm going to go figure out at 6:00 how many celebrities exist.
And then at 7:00, how many celebrities exist, and then at 8:00 all the way to 12:00, right? And I'm going to get in count, which is that list there, I'm going to get for each-- for count 6:00, I'm going to say two.","no matter how I think of it, the code at 13:45 is wrong, and there should be ""or"" instead of ""and"" at the loop where check celebrity is in the range time, because if she was already in, she is counted, and if she was going to go out within that time as well, she is also counted, so I can't understand why there is an AND instead of OR"
aDmFyu0Yt7s,"The decision question is does white win? Now we end up with a tie in the case white doesn't win.
You can also change that by having a long path of blacks.
And black is going to sit there flipping, trying to get to flipping their edge.
And the length of the wire is exactly how long it takes to fill in all these things.
Yeah.
AUDIENCE: The reason why they have that gadget there at the end is you need to set exactly a certain number of them true? Before black can, and so-- PROFESSOR: Oh.
I see.
Right.
Yeah.
That's a little more subtle.
OK.
Zero minutes remaining.
The same crossover works.
And you can also build a protected OR.
Protected OR is where you never have both of these coming in.
If you allow me the notion of a free edge, which is just floating there and can be reversed whenever you want, this is a white edge, then this will act as a OR, because this choice gadget can only go one way or the other.
It's only one of the inputs can actually flip one of these edges, and then the OR will just take it.
So we can build predicted ORs.
And then I have a bunch of PSPACE hardness proofs based on bounded constraint logic, but I suggest we wait til Thursday to see this.
Because they're kind of fun.
And I'd rather spend a little time going through the proof details.
So you'll have to wait in suspense before we get to Amazons, which is a very fun-- you should play Amazons meanwhile, if you get a chance.
It's very fun if you have a chessboard around.
And Konane, which is an ancient, at least 300-year-old game from Hawaii, and Cross Purposes, which is a modern game.",Can someone explain that comment at 1:18:44 ? It seems useful but I don't understand what that gadget is good for.
aIsgJJYrlXk,"It's either odd or even, that's two.
And then my current city.
And I have n possible options for my current city.
It could be city one, city two, city three, so that's n.
So I have n options here.
I have two options here.
That's why I'm saying my whole state space is two times n, okay? All right.
Okay.
So let's try out this example.
Let's not put it in.
Uh, just talk to your neighbors about this, and then maybe, if you have ideas just let me know in a minute.
So- okay.
So what is the difference here? So we're traveling from city one to city n, and then the constraint is changed.
Now, we want to visit at least three odd cities.
So that's what we wanna do.
And then the question is, what is the minimal state? Talk to your neighbors.
[NOISE] All right.
Any ideas? Any ideas? [BACKGROUND] What is a possible state? Like it- don't worry about the minimal even, like for now.
Like what do I need to keep track of? Number of odd cities.
Number of, number of odd cities? Yeah.
Okay.
So- and is that it? Do I need to just know the number of odds cities? Um, or number of odd is about your, uh, [OVERLAPPING] So number- so, so what I meant is I also need to have current city, right? So, okay.",Thank you! On 1:07:29 how do you get 2N of options if it's an acyclic graph?
aIsgJJYrlXk,"Yes? Do you not also need an option for zero odd cities specific to [inaudible] Zero.
We're starting from city one, so we're already counting that in, but yeah, like, if you have zero odd cities, that is a good point too.
All right.
So I've gotta move.
Okay, so, um, that was that.
This is how it looks like.
Like you can think of your state space like this again as a tuple of I visited one, two, three, and- and then the cities.
I have another example here, you can think about this later and yeah, like, work, work it at home.
But, uh, basically the question is, again, you're going from city one to N, and you want to visit more odd cities than even cities.
What would be the minimal state space? But we can talk about it offline.
So the summary so far, is- is that state is going to be a summary of past actions sufficient to choose future actions optimally.
And then dynamic programming, it's not doing any magic, right, it's using this notion of state to bring down this exponential time algorithm to a polynomial time algorithm, and then, with the trick of using memoization, and with a trick of choosing the right state, okay? And we have talked about dynamic programming and how it doesn't work for acyclic graphs.
And now, we want to spend a little bit of time talking about uniform cost search, uh, and how that can help with the- with the cycles.
So if you guys have seen Dijkstra's algorithm, this is very similar to Dijkstra's, like, yeah.
So- so it's basically Dijkstra's.
But- all right.
So let's- let's actually talk about this.
So- so the observation here is that when we- when we think about the cost of getting from start state to some s prime, well, that is going to be equal to cost of going from s to s prime and then some past cost of s, okay.",01:12:22 the second dp question is there a better state than ( 2 * (n^2) )?
auK3PSZoidc,"But the most important thing in here is not the details of make implications.
And I'll give you some sense of that before we're done.
But it's really the structure that is the most important.
The fact that I've done make implications here and undo implications here is the correctness requirement that is important to exhaustive search.
So if I do this and I do kind of the implications that we had right at the beginning of lecture and I go ahead and run it, just take a look.
I won't write this out, but remember what the backtracks are for these things, roughly speaking, for the original Sudoku.","The first time around, he doesn't show the solution to the ""difficult"" 4th puzzle! For each input, the program outputs: Input grid (true or false) Output grid And if you check basically any of the non zero numbers you will see they don't match... I don't see how his program messed it up though... So it probably took a lot fewer backtracks because it changed the input values so the puzzle had more solutions and solved for the first it found. If you don't believe the program shows the input before the solution, you can look a little before mark 45:00 to see the ""dif"" input variable and see it matches the 2nd to last grid at 45:27, which doesn't match the last grid. His optimized program doesn't have this issue as you can see at 46:06."
auK3PSZoidc,"OK, that make sense? Good.
So here's the core routine that corresponds to the search.
And ignore this global variable here.
I'll explain that in a minute.
That's going to be our metric.
Backtracks is going to be our metric for computing performance.
And it's going to be quite interesting.
It's going to produce some interesting results for us when we run this on various different examples.
But this core procedure looks a lot like the n-queens search in the sense that you have a for loop and a recursive call.
And in this case the for loop is going to be something that ranges through the different values, that you find a location that you want to put something into, which is the next empty location in your current configuration.","at 25:50 The solveSudoku it says Return true in for loop return true and return False, I dont understand to whom is it sending true. What I also dont understand suppose you fill in 3 in first cell .. but the real solution should be 9 ...en he detects the error after filling 10 cells that somehing is wrong se he has to go back..so how does he knows where he was or does he start at 1 again, but then you will run at the same problem, that is a bit unclear for me."
auK3PSZoidc,"Oh, I'm sorry, I need to go to the shell.
And it was 335,000-- what is it-- 579, 6363, 335,000, and 9949.
So if I go off and I run Sudoku optimized, which is doing these implications like I describe, and I go ahead and run that.
The first one goes from 579 to 33 backtracks.
OK so that's pretty good.
Because it's done a bunch of implications.
It's still-- it's not super smart.
I mean that is a simple enough puzzle that a human being would not backtrack.
I mean a human being would not backtrack in that first puzzle, right? And you should check that.
And-- oh, this thing finished in the middle.
So it went to 33.
Oh, only had three of them? What do I have here in Sudoku Opt? Oh I see.
I only ran-- oh wow.
OK so I ran inp2, hard, and difficult.
So it really went from 6363 to 33.
It went from 335,000 to 24,000.
And then it went to-- 7-- went from 9949 to 726.
The details aren't-- the numbers aren't super important.","The first time around, he doesn't show the solution to the ""difficult"" 4th puzzle! For each input, the program outputs: Input grid (true or false) Output grid And if you check basically any of the non zero numbers you will see they don't match... I don't see how his program messed it up though... So it probably took a lot fewer backtracks because it changed the input values so the puzzle had more solutions and solved for the first it found. If you don't believe the program shows the input before the solution, you can look a little before mark 45:00 to see the ""dif"" input variable and see it matches the 2nd to last grid at 45:27, which doesn't match the last grid. His optimized program doesn't have this issue as you can see at 46:06."
b0HvwszmqcQ,"And we're gonna see just one example of the product rule.
So what does the, uh, gradient with respect to x of x transpose Ax, right? We're gonna apply the product rule as we know it.
So this is gonna be the gradient with respect to, um, let me use two colors to- to, um, right? Or precisely, right? Treat it as a product of two things and, you know, um, ah, ah, by them, uh, uh, differently.
And this is going to, um, come out to be, um, um, so here the- the gradient here is, um, you can think of this as gradient of x with, sorry, I forgot a transpose here, right? So, yeah, so this is gonna be, um, just Ax d transpose- Ax, and this is going to be, um, A transpose x, right? So, um, the- this is gonna be x times A plus A transpose.
And if A is- is, uh, uh, symmetric, this is going to be just 2Ax, right? A few more, um, matrix derivates which are gonna be very useful.
Yeah.
[BACKGROUND].
Uh, I'm sorry.
What's the- what's the question.
[BACKGROUND].
Oh, this is the product rule, so for example, um, um, when you are taking, um, so d by dx of f of x, g of x is equal to d by dx of f of x times g of x plus f of x times d by dx of g of x, right? This is the product rule, and this is the multivariate version of that.","A very basic question. At timestamp 1:27:27 the second term in the product rule, results in A^T x. and not x^T A. Why is it so? Are both equivalent and written so only for symmetry with the first term, which depicts x is a row vector?"
b0HvwszmqcQ,"It- it- goes down along one axis and go- and goes up on another axis, right? So the- the- the connection between multi-variable calculus and linear algebra is- are- are very deep and- and you're going to- you're going to be using an analyzing Hessians of different loss functions to kind of characterize the convex, which means if they are convex, you know, that is good news.
It means the- when you minimize the loss function, you reach a unique global minimum, because if, you know, any bowl-shaped function or, you know, take any bowl, there's always like a unique, uh, global minimum, whereas if- if- if a function is- is not convex or if it has, you know, uh, uh, saddle points, then, you know, optimization is- is- is a little harder, trying to minimize the loss function is going to be harder, right? Now, some examples of, uh, how we actually go about calculating, uh, gradients.
Uh, for example, if you have a function x, the gradient of the function with respect to x is written like this.
This- this is the terminology that we generally use, the inverted, um, uh, triangle, um, is- is- denotes the gradient symbol.
And if you- if you are writing your homeworks in LaTeX, then this, you know, to get the symbol in LaTeX, you use backslash nabla, um, and, um, the- the- the, uh, uh, uh, subscript for nabla indicates what is the variable with respect to which you are, um, um, taking the, um, um, gradient and, you know, for gradients, this is going to be vector-valued, obviously, and the definition is just this.","Hello, I have a small question at timestamp: 1:13:00. Why are we using Hessian matrix to determine the shape of the function? I understand why a positive semi-definite matrix has a bowl shape and why it's always positive. Why should it be the Hessian which determines these properties of a function? There could be other matrices related to a function, right?"
bQI0OmJPby4,"And we can even go to a level of specificity for pianos by saying we've got a Bosendorfer.
Why is a Bosendorfer special? I mean, is it like a Baldwin? Something's [INAUDIBLE], Yoka-- Lots of piano types, what's special? You see, you don't know because unless you play the piano, and probably unless you're a serious piano player you don't know that a Bosendorfer-- Ariel, you know.
STUDENT: I think its supposed to have an extra octave at the bottom, black keys.
Pretty cool.
PATRICK WINSTON: It's got some extra keys at the bottom.
And most people don't know that unless they're serious about the piano.
Some professional piano players, when they're confronted with a Bosendorfer have to have someone cover those keys because it screws up their peripheral vision, and hit the wrong key.
Because they're not used to having those extra keys at the bottom.
So that's a little detail but the Bosendorfer.
So you can make a kind of graph, and you can say, let's go from low, very general, to a basic level, to a specific level.
So it is the case in human knowledge that that graph has a tendency to look sort of like this.
So here's tool, here's hammer, here's ball peen.
So that level, where you have a big jump, that's the general to basic level of transition.
So that basic level is probably there because that's the level on which we hang a huge amount of our knowledge.
We know a lot about pianos, and it all seems to be hanging on that word piano, which gives us power with the concept.",18:30 what does the vertical axis on the graph represent?
bVjCjdq06R4,"You can possibly even just pretrain on the target data set itself.
So this is kind of breaking the common knowledge, or the common wisdom, here.
If you have a supervised pretraining task, then this won't work out, because you-- like running supervised learning on your fine tuning data set and then running fine tuning on your fine tuning data set, those are going to be the same exact thing.
So this is-- I would expect to only hold in the unsupervised pretraining case.
But it's something that breaks the common wisdom and suggests that we don't have everything figured out, even when it comes to fine tuning.
Yeah.
[INAUDIBLE] This is only in stages versus can you-- with this-- you're asking if this would also hold in the multi task setting? [INAUDIBLE] I think that they only ran experiments in the pretraining and fine tuning phase.
But you could read the paper to check.
And because it came out last week, I don't think anyone has built on it yet.
I should mention that this is averaged over a number of different-- they ran this on a number of different target tasks.
This was all in the NLP domain.
But there's actually another paper that came out actually a little bit before this that actually showed a similar result in computer vision tasks, as well.
Now the second paper is actually a paper that was co-authored by Yoonho and others-- Yoonho is a TA in the class.","Hello, Thanks for the great content! I was wondering what the other paper was you mentioned at 34:57? - pre-training on the downstream dataset but in the vision domain."
buzsHTa4Hgs,"So 4,345, uh, 4, um, er, where is, uh, er, uh, 34- uh, 345- um, is, um, layer hashes into color 10s, so we replace a color 10, uh, here.
And we could keep iterating this and we would come up, uh, with, uh, more and more, uh, refinement, uh, of the, uh, uh, colors of the graph.
So now that we have run this color refinement for a, uh, a fixed number of steps, let say k iterations, the Weisfeiler-Lehman, uh, kernel counts number of nodes with a given color.
So in our case, we run- we run this three times, so we have 13 different colors..
And now the feature description for a given graph is simply the count, the number of nodes of a given color, right? In the first iteration, uh, all the nodes were colored, um- all six nodes were colored the same one- uh, the same way.
Um, so there is six instances of color 1.
Then, um, after we iter- um, agg- aggregated the colors and refined them, you know, there were two nodes of color 2, one node of color 3, two nodes of color 4, um, and so on.
So here is now the feature description in terms of color counts, uh, for, uh, for, uh, different colors for the first graph and different colors, uh, for the second graph.
So now that we have the feature descriptions, the Weisfeiler-Lehman graph kernel would simply take the dot product between these two, uh, uh, feature descriptors and return a value.","At 16:00, is there an obvious error on the counts regarding colors? I think there are 2 times of 11, 1 12, and no 13 in the first graph. Resultingly, I get the WL kernel value of 50."
buzsHTa4Hgs,"So this is the definition of graphlets in the graph kernel.
And for example, for k equals 4, that are 11 different graphlets, fully connected graph all the way to the graph on four nodes without any connections.
And now, uh, given a graph, we can simply represent it as count of the number of structures, um, er, different graphlets that appear, uh, in the graph.
So for example, given a graph, and the graphr- graphlet list, we define the graphlet count vector f, simply as the number of instances of a given graphlet that appears, uh, in our graph of interest.
For example, if these G is our graph of interest, then in this graph, there resides one triangle, there resides three different parts of land 2, there reside six different edges with an isolated nodes, and there, uh, exist no, uh, triplet, uh, of nodes, uh, that are, uh, that are not connected, uh, in this graph.","I think there is an error in the graphlet representation in the slide at 7:34. The graphlet vector = (1, 3, 6, 0) is for the graph G shown in the slide but without the bottom (or top) edge. For the graph shown in the slide the right representation is: (1, 6, 3, 0)"
buzsHTa4Hgs,"To be, for example, a bit more expressive, we could have what we could call, uh, degree kernel, where we could say, w- how are we going to represent a graph? We are going to represent it as a bag-of-node degrees, right? So we say, ""Aha, we have one node of degree 1, we have three nodes of degree 2, and we have 0 nodes of degree, uh, 3."" In the same way, for example, uh, here, we could be asking, how many nodes, uh, of different degrees do we have here? We have 0 nodes of degree, um, 1, we have two nodes, uh, of degree 2, and two nodes, uh, of degree, um, 3.
So, um, and this means that now we would, er, obtain different feature representations for these, uh, different, uh, graphs, and that would allow us to distinguish these different, uh, graphs.","At 4:59, the first graphlet seems to be [1,2,1] to me since the bottom right node has 3 neighbors."
cUYTlKA8jaw,"In fact, in the previous example, it turns out you could get away with three processors.
It was possible to schedule the subjects so you only took three courses a term and still finished in minimum time.
So can you do better than three subjects? Well, there's a trivial argument that says, no, you can't.
Because in that previous example, we had 13 subjects to schedule.
The maximum chain size was 5.
So it was going to take at least five terms.
So that means you have to distribute these 13 subjects among five terms.
There has to be some term that has at least the average number of subjects, namely 13 divided by 5.",2:53 I think the title of the slide should have been min # of subjects
cjTs7lH5eUs,"So these are maybe yeah, two.
This will be our d train for task i, maybe we sampled task i.
And this is going to be d test for task i.
Now one thing that we're missing here is we're also missing labels.
We need labels in our training set and test set.
And so what we're going to do is we're also going to assign labels to each one of these.
So we can say labels zero, one, two.
It's important to have consistent labels across our characters.
OK.
And now we get to the fun part.
So what we're going to do is we are going to pass our training data set into a neural network that will implement the learning process.
And so in terms of passing this into a neural network, these data sets may have a number of different examples.
And so things like a recurrent neural network are typically a good choice or a transformer, if you want to be a little more into the times.
And so you'll pass this into-- your training data points into a recurrent neural network.
And at this point, we're going to get to the two different versions.
So one version of black box meta learning will take as input these training data points and output a set of parameters.","Thanks a lot for this! What is not clear to me is when at 28:05 she says 'we pass our training dataset to the network', does it mean also passing the labels to the network? Like concatenated with the inputs or something? Because the labels need to be used in some way, right?"
cjTs7lH5eUs,"So this would be like our training set and our test data set from all of the examples that we have for a particular task.
I should mention maybe here that it's worthwhile to kind of mix and match these.
So you shouldn't always use this as your test examples and always use this as your training examples.
You can get a little bit more data for meta-training by randomly sampling what you use as a training example and what you use as a test example.
And so kind of visually what this looks like is, maybe these are all the examples you have for one of your tasks.
You want to allocate this into training examples and test examples.
And so you'll kind of just randomly assign them to a training set and do a test set.",In 57:48 i couldn't understand how they are using the training data for the training of the model. x test and y test are used for the loglikelihood but how are the training data part is used ?
dARl_gGrS4o,"So let's see if we can do it with nine.
Let's see if we can do it with eight.
Let's see if we can do it with seven.
These take almost zero time, right? Because they're under constraint.
Wow, that's good, seven.
Let's try six.
Actually let's try two.
It loses fast.
Let's try three.
I don't know.
Maybe if you let it run one long enough three will work.
I doubt it.
While we're at it though, we might as well go back here and try it with six.
Remember seven worked real fast.
Gees, six, that was six, right? Yeah.
So let's try it with five.
OK.
So it runs real fast with five.
It terminates real quick with two, so we got three and four left.
So we could tell our boss, a la any time algorithm, that you're not real sure, but you know it's going to be either three or four.
And then, you got two computers.
You can let both run and see if either one terminate.
So you have three and four.
My guess is that three will eventually give up.
But of course, there's another little problem here.","At about 43:00 it is stated that we can say it is 3 or 4. I don't think we can; we don't know 4 will terminate, so I think the best we can say is that it is 3, 4 or 5. Did I miss something?"
dNl22h1kW1k,"One of them is-- they make a big deal in this paper-- is performance, right? So if you have, I don't know, a couple million users on OkCupid, then all of a sudden you have a couple of million processes running here, or maybe a couple million dbproxies, or maybe you can optimize something on the dbproxy side, but here, yeah, you have a couple million userids and either you have a lot of processes running all the time or you're starting these processes on demand.
And starting a process involves some nontrivial amount of overhead, so you probably wouldn't be able to get as good of performance numbers as these guys are able to show with OKWS.
There's a performance argument.
Question.
AUDIENCE: Yeah, I was just reading in the paper that said the performance of the system was better than others? PROFESSOR: Yeah.
AUDIENCE: How come? PROFESSOR: Well, I think it's partly because they fine tuned their design to their particular workload and it's also they write their whole thing in C++.
The alternative is you're writing some stuff in PHP, then you're probably going to win on that front.
It's also the case that they don't have nearly as many features as, let's say, Apache has.
Apache has a very general purpose design, so it has lots of processes running, it restarts them every once in awhile.
It actually has every ttp connection tying up a process for the duration of that connection.
They do keep-alives.
That also increases the number of processes you have to run for their design.
So all those things just add up in terms of overhead for Apache, because it wants to handle anything possibly you could do with a web server.","I think professor misunderstood something. At 1:09:46, He mentions each okcupid user won't correspond to a linux uid. The linux uid would be basically service id. So, Each of the different services(signup, forget password, etc) will be running under a different user. At 1:10:13, Some student, elaborated the advantages of aforementioned point. i.e. If one service is compromised, attacker can only get access to the service running under that user and everything else will be safe. Db proxy can have seperate instances running corresponding to each of the services that need db running under the same user as that service. At 1:10:42, He went off the tangent and said, Ok cupid probably didn't did that because there will be millions of process for each service and immediately after said, Spawning processes with appropriate user permission dynamically would be costly and perf would take a hit.."
dNl22h1kW1k,"There's not really userids, they're service IDs.
So, would it make sense to have different uids for every OKWS customer? Is there a reason for that? Yeah.
AUDIENCE: So at the moment, if one user compromises the service, then they can get access to all the other user's data for that same server.
PROFESSOR: That's right, yeah.
AUDIENCE: Whereas, if you had a separate-- essentially a separate service and a separate dbproxy for every user, there's no way you could access anyone else's data either.
PROFESSOR: Right, but could it be actually a stronger model? So especially for-- well, I guess there's really two reasons why I think the OKWS guys don't go to that extreme model.","I think professor misunderstood something. At 1:09:46, He mentions each okcupid user won't correspond to a linux uid. The linux uid would be basically service id. So, Each of the different services(signup, forget password, etc) will be running under a different user. At 1:10:13, Some student, elaborated the advantages of aforementioned point. i.e. If one service is compromised, attacker can only get access to the service running under that user and everything else will be safe. Db proxy can have seperate instances running corresponding to each of the services that need db running under the same user as that service. At 1:10:42, He went off the tangent and said, Ok cupid probably didn't did that because there will be millions of process for each service and immediately after said, Spawning processes with appropriate user permission dynamically would be costly and perf would take a hit.."
dNl22h1kW1k,"These are the main things that you can't do unless you're root, and okld is sort of the component that ends up having to do all these operations.
So I guess we had this homework question about what happens if you leak this 20 byte database token thing.
So what do you guys think? What's the damage? Should we leak these guys? Should we worry about it? Anything else? AUDIENCE: The attacker can pretend to be that specific service [INAUDIBLE].
PROFESSOR: That's right, yeah.
So, you might be able to now connect and issue, of course, all these template queries.
That actually seems fairly straightforward, I guess, from this picture.
You probably need to compromise one of these components to be able to connect to the database server in the first place.
So I guess if you have this token and you manage to compromise one of these pieces in the picture, then you could run all these queries as well.
Make sense? Fairly straightforward stuff.
OK, I guess let's look at, could you do better? Could you do better than this OKWS design? Except for make this whole argument about, well, we might be able to do even better, like allocating a separate unit uid per user in this design instead of per service.
But here, every service, like newsletters or friend matching or account sign up is a separate userid, but every OKWS user isn't really represented by a Unix uid.","I think professor misunderstood something. At 1:09:46, He mentions each okcupid user won't correspond to a linux uid. The linux uid would be basically service id. So, Each of the different services(signup, forget password, etc) will be running under a different user. At 1:10:13, Some student, elaborated the advantages of aforementioned point. i.e. If one service is compromised, attacker can only get access to the service running under that user and everything else will be safe. Db proxy can have seperate instances running corresponding to each of the services that need db running under the same user as that service. At 1:10:42, He went off the tangent and said, Ok cupid probably didn't did that because there will be millions of process for each service and immediately after said, Spawning processes with appropriate user permission dynamically would be costly and perf would take a hit.."
dRIhrn8cc9w,"Here we can think of there as just being a value function and you're just sort of updating one entry of that value function depending on which state you just reached.
So there's not kind of this nice notion of the whole previous value function of any value function.
I'll keep that there just for that reason.
Now, people often talk about the TD error, the temporal difference error.
What that is is it's comparing what is your estimate here.
So, your new estimate, which is your immediate reward plus gamma times your value of the state you actually reached minus your current estimate of your value.
Now, notice this one should have been sort of essentially approximating the expectation over S prime.
Because for that one you're going to be averaging.
And so this looks at the temporal difference.
So this is saying how different is your immediate reward plus gamma times your value of your next state, versus your sort of current estimate of the value of your current state.
Now note that that doesn't have to go to zero because that first thing is always ever just a sample, it's one future.
The only time this would be defined to go to zero is if this is deterministic, so there's only one next state.
So, you know, if half the time when I try to drive to the airport I hit traffic and half the time I don't, then that's sort of two different next states that I could go to for my current start state, either hit traffic or don't hit traffic.
Um, and so I'm either going to be getting that v pi of hitting traffic or v pi of not hitting traffic.
So this TD error will not necessarily go to zero even with infinite data because one is an expected thing from the current state and the other is which actual next state did you reach.",43:34 should that V^{pi}(s_{t}) be approximated over s instead of s`?
dZgI16nMuqE,"As I said, the one thing we have to watch out for, it shouldn't be a surprise, is we know that you can't cancel with respect to congruence mod n.
And that's reflected in the fact that you can't cancel in Zn.
Namely, in Z12, for example, 3 times 2 is equal to 2 times 8.
Again, 3 times 2 is 6, 2 times 8 is 16, you immediately take the remainder to get back to 6.
In Z12, these two things are equal.
But if you tried to cancel the 2, you'd conclude that 3 was 8, and neither 3-- 3 and 8 are different numbers in the range from 0 to 12, and they're different in Z12.",at 6:37 i think you meant Z10
eHZifpgyH_4,"So if you have a reduction like this and if say, B, has a polynomial time algorithm, then so does A, because you can just convert A into B, and then solve B.
Also this works for nondeterministic algorithms.
Not too important.
So what this tells us is that in a certain sense-- get this right-- well this is saying, if I can solve B, that I can solve A.
So this is saying that B is at least as hard as A.
I think I got that right, a little tricky.
So if we want to prove the problem is NP hard, what we do is show that every problem in NP can be reduced to the problem of X.
So now we can go back and say well, if we believe that there is some problem Y, that is in NP minus P, if there's something out here that is not in P, then we can take that problem Y, and by this definition, we can reduce it to X, because everything in NP reduces to X.
And so then I can solve my problem Y, which is in NP minus P, by converting it to X and solving X.","There is a typo at 20:40-20:50, should be [ (A in NP ) --> (B in NP)] as reduction is saying B is at least as 'hard' as A is, this is the contrapositive of (B in P) --> (A in P)."
eHZifpgyH_4,"So that means X better not have a polynomial time algorithm, because if it did, Y would also have a polynomial time algorithm.
And then in general, P would equal NP, because every problem in NP can be converted to X.
So if X has a polynomial time algorithm, then every problem Y does.
Question? AUDIENCE: For the second if statement, why can't you say that if A is in NP, B is in NP? ERIC DEMAINE: So you're asked us about the reverse question.
If is A in NP, can we conclude that B is in NP? And the answer is no.
Because this reduction only lets us convert from A to B.
It doesn't let us do anything for converting from B to A.
So if we know how to solve A and we also know how to convert A into B, it doesn't tell us anything.
It could be B is a much harder problem than A, in that situation.
That's, I think, as good as I can do for that.
Other questions? All right.
It is really tricky to get these directions right.
So let me give you a handy guide on how to not make a mistake.
So maybe over here.
What we care about, from an algorithmic perspective, is proving the problems are NP-complete.
Because if we prove NP-completeness-- I mean, really we care about NP-hardness, but we might as well do NP-completeness.
Most of the problems that we'll see that are NP-hard are also NP-complete.
So when we prove this, we prove that there is basically no polynomial time algorithm for that problem.
So that's good to know, because then we can just give up searching for a polynomial time algorithm.
So all the problems we've seen so far have polynomial time algorithms, except a couple in your problem sets, which were actually NP-complete.
And the best you could have done was exponential, unless P equals NP.
So here's how you can prove this kind of lower bound to say look, I don't need to look for algorithms any more because my problem is just too hard.
It's as hard as everything in NP.
So this is just a summary of those definitions.
The first thing you do is prove that X is in NP.
The second thing you do is prove that X is NP-hard.
And to do that, you reduce from some known NP-complete problem-- or I guess NP-hard, but we'll use NP-complete-- to your problem X.
Maybe I'll give this a name Y.
OK, so to prove that X is in NP, you do something like what we did over here, which is to give a nondeterministic algorithm.","I think last two statements 23:44 are wrong, namely "" B elem (N)P -> A elem (N)P"" because we can turn 2SAT into 3SAT and solve it"
eg8DJYwdMyg,"I'm creating a function a new function called KNN.
This will be a function of two arguments, the training set and the test set, and it will be K nearest classifier with training set and test set as variables, and two constants, survived-- so I'm going to predict who survived-- and 3, the K.
I've been able to turn a function of four arguments, K nearest classify, into a function of two arguments KNN by using lambda abstraction.
This is something that people do fairly frequently, because it lets you build much more general programs when you don't have to worry about the number of arguments.
So it's a good trick to keeping your bag of tricks.
Again, it's a trick we've used before.
Then I've just chosen 10 for the number of splits, and we'll try it, and we'll try it for both methods of testing.
Any questions before I run this code? So here it is.
We'll run it.
Well, I should learn how to spell finished, shouldn't I? But that's OK.
Here we have the results, and they're-- well, what can we say about them? They're not much different to start with, so it doesn't appear that our testing methodology had much of a difference on how well the KNN worked, and that's actually kind of comforting.","27:30 Wouldn't it be better to set label and k as keyword arguments instead of creating a separate knn function via lambda abstraction? He talks about using this to build much more general programs, yet he created two functions when you could just create one that does both, which would be more general than creating two."
eg8DJYwdMyg,"Sensitivity, think of that as how good is it at identifying the positive cases.
In this case, positive is going to be dead.
How specific is it, and the positive predictive value.
If we say somebody died, what's the probability is that they really did? And then there's the negative predictive value.
If we say they didn't die, what's the probability they didn't die? So these are four very common metrics.
There is something called an F score that combines them, but I'm not going to be showing you that today.
I will mention that in the literature, people often use the word recall to mean sensitivity or sensitivity I mean recall, and specificity and precision are used pretty much interchangeably.
So you might see various combinations of these words.
Typically, people talk about recall n precision or sensitivity and specificity.
Does that makes sense, why we want to look at the measures other than accuracy? We will look at accuracy, too, and how they all tell us kind of different things, and how you might choose a different balance.",Precission is Positive predictive value and not specificity! 19:20
eg8DJYwdMyg,"So we worry about how big K should be.
And if we make it too big-- and this is a crucial thing-- we end up getting dominated by the size of the class.
So let's look at this picture we had before.
It happens to be more red dots than black dots.
If I make K 10 or 15, I'm going to classify a lot of things as red, just because red is so much more prevalent than black.
And so when you have an imbalance, which you usually do, you have to be very careful about K.
Does that make sense? AUDIENCE: [INAUDIBLE] choose K? PROFESSOR: So how do you choose K? Remember back on Monday when we talked about choosing K for K means clustering? We typically do a very similar kind of thing.
We take our training data and we split it into two parts.
So we have training and testing, but now we just take the training, and we split that into training and testing multiple times.
And we experiment with different K's, and we see which K's gives us the best result on the training data.
And then that becomes our K.
And that's a very common method.
It's called cross-validation, and it's-- for almost all of machine learning, the algorithms have parameters in this case, it's just one parameter, K.
And the way we typically choose the parameter values is by searching through the space using this cross-validation in the training data.",12:01 why is the k nearest neighbor data training separated into testing and training again?
eg8DJYwdMyg,"You're not looking for a real number, you're looking for will they get sick, will they not get sick.
Maybe you're trying to predict the grade in a course A, B, C, D, and other grades we won't mention.
Again, those are labels, so it doesn't have to be a binary label but it's a finite number of labels.
So here's an example to start with.
We won't linger on it too long.
This is basically something you saw in an earlier lecture, where we had a bunch of animals and a bunch of properties, and a label identifying whether or not they were a reptile.
So we start by building a distance matrix.
How far apart they are, an in fact, in this case, I'm not using the representation you just saw.
I'm going to use the binary representation, As Professor Grimson showed you, and for the reasons he showed you.
If you're interested, I didn't produce this table by hand, I wrote some Python code to produce it, not only to compute the distances, but more delicately to produce the actual table.
And you'll probably find it instructive at some point to at least remember that that code is there, in case you need to ever produce a table for some paper.
In general, you probably noticed I spent relatively little time going over the actual vast amounts of codes we've been posting.","3:12 I believe that this statement is wrong. He is ACTUALLY using the full representation using the number of legs too. If you do the math, using the binary rep only, then the distance matrix shown is incorrect. CORRECTION: They are NOT using the number of legs, however, they erroneously threw a 2 in the last element of the binary data for chicken while it should be 0. I tested my own algorithm with this number and I get the same result as shown in the video. Additionally, the last binary feature should be 'reptile', correct? In the python data set the last element is zero in various of the reptile cases. Please let me know if I am missing something obvious..."
eliMLfJeu7A,"So the goal is to embed an entire graph or you can think of it also as embedding a subset of the nodes in the graph.
So first idea, that is very simple and people have tried.
So the idea is that you run standard node embedding, uh, uh, technique, uh, like we- what- like we already dis- discussed in terms of node to walk or, uh, deep walk.
And then, just sum up or average, uh, node embeddings, either in the entire graph, or in the sub-graph, right? So the idea is, um, to say, the embedding of the graph is simply a sum of the embeddings of the nodes in that graph.
And for example, this method was used in 2016, to classify, uh, molecules, uh, based on the graph structure, and it was very, um- very successful.
So even though simplistic, um, uh, works quite well in practice.
An improvement over this initial idea of averaging node embeddings is to introduce a virtual node to represent an entire graph or a sub-graph, and then run a standard graph embedding or node embedding technique, uh, and then think of the- this virtual node as the embedding for the graph.
So let me explain.
Uh, here is the idea, right? I will create these virtual node.
I will connect it to the set of nodes I want to embed.
Now I can run node to walk on this, uh- on this, um- on this graph to determine the embedding of this virtual node.","nit: Slide at 0:58 shall read ""Run a standard node embedding ..."""
esmzYhuFnds,"And so for the reptiles, he said, well, OK, we'll just make it a binary variable.
But maybe we don't want to make weight a binary variable, because maybe it is something we want to take into account.
So what we do is we scale it.
So this is a method called z-scaling.
More general than just making things 0 or 1.
It's a simple code.
It takes in all of the values of a specific feature and then performs some simple calculations, and when it's done, the resulting array it returns has a known mean and a known standard deviation.
So what's the mean going to be? It's always going to be the same thing, independent of the initial values.
Take a look at the code.
Try and see if you can figure it out.
Anybody want to take a guess at it? 0.
Right? So the mean will always be 0.
And the standard deviation, a little harder to figure, but it will always be 1.",40:00 why is mean 0 and standard deviation 1?
esmzYhuFnds,"And then I'm going to return the centroid.
Variability is exactly what we saw in the formula.
And then just for fun, so you could see this, I used an iterator here.
I don't know that any of you have used the yield statement in Python.
I recommend it.
It's very convenient.
One of the nice things about Python is almost anything that's built in, you can make your own version of it.
And so once I've done this, if c is a cluster, I can now write something like for c in big C, and this will make it work just like iterating over a list.
Right, so this makes it possible to iterate over it.
If you haven't read about yield, you probably should read the probably about two paragraphs in the textbook explaining how it works, but it's very convenient.
Dissimilarity we've already seen.
All right, now we get to patients.
This is in the file lec 12, lecture 12 dot py.
In addition to importing the usual suspects of pylab and numpy, and probably it should import random too, it imports cluster, the one we just looked at.
And so patient is a sub-type of cluster.Example.
Then I'm going to define this interesting thing called scale attributes.
So you might remember, in the last lecture when Professor Grimson was looking at these reptiles, he ran into this problem about alligators looking like chickens because they each have a large number of legs.","Just a nitpick: the example he gives (36:11) of using an iterator (the ""members"" method in his Cluster class) isnt't quite right. The for loop should be for c in C.members(): There is a more complete, if slightly wordier way to give the class true iterator behaviour. If you're interested, see https://stackoverflow.com/questions/19151/build-a-basic-python-iterator for a couple of good examples."
esmzYhuFnds,"The first piece of our [? dendogram ?] says, well, all right, I have six cities, I have six clusters, each containing one city.
All right, what happens next? What's the next level going to look like? Yeah? AUDIENCE: You're going from Boston [INAUDIBLE] JOHN GUTTAG: I'm going to join Boston and New York, as improbable as that sounds.
All right, so that's the next level.
And if for some reason I only wanted to have five clusters, well, I could stop here.
Next, what happens? Well, I look at it, I say well, I'll join up Chicago with Boston and New York.
All right.
What do I get at the next level? Somebody? Yeah.
AUDIENCE: Seattle [INAUDIBLE] JOHN GUTTAG: Doesn't look like it to me.
If you look at San Francisco and Seattle, they are 808 miles, and Denver and San Francisco is 1,235.
So I'd end up, in fact, joining San Francisco and Seattle.
AUDIENCE: That's what I said.
JOHN GUTTAG: Well, that explains why I need my hearing fixed.
[LAUGHTER] All right.
So I combine San Francisco and Seattle, and now it gets interesting.
I have two choices with Denver.
Obviously, there are only two choices, and which I choose depends upon which linkage criterion I use.
If I'm using single-linkage, well, then Denver gets joined with Boston, New York, and Chicago, because it's closer to Chicago than it is to either San Francisco or Seattle.
But if I use complete-linkage, it gets joined up with San Francisco and Seattle, because it is further from Boston than it is from, I guess it's San Francisco or Seattle.
Whichever it is, right? So this is a place where you see what answer I get depends upon the linkage criteria.
And then if I want, I can consider to the next step and just join them all.
All right? That's hierarchical clustering.","14:20 the distance from Denver to Seattle is 1307 and the distance from Denver to Boston is 1949, so why he clustered Denver to Seattle instead of Boston when using Complete linkage? should it not be clustered to the greatest distance?"
esmzYhuFnds,"This is not something you want to do on a million examples.
The naive algorithm, the one I just sort of showed you, is N cubed.
N cubed is typically impractical.
For some linkage criteria, for example, single-linkage, there exists very clever N squared algorithms.
For others, you can't beat N cubed.
But even N squared is really not very good.
Which gets me to a much faster greedy algorithm called k-means.
Now, the k in k-means is the number of clusters you want.
So the catch with k-means is if you don't have any idea how many clusters you want, it's problematical, whereas hierarchical, you get to inspect it and see what you're getting.",16:10 could anyone explain what the professor is talking about when he's mentioning n-squared and n-cubed algorithms ?
esmzYhuFnds,"Well, couple of things you can do about that.
You could be clever and try and select good initial centroids.
So people often will do that, and what they'll do is try and just make sure that they're distributed over the space.
So they would look at some picture like this and say, well, let's just put my centroids at the corners or something like that so that they're far apart.
Another approach is to try multiple sets of randomly-chosen centroids, and then just select the best results.
And that's what this little algorithm on the screen does.
So I'll say best is equal to k-means of the points themselves, or something, then for t in range number of trials, I'll say C equals k-means of points, and I'll just keep track and choose the one with the least dissimilarity.
The thing I'm trying to minimize.
OK? The first one is got all the points in one cluster.
So it's very dissimilar.
And then I'll just keep generating for different k's and I'll choose the k that seems to be the best, that does the best job of minimizing my objective function.
And this is a very common solution, by the way, for any randomized greedy algorithm.
And there are a lot of randomized greedy algorithms that you just choose multiple initial conditions, try them all out and pick the best.","At 28:00, can anyone help here ? How do we compare this dissimilarity (mentioned in IF statement), in Python. Badly need this."
f9cVS_URPc0,"Why is that? Why is that? Because this is the weight of some vertex-- this is the weight-- the shortest-path distance to my predecessor using one fewer edge.
And so this in particular is the weight of some path that uses V edges.
So if this is the shortest such path distance, this has to upper bound it at least-- at most.
Yeah? AUDIENCE: Is that the triangle inequality? JASON KU: That is a statement of the triangle inequality, thank you.
All right.
So, yes, this is just by triangle inequality.
OK.
Now what we can say is, let's take this equation summed over all vertices in my cycle.
So I'm just going to add summation here of all vertices in my cycle of this whole thing.
I'm going to do that out a little bit neater.
Summation of delta, not d.
Delta V S, V.
I guess I don't need this open parentheses.
Equals-- or less than or equal to sum of V and C of delta V minus 1 V prime.
And here, I'm summing over V and C, and this is just my notation for the predecessor.
And then I'm going to sum over the weights in my cycle V and C.
These are the sum of the weights in my cycle.
Well, what do I know about this cycle? This is just the weight of C.
The weight of C-- that's awful handwriting.
C, what do I know about the weight of the cycle? It's negative.
So, this is less than 0, which means that if I remove this, this needs to be a strict equality.
But if the sum of all of these is strictly less than the sum of all these, we can't have none of the vertices in my graph satisfying-- not satisfying this property.
If all of them are not witnesses, then this thing is bigger than this thing-- at least as big as this thing for every vertex in my cycle, which is a contradiction.","At 28:53, I don't see how instructor's statement holds. In the earlier claim that he references we had distances between s and v on both sides of the inequality, but here we have s,v on the left side and s,pred(v) on the right, not s,v, therefore I cannot see how the claim can be applied here."
fQvg-hh9dUw,"Or just what's called, repeated random sampling.
But we can use this same idea of validating new data to try and figure out whether the model is a good model or not.
Leave one out cross-validation.
This is as written in pseudocode, but the idea is pretty simple.
I'm given a dataset.
It's not too large.
The idea is to walk through a number of trials, number trials equal to the size of the data set.
And for each one, take the data set or a copy of it, and drop out one of the samples.
So leave one out.
Start off by leaving out the first one, then leaving out the second one, and then leaving out the third one.","38:50 I think ""training = D[:].pop(i)"" should be ""training = D[:i]+D[i+1:]"" because list.pop(index) returns value at index not the modified list. Correct me if I was wrong."
g0bXSXuLVb0,"Insert, delete one of them.
And then rebuild.
OK, and if I was writing a P set answer, I would say a little bit more detail what I mean in this step.
I've done it in the notes.
Not that hard.
But we can afford linear expected time.
I can afford to call build again.
I guess, technically, I'm calling this build, sequence build.
So I can afford just to extract things into an array, do the linear time operation on the array with the shifting and everything, and then just call build again.
Yeah, question? AUDIENCE: I had a question about get_at.
ERIK DEMAINE: Yeah.
AUDIENCE: [INAUDIBLE] get_at.
ERIK DEMAINE: Sorry, no.
These are separate definitions, yeah? Sorry, they got a little close.
AUDIENCE: Oh.",Could anyone explain why it's 2^10lgn at 36:31? Where is the 2 from?
g5v-NvNoJQQ,"And Anna took this other treatment from John, and this is what happened for Anna.
And that's why I conjecture that, for John, the difference between Y1 and Y0 is as follows.
And so then, that can be criticized.
So for example, a clinician who has some domain expert, can look at Anna, look at John, and say, oh, wait a second, these two individuals are really different from one another.
Let's say the treatment, for example, had to do with something which was gender specific.
Comparing two individuals which are of different genders are obviously not going to be comparable to one other, and so then the domain expert would be able to reject that conclusion and say, nuh-uh, I don't trust any of these statistics.
Go back to the drawing board.
And so type of interpretability is very attractive.
The second aspect of this, which is very attractive is that it's a non-parametric method, non-parametric in the same way that neural networks or random forest are non-parametric.
So this does not rely on any strong assumption about the parametric form of the potential outcomes.
On the other hand, this approach is very reliant on the underlying metric.
If your distance function is a poor distance function, then it's going to give poor results.","Thanks for the lecture! 1) I think that it should be another assumption for the linear model, s.t. Cov(t,eps) = 0. Otherwise the gamma would be not consistent estimation 2) 39:00 Random forest and Neural network are parametric models. Because you are training parameters and not using train data in predictive mode unlike NN classifier and matching."
g5v-NvNoJQQ,"Now, these two are two different estimators for the same thing, and the reason why you can say they're the same thing is that, in a randomized control trial, the number of individuals that receive treatment 1 is, on average, n over 2.
Similarly, the number of individuals receiving treatment 0 are, on average, n over 2.
So if you were to-- that n over 2 cancels out with this 2 over n is what gets you a correct estimator.
So this is a slightly different estimator, but nearly identical to the one that I showed you earlier, and by this argument, is a consistent estimator of the average treatment effect in a randomized control trial.
So any questions before I try to derive this formula for you? So one student asks, so the propensity score is the, quote, unquote, ""bias"" of how likely people are assigned to T equals 1 or T equals 0? Yes, that's exactly right.
So if you were to imagine taking an individual where this probability for that individual is, let's say, very close to 1, it means that there are very few other people in the data set who receive treatment 1.
They're a red data point in a sea of blue data points.
And by dividing by that, we're going to be trying to remove that bias, and that's exactly right.
Thank you for that question.
Are there other questions? I really appreciate the questions via the chat window, so thank you.","52:48 I think David meant that when P(t = 1 | xi) is close to ZERO, that means there are few other data points who received treatment = 1 around the data point xi."
g5v-NvNoJQQ,"Now, what we showed on Tuesday was that, under ignorability, where ignorability, remember, was the assumption of no hitting confounding, then the conditional average treatment effect could be defined as just a difference-- could be could be computed as the expectation of Y1 now conditioned on T equals 1, so this is the piece that I've added in here, and minus the expectation of Y0 now conditioned on T equal 0.
And it's that conditioning which is really important, because that's what enables you to estimate Y1 from data where treatment 1 was observed, whereas you never get to observe Y1 in data when treatment 0 was performed.
So we have this formula, and after fitting that model F, one could then use it to try to estimate CATE by just taking that learned function, plugging in the number 1 for the treatment variable in order to get your estimate of this expectation, and then plugging in the number 0 for the treatment variable when you want to get your estimate of this expectation.
Taking the difference between those then gives you your estimate of the conditional average treatment effect.
So that's the approach, and what we didn't talk about so much was the modeling choices of what should your function class be.
So this is going to turn out to be really important, and really, the punchline of the next several slides is going to be a major difference in philosophy between machine learning and statistics, and between prediction and causal inference.
So let's now consider the following simple model, where I'm going to assume that the ground truth in the real world has that the potential outcome YT of X, where T, again is the treatment, is equal to some simple linear model involving the covariates X and the treatments T, the treatment T.
So in this very simple setting, I'm going to assume that we just have a single feature or covariate for the individual, which is there age.
I'm going to assume that this model doesn't have any terms with an interaction between X and T, so it's fully linear in X and T.",At 12:00 the formula (with outer expectation) is for ATE (not CATE).
gFD1Lp6zK3w,"So total and function means that there's exactly one arrow out, and that's probably the most familiar case of functions.
And lots of fields just assume that functions are total, but the truth is that there often is not total.
And people aren't careful about.
So let's look at a calculus-like example.
Here's a function g that takes a pair of reals and returns a real.
It maps the real plane into the real line.
And the definition of it is g of x, y is 1 over x minus y.
Now, the domain of this function g is in fact all the pairs of reals.
That's what it means to say that it goes from R cross R-- shorthand R squared-- to the codomain R.
The codomain is the set of all reals.
But this g is obviously not total because 1 over 0 is not defined, which means that on the 45 degree line, g is not defined.
g of r, r is not defined.
So g in fact, is not a total function even though it's familiar.
And you'd not worry about partial functions normally.
You wouldn't notice that this was partial because you're not used to paying attention to that.
OK.
Let's look at a slight variation.
This is function g 0 that goes from some unspecified domain.
I'll specify it in a minute to the reals.
It has exactly the same formula g of x, y is 1 over x minus y.
But now, I'm going to tell you that the domain-- instead of being all the reals-- is the reals except for that 45 degree line.
I just want to get rid of the bad points and not worry about them.
The minute I do that, I have these two functions relations that have the same graph but different domains.","At 5:38, the slide reads ""D ::= R^2 - { (x,y) | x = y }"" What is the significance of the ""^2""?"
gGQ-vAmdAOI,"AUDIENCE: I meant like, does the map-- PROFESSOR: So now I'm taking the sum of the actual distance, plus the estimated distance to go.
AUDIENCE: All right.
I'm just wondering if the original map has to be [INAUDIBLE].
PROFESSOR: See this is not a map.
She was asking if the map has to be geometrically accurate.
See, this could be a model of something that's not a map.
And so, I'm free to put any numbers on those links that I want, including estimates, as long as they're underestimates of the distance along the lengths.
So this tells me that my estimated distance here, so far, is 1.
So I'll, surely, go down here to C.
And if I go to C, then my accumulated distance is 11.
And my estimate of the remaining distance is 0.
So that's a total of 11.
So now I'm following my heuristic again and saying what's the shortest path on a base of the accumulated distance plus the estimated distance? Here, the accumulated distance plus the estimated distance is 101.
Here, it's only 11.
So plainly, I extend this guy.
And that gets me to the goal.
And the total accumulated distance is now 111 plus 0 equals 111.
And that's not the shortest path, but wait.
I still have to do my checking, right? I have to extend A.
I when I extend A, I get to B.
And now, when I get to B that way, my accumulated distance is 2 plus my-- oh, sorry.
S, A, C.
My accumulate distance it 2.
My estimated distance is 0, so that's equal to 2.
So I'm OK because I'm still going to extend to this guy, right? Wrong.
I've already extended that guy.
So I'm hosed.
I won't find the shortest path because I'm going to stop there.","43:00 why is the estimate distance 0 if the model shows that it's at least 100?
at 42:11, why not extend C again since the total distance is less than the previously extended C node?"
gGQ-vAmdAOI,"And I'm going to stop there because this is an admissible heuristic and that's not good enough unless it's a map.
It's not good enough for this particular case because this is not geometric.
This cannot be done as a map on a plane.
So that's a situation where what I've talked to you about, so far, works with branch-and-bound.
Works with branch-and -bound plus an extended list.
But doesn't work when we added an admissible heuristic.
So if we're going to do this in general, we need something stronger than admissibility, which works only on maps.
And so the flourish that I'll tell you about here in the last few seconds of today's lecture is to add a refinement as follows.
So far, we've got admissibility.
And if we want to write this down in a kind of mathematical notation, we could say that it's admissible if the estimated distance between any node X and the goal is less than or equal to the actual distance between X and the goal.","Guys 44:35 ! I couldn't understand : either the situation we're solving is a map or not, the heuristic still doesn't give a value less than the actual distance, then why is the professor considering it admissible (tho it doesn't satisfy the admissibility rule ) ??"
gGQ-vAmdAOI,"And the actual distances are 1, 1, 1, and 10.
And over here, we'll make that 100.
So it's a kind of oddly constructed map, but it's there because we need a pathological case to illustrate the idea.
Now that's the actual distances.
And if we did branch and down with an extended list, everything would work just fine.
But we're not.
We're going to use an admissible heuristic.
And we're going to say that this guy has an estimated distance to the goal of 100.
This guy is 0.
And this guy is 0.
Now, 0 is always an underestimate of the actual distance to the goal, right? So I'm always free to use 0.
Is that 100 OK? Yeah because the actual distances is 101, so it's less than that the actual distance.
So it's OK as an admissible heuristic.
So these numbers that I put up here, together, constitute an admissible heuristic set of estimates to the goal.
So now, let's just simulate A star and see what happens.
So first of all, you start with S, and that can either go to A or B.",40:38 He said 100 was okay as it was less than the actual distance Can we ever say that actual distance < admissible heuristic ? And do we measure the actual distance and then look for admissible heuristic ? Wouldnt that be a waste of time ?
gGQ-vAmdAOI,"So A star is just branch and bound, plus an extended list, plus and admissible heuristic.
So let's go back to our original problem and try A star on that.
We're running this at a pretty slow speed because we're expecting it to be a lot more efficient than the original branch and bound.
And sure enough it is.
The number of extensions is 27.
So look at that.
A lot better than either of those working independently.
Now I can stick the thing in the center and see what happens then.
So in this particular case, the extended list didn't, actually, help us because our admissible heuristic was channeling us so tightly toward the goal it didn't matter.
So it all depends on the nature of the space that you're trying to explore.
By the way, you know how the whole works, right? So what you want to do is you want to extend the first path and sort.
But not just by accumulated distance.
Sort by accumulated distance plus admissible heuristic.
But what are the theoreticians? You must be complaining.
Sort's expensive.
Do we actually need to sort? No, we don't actually need to sort.
What do we to do? AUDIENCE: We just need to keep track of what's the minimum.
PROFESSOR: We just need to keep track of what's the minimum.
So we don't need to, actually, do that sort.
That's an unnecessary computation.
So instead, we can test, not the first path but the shortest path.
And now you have it.
Now you have the whole of A star.
And now you can go home, but I don't think you should because I'm about to show you that this idea of admissibility, actually, leads to certain screw cases that we're very fond of asking about on exams.","Pause at 38:29 He wrote ""Acc dist + admissable height"" Shouldnt it be ""acc dist + airline distance"" ?"
gGQ-vAmdAOI,"So now we have to carry on with the same algorithm that we started with.
The Oracle checking algorithm.
And when we do that, we look for this shortest path, so far, that has not been extended.
That's B, S, A, B.
That goes to C.
That's 11.
So we're done there.
A goes to D.
That adds 3.
That's 12.
C goes to E.
That adds 6.
That's 15.
And sure enough, we're done.
OK? Elliot? AUDIENCE: Does it know that there's know that there isn't a chance that you could have a zero distance extension from the [INAUDIBLE]? PROFESSOR: The question is, does it know that there's no zero distance length that's coming up.","Someone correct me on this if I'm wrong, but at 14:40, isn't Dr Winston saying that shortest path is always guaranteed to be unique as long as paths are non-zero in length? Isn't that wrong? You could have a graph that's a rectangle, and starting from one corner, if you wanna get to the other corner, clearly, you will have two shortest paths, right?"
gGQ-vAmdAOI,"That's the definition of admissible.
As long as heuristic does that it's admissible.
And A star works if it's a map.
But for that kind of situation where it's not a map we need a stronger condition, which is called consistency.
And what that says is that the distance between X and the goal minus the distance between some other node in the goal, Y.
Take the absolute value of that.
That has to be less than or equal to the actual distance between X and Y.
So this heuristic satisfy the consistency condition? Well, let's see.
Here the guess is 100.
Here it's 0.
So the absolute difference is 100.
But the actual distance is only 2.
So it satisfies admissibility, but it doesn't satisfy consistency.
And it doesn't work.
So you can almost be guaranteed we'll give you a situation where if you use an admissible heuristic you'll lose.
And if you use a consistent heuristic, you'll still win.
So how can we bring this back into the fold? Well, we can't use that heuristic.
It's no good.
But if this heuristic estimate of the goal were 2, then we'd be OK because then it would still be admissible.
But it would also be consistent.
So the bottom line is that you now know something you didn't know when you started out two lectures ago.
You now know how MapQuest and all of its' descendents work.
Now you can find an optimal path, as well as a heuristically good path.
You see that if you don't do anything other than branch and bound it can be extremely expensive.
And you can even invent pathological cases where it's exponential and the distance to the goal.
So because it can be so computationally horrible, you want to use every advantage you can, which, generally, involves using an extended list.
As well as-- no laptops, please.
It still holds.
No smoking, no drinking, and no laptops.
So you're going to use all the muscles you can.
And those muscles include using an extended list and an admissible or consistent heuristic, depending on the circumstances.","46:27 How to calculate the ""actual distance"" between node x and y, if the problem is not a map? Use the absolute difference between the accumulated cost from start to x and the accumulated cost from start to y?"
gGQ-vAmdAOI,"The actual distance is 1 plus an estimate on the remaining distance.
That gives us 100 plus 100.
That's equal to 101.
If we go to B instead, the actual distance is 1 plus the heuristic's distance is 0, so that's equal to 1.
OK, good.
So now we know that we always extend the shortest path so far.
Did I goof this, or are you asking a question? AUDIENCE: [INAUDIBLE]? PROFESSOR: Yeah, when I say actual, it's the actual distance that you've traveled.
AUDIENCE: But that's [INAUDIBLE].
PROFESSOR: So wait a second.
If I go from S to A, the actual distance I've traveled is 1.",At 42:00 how was the estimated distance from B to G 0? Why can't a non-map search space be laid out geometrically?
gRkUhg9Wb-I,"For example, suppose you just had two random variables.
Because any distribution could be represented by probability of X1 times probability of X2 given X1, according to just rule of conditional probability, and similarly, any distribution can be represented as the opposite, probability of X2 times probability of X1 given X2, which would look like this, the statement that one would make is that if you just had data involving X1 and X2, you couldn't distinguish between these two causal graphs, X1 causes X2 or X2 causes X1.
And usually another treatment would say, OK, but if you have a third variable and you have a V structure or something like X1 goes to x2, X1 goes to X3, this you could distinguish from, let's say, a chain structure.
And then the final answer to what is causal inference from this philosophy would be something like, OK, if you're in a setting like this and you can't distinguish between X1 causes X2 or X2 causes X1, then you do some interventions, like you intervene on X1 and you look to see what happens to X2, and that'll help you disentangle these directions of causality.
None of this is what we're going to be talking about today.
Today, we're going to be talking about the simplest, simplest possible setting you could imagine, that graph shown up there.","At the 12:30 mark, XXX is described as a v-structure that can be distinguished from a chain structure with data. That's not a v-structure in that sense, you would need XXX."
gqaHkPEZAew,"So if you do that, you then get this property that the difference between two vectors, its similarity to another word corresponds to the log of the probability ratio shown on the previous slide.
So the GloVe model wanted to try and unify the thinking between the co-occurrence matrix models and the neural models by being in some way similar to a neural model.
But actually calculated on top of a co-occurrence matrix count.
So we had an explicit loss function.
And our explicit loss function is that we wanted the dot product to be similar to the log of the co-occurrence.
We actually added in some bias terms here but I'll ignore those for the moment.
And we wanted to not have very common words dominate.", Can you explain how the log-bilinear model 'with vector differences' formula came out that? Which property of conditional probability was used? Any useful links? Timestamp 43:03
gvmfbePC2pc,"And just so you know I'm not cheating, there's a little slider here that rotates that third object.
Let's see, why are there just two known objects and one unknown? Well that's because I've restricted the motion to rotation around the vertical axis and some translation.
So now that I've spun that around a little bit, let me pick some corresponding points.
Oops.
What's happened? Wow.
Let me run that by again.
OK.
So there's one point I've selected from the model objects.
The corresponding point over here on the unknown is right there.
I'm going to be a little off.
But that's OK.
So let me just pick that one and then that corresponds to this one.",i dont understand the 17:20 . When 3 objects sufficient for 3 axis rotations and translation and 2 objects are sufficient for 1 axis rotation and translation. how so?
h0e2HAPTGF4,"And notice right here.
It's going to be really hard to separate those two examples from one another.
They are so close to each other.
And that's going to be one of the things we have to trade off.
But if I think about using what I learned as a classifier with unlabeled data, there were my two clusters.
Now you see, oh, I've got an interesting example.
This new example I would say is clearly more like a receiver than a lineman.
But that one there, unclear.
Almost exactly lies along that dividing line between those two clusters.
And I would either say, I want to rethink the clustering or I want to say, you know what? As I know, maybe there aren't two clusters here.
Maybe there are three.
And I want to classify them a little differently.
So I'll come back to that.
On the other hand, if I had used the labeled data, there was my dividing line.
This is really easy.","22:01 sir, that one black dot is it called noise?"
h0e2HAPTGF4,"It's called the Manhattan metric.
The one you've seen more, the one we saw last time, if p is equal to 2, this is Euclidean distance, right? It's the sum of the squares of the differences of the components.
Take the square root.
Take the square root because it makes it have certain properties of a distance.
That's the Euclidean distance.
So now if I want to measure difference between these two, here's the question.
Is this circle closer to the star or closer to the cross? Unfortunately, I put the answer up here.
But it differs, depending on the metric I use.
Right? Euclidean distance, well, that's square root of 2 times 2, so it's about 2.8.",35:37 how is it 2.8 and not 2?
h0e2HAPTGF4,"This is a standard way to do it, simply repeating what we had on an earlier slide.
If I want to cluster it into groups, I start by saying how many clusters am I looking for? Pick an example I take as my early representation.
For every other example in the training data, put it to the closest cluster.
Once I've got those, find the median, repeat the process.
And that led to that separation.
Now once I've got it, I like to validate it.
And in fact, I should have said this better.
Those two clusters came without looking at the two black dots.
Once I put the black dots in, I'd like to validate, how well does this really work? And that example there is really not very encouraging.
It's too close.
So that's a natural place to say, OK, what if I did this with three clusters? That's what I get.","One comment on a part of the lecture that sounded counterintuitive to me. In the football example (42:27) he was arguably working with four different types of football players, but professor Grimson was rejecting working with four clusters, being afraid of overfitting, in stead choosing three, while he was clearly aware of there being four different types of football players. I would have chosen four clusters to reflect the four different types of football players and start from there."
h0e2HAPTGF4,"This solid line has the property that all the Democrats are on one side.
Everything on the other side is a Republican, but there are some Republicans on this side of the line.
I can't find a line that completely separates these, as I did with the football players.
But there is a decent line to separate them.
Here's another candidate.
That dash line has the property that on the right side you've got-- boy, I don't think this is deliberate, John, right-- but on the right side, you've got almost all Republicans.
It seems perfectly appropriate.
One Democrat, but there's a pretty good separation there.
And on the left side, you've got a mix of things.
But most of the Democrats are on the left side of that line.
All right? The fact that left and right correlates with distance from Boston is completely irrelevant here.
But it has a nice punch to it.
JOHN GUTTAG: Relevant, but not accidental.
ERIC GRIMSON: But not accidental.
Thank you.
All right.
So now the question is, how would I evaluate these? How do I decide which one is better? And I'm simply going to show you, very quickly, some examples.",What does he mean by the choice of using distance of voters from Boston not being accidental at 46:45 - 46:55
h0e2HAPTGF4,"We'll see that, if the examples are well separated, this is easy to do, and it's great.
But in some cases, it's going to be more complicated because some of the examples may be very close to one another.
And that's going to raise a problem that you saw last lecture.
I want to avoid overfitting.
I don't want to create a really complicated surface to separate things.
And so we may have to tolerate a few incorrectly labeled things, if we can't pull it out.
And as you already figured out, in this case, with the labeled data, there's the best fitting line right there.
Anybody over 280 pounds is going to be a great lineman.
Anybody under 280 pounds is more likely to be a receiver.","20:40 Isn't it a better idea to draw a linear line through the data, and the perpendicular to that line would be the ""dividing line"" ?"
het9HFqo1TQ,"Right? So this is probability of all the values of y of y1 up to ym given all the xs and given, uh, the parameters theta parameterized by theta.
Um, this is equal to the product from I equals 1 through m of p of yi given xi parameterized by theta.
Um, because we assumed the examples were- because we assume the errors are IID, right, that the error terms are independently and identically distributed to each other, so the probability of all of the observations, of all the values of y in your training set is equal to the product of the probabilities, because of the independence assumption we made.
And so plugging in the definition of p of y given x parameterized by theta that we had up there, this is equal to product of that.
Okay? Now, um, again, one more piece of terminology.
Uh, you know, another question I've always been asked if you say, hey, Andrew, what's the difference between likelihood and probability, right? And so the likelihood of the parameters is exactly the same thing as the probability of the data, uh, but the reason we sometimes talk about likelihood, and sometimes talk of probability is, um, we think of likelihood.","32:18 I have a question about this likelihood function. Can somebody help me with it? According to the IID assumption, the probability of all the observations is equal to the product of each probability . However, isnt the expression a density instead of a probability of a normal distribution? I am really confused. I think the probability should be the integral of density function. If it's density, what's the meaning of the product of densities?"
het9HFqo1TQ,"[NOISE] And then you return Theta transpose x, right? So you fit a straight line and then, you know, if you want to make a prediction at this value x you then return say the transpose x.
For locally weighted regression, um, you do something slightly different.
Which is if this is the value of x and you want to make a prediction around that value of x.
What you do is you look in a lo- local neighborhood at the training examples close to that point x where you want to make a prediction.
And then, um, I'll describe this informally for now but we'll- we'll formalize this in math for the second.
Um, but focusing mainly on these examples and, you know, looking a little bit at further all the examples.
But really focusing mainly on these examples, you try to fit a straight line like that, focusing on the training examples that are close to where you want to make a prediction.
And by close I mean the values are similar, uh, on the x axis.
The x values are similar.
And then to actually make a prediction, you will, uh, use this green line that you just fit to make a prediction at that value of x, okay? Now if you want to make a prediction at a different point.
Um, let's say that, you know, the user now says, ""Hey, make a prediction for this point."" Then what you would do is you focus on this local area, kinda look at those points.
Um, and when I say focus say, you know, put most of the weights on these points but you kinda take a glance at the points further away, but mostly the attention is on these for the straight line to that, and then you use that straight line to make a prediction, okay.
Um, and so to formalize this in locally weighted regression, um, you will fit Theta to minimize a modified cost function [NOISE] Where wi is a weight function.","10:24 - How is this just not a form of interpolation using shape functions? That doesn't really seem like ""learning"" to me."
iTMn0Kt18tg,"So this is n squared.
All this work, still n squared.
Clearly what we need is for x to get smaller, too.
If x-- if, in this recursion-- let me, in red, draw the recursion I would like to have.
If x became x over 2 here, that's the only change we'd need.
Then n and x change in exactly the same way, and so then we can just forget about x-- it's going to be the same as n.
Then we get 2 times n over 2 plus order n.
Look familiar? It is our bread and butter recurrence-- merge sort recurrence.
That's n log n.
That's what we need to do.
Somehow, when we convert our set x to x squared, I want x to get smaller.
Is that at all plausible? Let's think about it.
What's the base case? To keep things simple let's say the base case when x equals 1, I'll just let x be-- let's say I want to compute my a at 1.
Keep it simple.
What if I want two values in x? I'd like to have the feature that when I square all the values in x-- so I want two values, but when I square them I only have one value.
Solve for x.
Yeah.
AUDIENCE: Negative 1 and 1? ERIK DEMAINE: Negative 1 and 1.
Whew, tough one.",in 42:06 I think we need to compute the sum of cost in each level not only the last !!!
iTMn0Kt18tg,"So, all right, 2 times-- because there's two subproblems-- each with size n over 2 and size x, plus what goes here is however much it costs to do the divide step, plus however much it costs to do the combined step-- all the non-recursive parts.
So this is just partitioning the vectors-- linear scan, linear time.
This is-- we talked about it-- it's a constant number of arithmetic operations for each x.
So this cause-order x time, this cause-order n time.
So, in general, we get n plus x.
Now, this is, again, not a recurrent solvable by the master method because it has two variables.
So usually when you're faced with this sort of thing, you want to do back of the envelope picture.
Draw a recursion tree.
That's a good way to go.
So at the root-- now, I know that initially x equals n.","Can someone explain why at 40:16 the last item is O(n + |x|)? Where does this n come from? I think it should be O(|X|) because once we get Aeven(x) and Aodd(x) for any x in X, it just constant time to compute Aeven(x) + x*Aodd(x) for each x. Now we have |X| points to computer, so it takes |X| time to get A(x) for x in X. Correct me if I am wrong"
iTMn0Kt18tg,"Turns out inverse is the right answer here, but Gaussian elimination would be the standard way to solve a linear system like that.
The trouble with Gaussian elimination is it takes cubic time in its normal form.
In this case it's a little bit special because this matrix is essentially fixed.
The xi's don't need to change.
It could be xi is just i or something, so we can-- in this case it's a little better to compute the inverse first.
So we could also just do v inverse times a.
From a numerical analysis standpoint this is very bad, but don't worry about it for now.
We're going to get a better algorithm today.
Anyway, it doesn't involve matrices at all, but the nice thing is, if computing the inverse, that takes n cubed time, but you only have to do it once.
So if you have to do this many times you can do this product in n squared time.
You just have to maintain that v inverse once and for all.
OK, great.
So, we've got quadratic algorithms to go back and forth between these representations.
That, at least, tells us it's doable, but we, of course, need better than quadratic to improve on the naive multiplication algorithm, so that's what we're going to do.
In general, we can't do any better, but we have one freedom, which is we have said nothing about the x case.
We can choose them to be whatever we want them to be.
I keep saying xk equals k.
That seems fine.",I think he meant V\Y not V\A at around 29:00...
iTMn0Kt18tg,"When you multiply those together, you get x to the k, so this is the coefficient of x to the k.
Cool? So, that's what we'd like to compute.
Given a and b we want to compute this polynomial c.
How long does it take? We have to do this for all k.
So, to compute the k-th term takes order k time, so the total time is n squared.
So, that's not good for this lecture.
We want to do better.
In fact, today we will achieve n log n.
That's our goal-- polynomial multiplication in n log n.","Am I the only one who thinks the Ck is wrong at 9:25
9:54 can anybody why it is O(n^2) ?"
iTMn0Kt18tg,"You get the inverse.
Cool.
Very cool because what this tells us is we run exactly the same algorithm and do exactly the same transformation.
If we want to do the inverse we can actually just use v, but with a different choice of xk.
Namely, for the inverse, we'll call it xk inverse.
We just take the complex conjugate of this thing, which turns out to be e to the minus ijk tau over n, and then divide the whole thing by n.
I'm using a fact here, which is that the complex conjugate of this number is actually, just, you put a minus sign here.
Why does that hold? Because geometry.
Theta is usually measuring the counterclockwise angle from the x-axis.","1:07:35 if the complex conjugate is just minus the power in the exponential, why did he write exp(-ijkT/n/n)? why the divide by n divide by n (again)? is it a mistake?
 1:07:35 if the complex conjugate is just minus the power in the exponential, why did he write exp(-ijkT/n/n)? why the divide by n divide by n (again)? is it a mistake? (also sorry I asked as subcomment; I thought it'd get lost in the clutter otherwise)"
iTRW9Gh7yKI,"And then we're going to run this, uh, recursion for k iterations and whatever we end up in the end with, whatever embedding we end up at the end, that is what we call, uh, final embedding.
Now, what is- benefit of GCN is that it is so simple.
We can very nicely write it into the- in the matrix form.
So the way we are going to write it in the matrix form is that we are going to take these embeddings and we are going to stack them into an embedding matrix.
And then A is our adjacency matrix where every node also has a self-loop.
Then the way you can write the sum over the neighbors of a given node- sum of the embeddings of a- of a given no- of the neighbors of a given node, you can simply write this as a- as a product between the adjacency matrix A and the- and the embedding matrix H.
And, uh, what this means is that now, we can also define this notion of D to be a diagonal matrix where we- it is all of 0 only on the diagonal, we have the degree of every node.","I think there's a typo in 2:24 In the first bullet point, h^(1)_1 should be h^(k)_1"
iZTeva0WSTQ,"ouple of announcements, uh, before we get started.
So, uh, first of all, PS1 is out.
Uh, problem set 1, um, it is due on the 17th.
That's two weeks from today.
You have, um, exactly two weeks to work on it.
You can take up to, um, two or three late days.
I think you can take up to, uh, three late days, um.
There is, uh, there's a good amount of programming and a good amount of math you, uh, you need to do.
So PS1 needs to be uploaded.
Uh, the solutions need to be uploaded to Gradescope.
Um, you'll have to make two submissions.
One submission will be a PDF file, uh, which you can either, uh, which you can either use a LaTeX template that we provide or you can handwrite it as well but you're strongly encouraged to use the- the LaTeX template.
Um, and there is a separate coding assignment, uh, for which you'll have to submit code as a separate, uh, Gradescope assignment.
So they're gonna- you're gonna see two assignments in Gradescope.
One is for the written part.
The other is for the, uh, is for the programming part.
Uh, with that, let's- let's jump right into today's topics.
So, uh, today, we're gonna cover, uh- briefly we're gonna cover, uh, the perceptron, uh, algorithm.
Um, and then, you know, a good chunk of today is gonna be exponential family and, uh, generalized linear models.
And, uh, we'll- we'll end it with, uh, softmax regression for multi-class classification.
So, uh, perceptron, um, we saw in logistic regression, um.
So first of all, the perceptron algorithm, um, I should mention is not something that is widely used in practice.
Uh, we study it mostly for, um, historical reasons.
And also because it is- it's nice and simple and, you know, it's easy to analyze and, uh, we also have homework questions on it.
So, uh, logistic regression.
Uh, we saw logistic regression uses, uh, the sigmoid function.
Right.
So, uh, the logistic regression, uh, using the sigmoid function which, uh, which essentially squeezes the entire real line from minus infinity to infinity between 0 and 1.","At about 1:07, ""These ""Ys"" and ""xs"" would have been sampled"". I thought for Sufficient Statistics, the Bernoulli distribution would not need to be sampled, it is assumed to have enough data, as a GLM?"
iZX8WEGZTVw,"And I'm interested in the way that these probabilities update after one step.
If p prime B is the probability of being in state B after one step, and p prime O is the probability of being in the orange state one step later-- and likewise for green-- what are these probabilities? Well it's easy to see just reading off this graph that the only place you're at is B.
So the only way to get probability of being somewhere is by following an edge out of B.
So the probability of being at one step at the orange vertex is 1/4.
And it's likewise 1/4 for being at the green state.
And it's 1/2 for staying at the blue state.
So what we can say is that the updated probabilities of being at these different states is 1/2, 1/4, and 1/4, as we've just reasoned.
OK.
Let's keep going.
Given that the probability that I'm at the states blue, orange, and green are given by this vector of probabilities, what's the distribution after two steps? So let p double-prime B be the probability of being at state B after two steps, starting from B.
Well, the way we can figure that out is by using conditional probabilities.
Let's look at the example of calculating the probability of being in the orange state two steps after you've started at the blue state.
So here was the probabilities of being at the different states after one step.
How do I get to the orange state? Well I can get to the orange state from the blue state.","Is the green state supposed to read ""G"" at 0:58 ?"
j080VBVGkfQ,"Uh, what about for, what action do we take from S2? Right.
So for that one, we get a 1.
So we basically, uh, distributing your experience.
So now if you were going to take a max over those, then you would get the same thing that we saw last time for Monte Carlo, which would be 11100000 to the end, um, but here we- we're subdividing our samples.
So, you only get to get an experience for the action that you actually took in the state.
And because we're in the Monte Carlo case, we'll see the TD case or, Q-learning we'll call it later, um, then we get to add up all the rewards to the end of the episode.
So G here is gonna be the sum of all these steps, and I didn't speci- oh, I did.
Good.
And we're keeping Gamma equal to 1 here just to make all the math.
Just adding.
Yeah? Should we just [OVERLAPPING].
Sorry.
Can just be one half for Q S1A1 or in Q S3A1? Uh, is talking about whether or not if we did every visit, if anything would change here.
[NOISE] Excuse me.
It would not change in this case, because, um, both times when you visited S3, the sum of rewards to the end of the episode was 1.","26:23 I think the reason why people got a bit confused and obatain different answers was because they forgot the essence of MC for policy evaluation. For MC policy evaluation, it will start only when a full episode is completed. In this case, therefore, G_{i,t} for all existed state-action pair (s3,a1), (s2,a2), and (s1,a1), are all 1 as gamma is zero. Then just follow the pesudo code for MC, you will get the right answer. If you are doing TD, where policy evaluation starts immediately without waiting for the completion of the entire episode, I think the first student's answer was correct."
j1H3jAAGlEA,"And it is informed, because it's taking advantage of this extra information.
It may not be in your problem.
It's not often the case you've got this information in a map.
Your problem may not have any heuristic measurement of distance to the goal.
In which case, you can't do it.
But if you've got it, you should use it.
Oh, yeah, there's one more.
And I've already given it away by having it on my chart.
It's called Beam Search.
And just as Hill Climbing is an analog of Depth-first Search, Beam Search is a complement or addition of an informing heuristic to Breadth-first Search.
What you do is you start off just like Breadth-first Search.
But you say I'm going to limit the number of paths I'm going to consider at any level to some small, fixed number, like, in this case, how about two.
So, I'm going to say that I have a Beam of two for my Beam Search.
Otherwise, I proceed just like Breadth-first Search, b, d, a, g.","Beam Search: At 32:24, how is Node B connected to Node G ? There is no path drawn from B to G on the map ?"
j1H3jAAGlEA,"And the reason is that there are large lawns on the shoulders of Mt.
Washington.
It's quite flat.
So, it's the telephone pole problem.
That space looks like this.
Well, this isn't what Mt.
Washington looks like.
But it's the telephone pole problem.
So, when you're wandering around here, the idea of trying a few directions and picking the one that's best doesn't help any, because it's flat.
That can be a problem with Hill Climbing.
Now, there's one more problem with Hill Climbing that most people don't know about.
But it works like this.
This is a particularly acute problem in high dimensional spaces.
I'll illustrate it here just in two.
And I'm going to switch from a regular kind of view to a contour map.
So, my contour map is going to betray the presence of a sharp bridge along the 45 degree line.",39:05 I dont get the 2nd drawback of Hill climbing He said telephone pole problem what is that ?
j1H3jAAGlEA,"But of course, all isn't lost.
Because we have the choice of backing up to the place where we last made a decision and choosing another branch.
So, that process is called variously back-up or backtracking.
At this point, we would say, ah, dead end.
The first place we find when we back up the tree where we made a choice is when we chose b instead of d.
So, we go back up there and take the other route.
s, a, d now goes to g.
And we're done.
We're going to make up a little table here of things that we can embellish our basic searches with.
And one of the things we can embellish our basic searches with is this backtracking idea.
Now, backtrack is not relevant to the British Museum algorithm, because you've got to find everything.
You can't quit when you've found one path.
But you'd always want to use backtracking with Depth-first Search, because you may plunge on down and miss the path that gets to the goal.
Now, you might ask me, is backtracking, therefore, always part of Depth-first Search? And you can read textbooks that do it either way.
Count on it.
If we give you a Search problem on a quiz, we'll tell you whether or not your Search is supposed to use backtracking.
We consider it to be an optional thing.
You'd be pretty stupid not to use this optional thing when you're doing Depth-first Search.
But we'll separate these ideas out and call it an optional add-on.
so, that's Depth-first Search, very simple.
Now, the natural companion to Depth-first Search will be Breadth-first Search, Breadth-first.
And the way it works is you build up this tree level by level, and at some point, when you scan across a level, you'll find that you've completed a path that goes to the goal.",9:42. Isn't the backtracking idea contrary to the rule of not biting one's tail ?
j1H3jAAGlEA,"But, of course, I keep s, a, d and s, b on the queue.
Now, I take the front off the queue again, and I get s, a, b, c, e, and not to forget s, a, d and s, b.
I take the first one off the queue.
It doesn't go to the goal.
I try to extend it, but there's nothing there.
I've reached a dead end.
So, in this operation, all I'm doing is taking the front one off the queue and shortening the queue.
We're almost home.
I take s, a,d off of queue.
And I get s, a, d, c.
And, of course, I still have s, b.
Now, the next time I visit the situation, buried in that first step, I discover a path that actually does get to goal, and I'm done.
So, each time around I visualize the queue.
I check to see if I'm done.
If not, I take the extensions and put them somewhere on the queue.",18:52 Doesn't DFS use a stack and BFS use a queue?
j1H3jAAGlEA,"In Hill Climbing Search, just like a Depth-first Search, we have a and b.
And we're still going to list them lexically on underneath the parent node.
But now which one is so closer to the goal? Now, this time b is closer to the goal than a.
So, instead of following the Depth-first course, which would take us down through a, we're going to go to the one that's closest which goes through b.
And b can either go to a or c.
b is six units away from the goal.
a is about seven plus, not drawn exactly to scale.","Hill Climbing Search: At 27:53 why does the instructor say that Node B is closer to the goal than A? From the route map, given the weights, I understand that A is just 8 units away from goal i.e S -> A -> D -> G = 11 units and via node B it is S -> B -> A -> D -> G = 17 units. So why would hill climbing search take the node B path ?
I'm new to this stuff, but I think the program calculates the distance to the target as the crow flies, without minding the paths. Otherwise it wouldn't have chosen the path B as the nearest one (27:52). I don't know how it'd calculate such a thing if there wasn't a map, since without map it wouldn't be able to count pixels or somehow understand the shortest distance..."
j1H3jAAGlEA,"In here there's an explicit step where I've checked to see if that first path is a winner.
If it's not, I extend it.
And I have to put those paths onto the queue.
So, I'll say that what I do is I end queue.
Now, I've done one step.
And let's let me do another step.
I'm going to take this first path off.
I'm going to extend that path.
And where do I put these new paths on the queue if I'm doing Depth-first Search? Well, I want to work with the path that I've just generated.
I'm taking this plunge down deep into the search tree.
So, since I want to keep going down into the stuff that I just generated, where then do I want to put these two paths? At the end of the queue? I don't think so, because it'll be a long time getting there.
I want to put them on the front of the queue.
For Depth-first Search, I want to put them on the front of the queue.
And that's why s, a, b goes here, and s, a, d, and then that's s, b.
So, s, b is still there.
That's still a valid possibility.
But now I've stuck two paths in front of it, both of the ones I generated by taking a path off the front of the queue, discovering that it doesn't go to the goal, extending it and putting those back on the queue.
I might as well complete this illustration here.
While I'm at it, I take the s, a, b off, s, a, b, and I can go only there to c.","Helped me understand a lot thanks! Although, at 16:55, on enqueueing the paths in the front of the queue, doesn't that violate the FIFO property of a queue?"
j1H3jAAGlEA,"So, level by level, s can go to either a or b.
a can go either to b or d.
And b can go to either a or c.
So, you see what we're doing.
We're going level by level.
And we haven't hit a level with a goal in it yet, so we've got to keep going.
Note that we're building up quite a bit of stuff here, quite a lot of growth in the size of the path set that we're keeping in mind.
At the next level, we have b going to c, d going to g, a going to d, and c going to e.
And now, when we scan across, we do hit g.
So, we found a path with Breadth-first Search, just as we found a path with Depth-first Search.
Now, you might say, well, why didn't you just quit when you hit g? Implementation detail.
We'll talk about a sample implementation.
You can write it in any way you want.
But now that we know what these searches are, let's speed things up a little bit here and do a couple searches that now have names.
The first type will be Depth-first, boom.
That's the one that produces the thief path.
And then we can also do a Breadth-first Search, which we haven't tried yet.
What do you suppose is going to happen? Is it going to be fast, slow, produce a good path, produce a bad path? I don't know, let's try it.
I had to speed it up, you see, because it's doing an awful lot of Search.
It's generating an awful lot of paths.
Finally, you got a path.
Is it the best path? I don't think so.
But we're not going to talk about optimal paths today.
We're just going to talk about pretty good paths, heuristic paths.
Let's move the starting position here in the middle.","13:10 Sir, does breadth first search require a high calculation time compared to depth first search?"
j9AcEI98C0o,"This dataset has 2.5 thousand training examples.
And before sort of the big transformers came around we had 60% accuracy on it.
We run transformers on it, we get 10 points just by pretraining.
And this has been a trend that is just continued.
So why do anything, but pretrain encoders? But like we know encoders are good, and we like the fact that you have bidirectional context.
We also saw that BERT did better than GPT.
But it has-- if you want to actually get it to do things, right? You can't just generate it-- generate sequences from it the same way that you would from a model like GPT, a pretrained decoder, right? You can sort of sample what things should go in a mask.
So here's a mask, you can put a mask somewhere, sample the words that should go there.
But if you want to sample the whole context, right, if you want to get that story about the unicorns, for example, the encoder is not what you want to do.
So they have sort of different contracts in there.
It can be used naturally at least in different ways.
OK.
So let's talk very briefly about extensions of BERT.
So there are variants like RoBERTa and SpanBERT, and there's just a bunch of papers with the word BERT in the title, that did various things.","Why not add a [MASK] at the end of a sentence, and iterate, as a generative method using the encoder (1:09:30)?"
jGwO_UgTS7I,"My advice to students is that um, CS229, uh, CS229a, excuse me, let me write this down.
I think I'm- so CS229a, uh, is taught in a flipped classroom format which means that, uh, since taking it, we'll mainly watch videos um, on the Coursera website and do a lot of uh, programming exercises and then, meet for weekly discussion sections.
Uh, but there's a smaller class with [inaudible] .
Um, I, I would advise you that um, if you feel ready for CS229 and CS230 to do those uh, but CS229, you know, because of the math we do, this is a, this is a very heavy workload and pretty challenging class and so, if you're not sure you're ready for CS229 and CS229a, it may be a good thing to, to, to take first, uh, and then uh, CS229, CS229a cover a broader range of machine learning algorithms uh, and CS230 is more focused on deep learning algorithms specifically, right.
Which is a much narrow set of algorithms but it is, you know, one of the hardest areas of deep learning.
Uh, there is not that much overlap in content between the three classes.
So if you actually take all three, you'll learn relatively different things from all of them uh, in the past, we've had students simultaneously take 229 and 229a and there is a little bit of overlap.
You know, they, they do kind of cover related algorithms but from different points of view.
So, so some people actually take multiple of these courses at the same time."," It may be that the coursera one is exactly CS229A. Because when Andrew is explaining the difference between CS229 and CS229A (30:20) he says that CS229A is on coursera and the format described seems to be the same, but I'm not sure if that's the case. he starts talking about it at 29:39 btw."
jGwO_UgTS7I,"Um, uh, we actually kept the enrollments to CS229a at a relatively low number at 100 students.
So I actually don't want to encourage too many of you to sign up because uh, I think we might be hitting the enrollment cap already so, so please don't all sign up for CS229a because um, we- CS229a, does not have the capacity this quarter but since CS229a is uh, um, much less mathematical and much more applied, uh, uh, a relatively more applied version of machine learning and uh, so I, I guess I'm teaching CS229a and CS230 and CS229, this quarter.
Of the three, CS229, is the most mathematical.
Um, it is a little bit less applied than CS229a which is more applied machine learning and CS230 which is deep learning."," It may be that the coursera one is exactly CS229A. Because when Andrew is explaining the difference between CS229 and CS229A (30:20) he says that CS229A is on coursera and the format described seems to be the same, but I'm not sure if that's the case. he starts talking about it at 29:39 btw."
kHyNqSnzP8Y,"So there's a probability measure that's produced from the fitnesses.
Yeah.
STUDENT: You need to make sure that the fitnesses aren't negative.
PATRICK WINSTON: Have to make sure the fitnesses are what? STUDENT: Aren't negative.
PATRICK WINSTON: He says I have to make sure the fitnesses aren't negative.
Yeah, it would be embarrassing if they were.
So we'll just truncate anything like that as 0.
You've got a lot of choice how you can calculate the fitness.
And maybe you will produce negative numbers, in which case you have to think a little bit more about it.
So now, what about an example? Well, I'm going to show you an example.
Why don't I show you the example.","There's an incomplete subtitle line here: 13:59: ""So we'll just truncate anything like that at 0"" Translations are locked so I can't correct it. MIT pls fix"
kh3I_UTtUOo,"Um, here, I call it loopy belief propagation, because in practice, people tend to apply this algorithm to, um, graphs that have cycles or loops as well, even though than any kind of convergence guarantees and this kind of probabilistic interpretation that I gave here, uh, gets lost.
So, um, if you consider graphs with cycles, right, there is no- no longer a fixed ordering on- of the nodes which is, er, which, um, otherwise the fixed ordering exists in terms if the graphs are trees.
But if a graph has a cycle, you cannot- you cannot, uh, sort, uh, the nodes, uh, in- in a- in a- in a nice order.
And basically the idea is that we apply the same algorithm as in the previous slides, but we start from arbitrary nodes and then follow the edges to update the messages.
So basically, we kind of propagate this in some kind of, uh, random order, again, until it converges or until, uh, some fixed number of, uh, iterations is, uh, is reached.","Is there some mistake in indexing at 15:15? On the previous page, messages (\Pi term) come from all neighbors but j. Here, we are taking all neighbors including j (with an ambiguous iterative notation called j) and then we are having Y_j when the message is from m_{j \right i}. Shouldn't it be m_{j \right_i}(Y_i)?"
krZI60lKPek,"I claim this is a perfect hash function with certain probability.
Why? Because I want to calculate probability no collision.
Yeah, 1 minus probability I do have a collision.
And I can use a union bound.
That's the probability that any pair has a collision.
Any pair of hx equals hy.
How many pairs do I have? AUDIENCE: N choose 2.
LING REN: Yeah.
n choose 2, which is this number.
So if it's a universal hash function, then any collision, any two colliding, the probability is 1 over m.
So I choose my m to be n squared, so this one is larger than 1/2.
So what I'm saying, to get a perfect hash function, I'll just use the simplest way.
I select the universal hash function with m equals n squared.
I have a probability more than 1/2 to succeed.
Or if I don't succeed, I'll choose another one until I succeed.
So this is a randomized algorithm, and we can make it a Monte Carlo algorithm or Las Vegas algorithm.
So I can either say if I choose alpha log n times, then what's the chance that none of my choice satisfies perfect hashing? My failure probability is less than this.",46:11 second line that should be an inequality
krZI60lKPek,"So P equals NP.
SO we should all be getting Turing award for that.
So clearly something's wrong.
But there's no problem with this solution.
This covers all the cases.
And our analysis is definitely correct.
So does anyone get what I'm asking? So what's the contradiction here? I will probably discuss this later, in later lectures when we get to complexity or reduction.
But to give a short answer, the problem is that when we say the input is n, its size is not n.
So I only need log n this to represent this input.
Make sense? Therefore, for log n length input, my runtime is n.
That means my runtime is exponential.",13:46 could anyone explain why is it logN? I'm really confused here...
l-tzjenXrvI,"And if I've done it right, you get something that looks like so.
It's a little hard to see what's going on on the drawing itself.
So let me number these.
Now I have an easier-to-deal-with picture.
There are two links between 1 and 2 and 1 and 3.
One link between 2 and 3.
One between 2 and 4.
Two here.
Two here.
And one each from all of these.
So Guzman produces this evidence for how the problem ought to be solved and then he begins to think about various ways of using the evidence.
So he could, and did, decide that one link is enough to presume that the faces belong to the same object.
And you can see that that's a little bit too liberal.
Because that says that the whole thing is just one object.",6:50 you forgot 3 to 7
l-tzjenXrvI,"I'm fusing it with the table because I want to consider it to be one object.
We can view it from the three objects indicated by those three little chalk pieces.
And ask ourselves, well, in the event that we look at it from those three places, what do we see? And if we look at it from the perspective of the piece of purple chalk-- I'll have to walk around and be sure-- looks like an arrow junction with two concaves and a convex.
Did I get that right? So I'm looking at it from this perspective.
It's an arrow.
This is convex and these two are concave because I fused the paper box with the table.
Looking at it from the perspective of this blue guy-- let me rotate it so you can have a better understanding of the blue guy-- it looks like a concave line and a boundary.
So it's an L.
And this one is a boundary.
And that's concave.
And by a kind of symmetry, we're also going to get that one from the other side, the third of the three octants.
Well, we're off and running.
But we still have an awful lot to go.
And we could manage to deal with it by thinking about this object once again.
But instead of this situation out here, to turn it around and look at this vertex.
Think about the junctions that it can produce.","I got that but didnot understand at 21:30 why the line at the bottom is concave rather than a + at the bottom. Is it because of the angle one view that junction from?
hiI am confused about the direction of the arrow at 21:37,can you please help me to explain why
21:02 How are 2 convex and one concave? Shpuldnt they all be concave ?"
lDwow4aOrtg,"So for example, hypothetically you could impose a constraint, the normal w is equal to 1, or another way to do that would be to take w and b and replace it with w over normal b and replace b with, [NOISE] right, just the value of parameters through by the magnitude, by the- by the Euclidean length of the parameter vector w, and this doesn't change any classification, It's just rescaling the parameters.
Ah, but, ah, but, but that it prevents, you know, display of cheating on the functional margin.
Okay.
Um, and in fact, more generally you could actually scale w and b by any other values you want and- and it doesn't- doesn't matter, right? You can choose to replace this by w over 17 and b over 17 or any other number or any, right, and the classification stays the same.
Okay.
So we'll come back and use this property, in a little bit.
Okay.
[NOISE] All right.
So to find the functional margin, let's define the geometric margin.","1:11:20 I don't understand how w/17, b/17 can prevent increasing functional margin by just increasing weight and bias?"
lDwow4aOrtg,"This is kinda according to Bayes rule.
Okay? Um, all right.
So it turns out this algorithm will almost work and here's where it breaks down, which is, um, you know, so actually eve- every year, there are some CS229 students and some machine learning students, they will do a class project and some of you will end up submitting this to an academic conference.
Right? Some, some- actually some, some of CS229 class projects get submitted, you know, as conference papers pretty much every year.
One of the top machine learning conferences, is the conference NIPS.
NIPS stands for Neural Information Processing Systems, um, ah, and let's say that in your dictionary, you know, you have 10,000 words in your dictionary.
Let's say that the NIPS conference, the word NIPS corresponds to word number 6017, right? In your, in your 10,000 word dictionary.
But up until now, presumably you've not had a lot of emails from your friends asking, ""Hey, do you want to submit the paper to the NIPS Conference or not."" Um, and so if you use your current, you know, email, set of emails to find these maximum likelihood estimates of parameters, you will probably estimate that, um, probability of seeing this word given that it's spam email, is probably zero.
Right? Zero over the number of, ah, examples that you've labeled as spam in your email.
So if, if you train up this model using your personal email, probably none of the emails you've received for the last few ones had the word NIPS in it, um, maybe.
Uh, and so if you plug in this formula for maximum likelihood estimate, the numerator is 0 and so your estimate of this is probably 0.
Um, and then similarly, this is also 0 over, you know, the number of non-spam emails I guess.
Right.","In 19:15 wouldn't it be more accurate to say multinouli instead of multinomial, since the concept of number of trials that's a parameter of the multinomial distribution doesn't really apply here?
at 35:21 shouldnt there be ni in general instead of the 10000 that is being added"
lDwow4aOrtg,"Um, so here's what I mean.
Uh, we're gonna go to binary classification, and we're gonna use logistic regression, right? So, so let's, let's start by motivating this with logistic regression [NOISE].
So this, this is a classifier H of theta equals the logistic function of pi to theta transpose x.
And so, um, if you turn this into a binary classification, if, if, if you have this algorithm predict not a probability but predict 0 or 1, then what this classifier will do is, uh, predict 1.
If theta transpose x is greater than 0, right? Um, and predict 0 otherwise.
Okay.
Because theta transpose x greater than 0, this means that, um, g of theta transpose x is greater than 0.5 [NOISE], and you can have greater than or greater than equal to, it doesn't matter.","just had a doubt....... at 54:56 , what does g(z) denote ? is it the sigmoid function ?"
lNHaZlZJATw,"Is it faster or is it more advantageous to use that? Is it more advantageous to use that thing? It could be more advantageous, um, for- for, um, it so happens that the- the situations where we try- where we need to use, um, uh, uh, SGD or mini-batch SGD happen to be with deep learning or neural networks, where the cost function is not convex, right? And once you have, uh, uh, a nonconvex cost function, it's very hard to, um- it's very hard to analyze and make precise statements of what helps and what doesn't help.
And so in- in- in those situations, it- the answer is almost always try and see if it works better.",1:07:11 wouldnt be mini batch gradient descent itself be prone to overfit to that batch since we actually reduce the training set?
lU_QT5GSuxI,"It's a pretty tight bounds.
What that means is that informally speaking, the sum of the logs is about this term plus that term.
Plus, let's take the average value of that term, which is half this term.
So we could say that the sum of logs is approximately equal.
That's a little vague, but live with it.
n log n over e plus half of log n.
Well, now, if I'm interested, remember, in an estimate for n factorial-- so let's exponentiate both sides.","3:49 why are you ignoring the +1, that is comming out of the definite integral on the left side?"
leXa7EKUPFk,"And if this is an and tree, are there any and nodes? Sure, there's one right there.
So do you think then that you can answer questions about your own behavior as long as you build an and-or tree? Sure.
Does this mean that the integration program could answer questions about its own behavior? Sure.
Because they both build goal trees, and wherever you got a goal tree, you can answer certain kinds of questions about your own behavior.
So let me see if in fact it really does build itself a goal tree as it solves problems.
So this time, we'll put B6 on B3 this time.
But watch it develop its goal tree.
So in contrast to the simple example I was working on the board, this gets to be a pretty complicated goal tree.
But I could still answers questions about behavior.
For example, I could say, why did you put B6 on B3? Because you told me to.
All right, so the complexity of the behavior is largely a consequence not of the complexity of the program in this particular case, but the building of this giant goal tree as a consequence of the complexity of the problem.",14:49 Wait. It is just not a AND tree... there is more information than just AND. what about the ORDER / SEQUENCE of execution? a simple AND of all those action will not result in proper action. Where is that information coded / represented?
leXa7EKUPFk,"PROFESSOR PATRICK WINSTON: Yeah, but how would it do that? How would the program do that? We know that answering a why question makes it go up one level.
How does it answer a how question? Sebastian? SEBASTIAN: It goes down one level.
PROFESSOR PATRICK WINSTON: You go down one level.
So you start off all the way up here with a put-on.
You will say, oh, well I did these four things.
You say, why did you grasp B1? It will say because I was trying to clear its top.
Why did you clear its top? Because I was trying to get rid of it.","13:35 he made a mistake, he was answering the why again in the opposite sens."
leXa7EKUPFk,"Why were you trying to get rid of it? Because I was trying to put it on the table.
So that's how it answers how questions, by going down in this tree and this trace of the program of action so as to see how things are put together.
What are these things that are being put together? What's the word I've been avoiding so as to bring this to a crescendo? What are these objectives, these things it wants to do? They're goals.
So this thing is leaving a trace, which is a goal tree.
Does that sound familiar? Three days ago, we talked about goal trees in connection with integration.
So this thing is building a goal tree, also known as an and-or tree.
So this must be an and tree.",at 13:50 he is going down for 'how' questions but he says 'why'. Actually he means 'how' not 'why' there...
mJQrtXZT5pw,"So now you can say, I have these nodes, they have these spokes.
These are kind of these rough edges.
And now I wont to randomly connect, uh, these, uh, endpoints.
And of course, maybe between a pair of nodes, I will allow multiple edges because perhaps both of these two end points, randomly, you know, by chance decide to connect to these two end points so it'll be kind of a double-edge.
But for the purpose of this, uh, discussion right now, that's completely fine and okay.
So then, you can ask yourself, what is the expected number of edges between a pair of nodes i and j, where node i has degree, uh, k_i, and node j has deg- degree k_j.","9:35 I suspect there is a math simplification. If our process for creating random edges is that we choose at random node (i) and randomly choose one of (k_i) stub for this node and connect to any other free stub (which is 2m-1, because one stub we already took from node (i) ), chance that stub from node (i) connects to other stub of node (j) is k_j / (2m - 1). And to continue process further dont we need to count already taken stubs, because constructing edges in such way wont be independent? Correct me if Im wrong."
mUBmcbbJNf4,"You don't think of any of this when you're just trying to run an algorithm on one computer.
So distributed algorithms can be pretty complicated.
It's not easy to design them.
And after you design them, you still have to make sure they're correct.
So there are issues involved in proving them correct and analyzing them.
A little bit of history, the field pretty much started around the late '60s.
Edsger Dijkstra was one of the earliest leaders in the field.
He won of the first Turing Awards.
Leslie Lamport won the Turing Award last year.
Although he actually started as a very young guy, way back in the early days of the field.
If you want to look at some sources, I have a book.
There's another textbook by Attiya and Welch.
There's a new series of monographs that basically try to summarize many of the important research topics in distributed computing theory.
And the last two lines have a couple of the main conferences in the field.
OK so I can't do that much in one week.",what are the monographs that she is referring in the start of the lecture ( 3:06 ) and where can i find them ?
n0lce1dMAh8,"So let's think about words of length n that satisfy the password conditions.
So Pn is going to be the length n words starting with a letter, which is one of the password constraints.
So we can express that has a length n word can be broken up into the first character, which is an L, paired with the rest of the word-- the remaining n minus 1 characters.
And the remaining n minus 1 characters can be either L's or D's.
So though length n passwords can be expressed as the product of L with the n-th power of L union D-- that is, L union D cross L union D cross L union D, n minus 1 times.
Well, now we have an easy way to count this, because the size of this product by the product rule is the size of L times the size of L union D to the n minus first power.
And of course, L union D, since letters and digits don't overlap by the sum rule, the size of them is just L plus D.",1:46 - it should be [ L U D ] ^n
n0lce1dMAh8,"That's one simple example where I'm translating a spec into because something that I can express easily as a products and disjoint sums of stuff that I already know the size of.
Let's just do another example.
Suppose that I want to count the number of 4-digit numbers.
So the elements of these 4-digit numbers are 0 through 9-- there are 10 possibilities-- with at least one 7-- the number of 4-digit sequences of digits that have at least one 7 in them.
And one way to count is I can make it a sum of different 4-digit numbers containing one 7, depending on where the first 7 is.","You're calculating the possibility of at least 1 7 at 3:30, but you seem to be calculating the possibility of exactly 1 7."
nt63k3bfXS0,"Um, and so here the feature vector is 0, 1 to the n, because there's a n-dimensional binary feature vector, where- where for the purpose of illustration, let's say, n is 10,000 because you're using, you know, take the top 10,000 words, uh, that appear in your e-mail training set as the dictionary that you will use.
So, um.
So in other words, X_i is indicator word i appears in the e-mail, right? So it's either 0 or 1 depending on whether or not that word i from this list appears in your e-mail.
Now, um, in the Naive Bayes algorithm, we're going to build a generative learning algorithm.
Um, and so we want to model P of x given y, right? As well as P of y, okay? But there are, uh, 2 to the 10,000 possible values of x, right? Because x is a binary vector of this 10,000 dimensional.
So we try to model P of x in the straightforward way as a multinomial distribution over, you know, 2 to the 10,000 possible outcomes.
Then you need, right, uh, uh, you need, you know 2 to the 10,000 parameters, right? Which is a lot, or technically, you need 2 to 10,000 minus 1 parameter because that adds up to 1, and you can see one parameter.",At 1:09:48 why do we need 2power10000 parameters i think its 2*10000 because each dimension needs two parameters and variance is same anyways.
nykOeWgQcHM,"And the last one deals mostly with the computer science part in Introduction to Programming and Computer Science in Python.
We're going to talk about, once you have learned how to write programs in Python, how do you compare programs in Python? How do you know that one program is better than the other? How do you know that one program is more efficient than the other? How do you know that one algorithm is better than the other? That's what we're going to talk about in the last part of the course.
OK.
That's all for the administrative part of the course.
Let's start by talking at a high level what does a computer do.","6:57 What does a computer do
6:29 how do you know what algorith is better thsn the other"
nykOeWgQcHM,"How do you represent knowledge with data structures? That's sort of the broad term for that.
And then, as you're writing programs, you need to-- programs aren't just linear.
Sometimes programs jump around.
They make decisions.
There's some control flow to programs.
That's what the second line is going to be about.
The second big part of this course is a little bit more abstract, and it deals with how do you write good code, good style, code that's readable.
When you write code, you want to write it such that-- you're in big company, other people will read it, other people will use it, so it has to be readable and understandable by others.
To that end, you need to write code that's well organized, modular, easy to understand.
And not only that, not only will your code be read by other people, but next year, maybe, you'll take another course, and you'll want to look back at some of the problems that you wrote in this class.
You want to be able to reread your code.
If it's a big mess, you might not be able to understand-- or reunderstand-- what you were doing.
So writing readable code and organizing code is also a big part.
And the last section is going to deal with-- the first two are actually part of the programming in Introduction to Programming and Computer Science in Python.","Question: At 5:04, on the 2nd line of the slide, should that be branching instead of recursion?"
nykOeWgQcHM,"If you want to go on the internet, send email with it, you can't.
It can only do this one thing.
And if you wanted to create a machine that did another thing, then you'd have to create another fixed-program computer that did a completely separate test.
That's not very great.
That's when stored-program computers came into play.
And these were machines that could store a sequence of instructions.
And these machines could execute the sequence of instructions.
And you could change the sequence of instructions and execute this different sequence of instructions.
You could do different tasks in the same machine.
And that's the computer as we know it these days.",17:01 - von Neumann architecture right?
nykOeWgQcHM,"In Python, And it'll retrieve these lost values, and it'll reuse them for new values, and things like that.
But radius now points to the new value.
We can never get back 2.2.
And that's it.
The value of area-- notice, this is very important.
The value of area did not change.
And it did not change because these are all the instructions we told the computer to do.
We just told it to change radius to be radius plus 1.
We never told it to recalculate the value of area.
If I copied that line down here, then the value of area would change.
But we never told it to do that.
The computer only does what we tell it to do.
That's the last thing.
Next lecture, we're going to talk about adding control flow to our programs, so how do you tell the computer to do one thing or another? All right.
",The last part was wrong When you changed radius= radius +1 the new memory will be radius +1 not 3.2 You commanded the phyton that its name will be radius +1. ( I am referring to 42:23 of video)
nykOeWgQcHM,"It's not syntactically valid.
But something like operator, operand, operator is OK.
So once you've created these phrases, or these expressions, that are syntactically valid, you have to think about the static semantics of your phrase, or of your expression.
For example, in English, ""I are hungry"" is good syntax.
But it's weird to say.
We have a pronoun, a verb, and an adjective, which doesn't really make sense.
""I am hungry"" is better.
This does not have good static semantics.
Similarly, in programming languages-- and you'll get the hang of this the more you do it-- something like this, ""3.2 times 5, is OK.
But what does it mean? What's the meaning to have a word added to a number? There's no meaning behind that.
Its syntax is OK, because you have operator, operand, operator.
But it doesn't really make sense to add a number to a word, for example.
Once you have created these expressions that are syntactically correct and static, semantically correct, in English, for example, you think about the semantics.
What's the meaning of the phrase? In English, you can actually have more than one meaning to an entire phrase.
In this case, ""flying planes can be dangerous"" can have two meanings.
It's the act of flying a plane is dangerous, or the plane that is in the air is dangerous.
And this might be a cuter example.
""This reading lamp hasn't uttered a word since I bought it.
What's going on?"" So that has two meanings.
It's playing on the word ""reading lamp."" That's in English.
In English, you can have a sentence that has more than one meaning, that's syntactically correct and static, semantically correct.
But in programming languages, the program that you write, the set of instructions that you write, only has one meaning.","23:21 & 24:25 - man, she keeps saying ""operator-operand-operator"" when her example is in fact operand-operator-operand! so much for syntactic and semantic correctness."
nykOeWgQcHM,"We're trying to find the square root of 16.
We're going to calculate g times g is 9.
And we're going to ask is if g times g is close enough to x, then stop and say, g is the answer.
I'm not really happy with 9 being really close to 16.
So I'm going to say, I'm not stopping here.
I'm going to keep going.
If it's not close enough, then I'm going to make a new guess by averaging g and x over g.
That's x over g here.
And that's the average over there.
And the new average is going to be my new guess.
And that's what it says.
And then, the last step is using the new guess, repeat the process.
Then we go back to the beginning and repeat the whole process over and over again.
And that's what the rest of the rows do.
And you keep doing this until you decide that you're close enough.
What we saw for the imperative knowledge in the previous numerical example was the recipe for how to find the square root of x.
What were the three parts of the recipe? One was a simple sequence of steps.
There were four steps.
The other was a flow of control, so there were parts where we made decisions.
Are we close enough? There were parts where we repeated some steps.","How do we know that we need to averaging g and x/g? 14:00
13:55 There is a error it's 3.17 not 4.17"
o57CTwt1-ck,"That's just for practice and fun, let's look at the space station Mir again.
Suppose that I tell you that there is a 1 in [? 10,000ths ?] chance that in any given hour, the Mir is going to crash into some debris that's out there in orbit.
So the expectation of f is 10 to the fourth, about 10,000 hours.
And the sigma is going to be the variance of f, which is about 1 over ten thousandths, that is 10,000 times 10,000 minus 1, which is pretty close to 10,000 squared for the variance.
And when I take the square root, I get back to 10,000.
So sigma is just a tad less than 10,000, is 10 to the fourth.
So with those numbers, I can apply the Chebyshev's Theorem and conclude that the probability that the Mir lasts more than 4 times 10 to the fourth hours is less than 1 chance in four.
If we translate that into years-- if it was really the case that there was a 1 in 10,000 chance of the Mir being destroyed in any given hour, then the probability that it lasts more than 4.6 years before destructing is less than 1/4.",12:53 isn't that Markov's theorem you've applied?
o9nW0uBqvEo,"But I just want to know what's the asymptotic complexity? And I'm going to say-- oh, sorry-- that is to say I could do this different ways.
I could have done this with two steps like that.
That would have made it not just 1 plus 5n plus 1.
It would have made it 1 plus 6n plus 1 because I've got an extra step.
I put that up because I want to remind you I don't care about implementation differences.
And so I want to know what captures both of those behaviors.
And in Big O notation, I say that's order n.
Grows linearly.
So I'm going to keep doing this to you until you really do wince at me.
If I were to double the size of n, whether I use this version or that version, the amount of time the number of steps is basically going to double.
Now you say, wait a minute.
5n plus 2-- if n is 10 that's 52.
And if n is 20, that's 102.
That's not quite doubling it.
And you're right.
But remember, we really care about this in the asymptotic case.
When n gets really big, those extra little pieces don't matter.
And so what we're going to do is we're going to ignore the additive constants and we're going to ignore the multiplicative constants when we talk about orders of growth.
So what does o of n measure? Well, we're just summarizing here.
We want to describe how much time is needed to compute or how does the amount of time, rather, needed to computer problem growth as the size of the problem itself grows.",I'm pretty sure the fact_iter(n) function (28:47) has a (n-1) while loop. Doesn't make sense he said it was an (n) while loop.
o9nW0uBqvEo,"I shouldn't say cheated.
I probably should have counted the return as one more operation, so that would be 1 plus 3x plus 1, or 3x plus 2 operations.
Why should you care? It's a little closer to what I'd like.
Because now I've got an expression that tells me something about how much time is this going to take as I change the size of the problem.
If x is equal to 10, it's going to take me 32 operations.
If x is equal to 100, 302 operations.
If x is equal to 1,000, 3,002 operations.
And if I wanted the actual time, I'd just multiply that by whatever that constant amount of time is for each operation.
I've got a good estimate of that.
Sounds pretty good.
Not quite what we want, but it's close.
So if I was counting operations, what could I say about it? First of all, it certainly depends on the algorithm.
That's great.
Number of operations is going to directly relate to the algorithm I'm trying to measure, which is what I'm after.
Unfortunately, it still depends a little bit on the implementation.
Let me show you what I mean by that by backing up for a second.
Suppose I were to change this for loop to a while loop.
I'll set i equal to 0 outside of the loop.
And then while i is less than x plus 1, I'll do the things inside of that.
That would actually add one more operation inside the loop, because I both have to set the value of i and I have to test the value of i, as well as doing the other operations down here.
And so rather than getting 3x plus 1, I would get 4x plus 1.
Eh.
As the government says, what's the difference between three and for when you're talking about really big numbers? Problem is in terms of counting, it does depend.
And I want to get rid of that in a second, so it still depends a little bit on the implementation.
I remind you, I wanted to measure impact of the algorithm.
But the other good news is the count is independent of which computer I run on.",So you say that count varies on different implementation because while has two basic operations assignment and testing but for only does assignment. So you say thatwhile and for actually take same time but when we want to count it differs. ANYWAY LETS JUST COUNT 2 OPERATIONS WHILE COUNTING OPERATIONS IN FOR IF FOR AND WHILE TAKE SAME TAME IN FACT 16:20 doesnt for loop test if i is less than x+1? You say it only assigns i values from range so its one operation.
o9nW0uBqvEo,"If I call that n, it's going to take that n times over the outer loop.
But what about n here? All of the earlier examples, we had a constant number of operations inside of the loop.
Here, we don't.
We've got another loop that's looping over in principle all the elements of the second list.
So in each iteration is going to execute the inner loop up to length of L2 times, where inside of this inner loop there is a constant number of operations.
Ah, nice.
That's the multiplicative law of orders of growth.
It says if this is order length L1.
And we're going to do that then order length of L2 times, the order of growth is a product.
And the most common or the worst case behavior is going to be when the lists are of the same length and none of the elements of L1 are in L2.
And in that case, we're going to get something that's order n squared quadratic, where n is the length of the list in terms of number of operations.
I don't really care about subsets.
I've got one more example.
We could similarly do intersection.
If I wanted to say what is the intersection of two lists? What elements are on both list 1 and list 2? Same basic idea.
Here, I've got a pair of nested loops.
I'm looping over everything in L1.
For that, I'm looping over everything in L2.
And if they are the same, I'm going to put that into a temporary variable.
Once I've done that, I need to clean things up.","48:42 why would the worst-case scenario be when the lists are the same length and none of the elements in L1 are in L2? in that case, the program wouldn't go through all the elements in L1 since it breaks out the outer loop returning False when it goes through L2 once and for all and finds no matching elements in L2. Shouldn't the worst-case scenario be when the program goes through all the elements in both inner and outer loops? For instance, that case could be when L1 is [1, 2] and L2 is [1, 3, 4, ...]. Am I miscalculating something? I'd appreciate it if someone could help me.
48:23 The worst case is not right: ""worst case when L1 and L2 same length, none of elements of L1 in L2"" If none of the elements of L1 in L2, during the first run of the loop, the function will return false and exit the function. The worst case should be, in my opinion, every item in L1 is at the last possible position of L2. For example, first item of L1 is at last position of L2, second item of L1 is at the second to last position of L2, third item is at the third to last position of L2, etc. When the L1 and L2 have the same length, the order of growth would be len(L2)+(len(L2)-1)+(len(L2)-2)+(len(L2)-3)+...+1"
o9nW0uBqvEo,"So I'm going to write another loop that sets up an internal variable and then runs through everything in the list I accumulated, making sure that it's not already there.
And as long as it isn't, I'm going to put it in the result and return it.
I did it quickly.
You can look through it.
You'll see it does the right thing.
What I want it to see is what's the order of growth.
I need to look at this piece.
Then I need to look at that piece.
This piece-- well, it's order length L1 to do the outer loop.
For each version of e1, I've got to do order of length L2 things inside to accumulate them.
So that's quadratic.
What about the second loop? Well, this one is a little more subtle.
I'm only looping over temp, which is at most going to be length L1 long.","I don't get one thing. At 50:12 it says: ""I'm only looping over tmp, which is at most going to be len(L1) long"", but that would be in the best case. The wost case is when the length of L1 and L2 are the same and all of the numbers inside both are the same (let's say all 1's), then tmp will be of length L1 ^ 2 because the if statement in the first nested loop will always be true. Only then I can see O(n^2) being applicable to the second loop (in the worst case the implicit ""e in res"" loop has a constant run time because res array doesn't get bigger than one item). If I'm wrong, I can not see how O(n^2) is applicable to the second loop."
o9nW0uBqvEo,"So looping around it is order n.
There's the actual expression.
But again, the pattern I want you to see here is that this is order n.
OK.
Last example for today.
I know you're all secretly looking at your watches.
Standard loops, typically linear.
What about nested loops? What about loops that have loops inside of them? How long do they take? I want to show you a couple of examples.
And mostly, I want to show you how to reason about them.
Suppose I gave you two lists composed of integers, and I want to know is the first list a subset of the second list.
Codes in the handbook, by the way, if you want to go run it.
But basically, the simple idea would be I'm going to loop over every element in the first list.
And for each one of those, I want to say is it in the second list? So I'll use the same kind of trick.",in 46:51 what does `if not matched:` mean? if what is not equal to match?
o9nW0uBqvEo,"So this will be an example of a linear algorithm.
And you can see, I'm looping length of L times over the loop inside of there.
It's taking the order one to test it.
So it's order n.
And if I were to actually count it, there's the expression.
It's 1 plus 4n plus 1, which is 4n plus 2, which by my rule says I don't care about the additive constant.
I only care about the dominant term.
And I don't care about that multiplicative constant.
It's order n.
An example of a template you're going to see a lot.
Now, order n where n is the length of the list and I need to specify that.","41:26 shouldn't it be 1+3n+1, instead of 1+4n+1? How come there is 4n?
at 41:26 how is it 1+4n+1, should it not be 1+3n+1 or is the Len func in range loop adding another constant to make it 4n"
oS9aPzUNG-s,"It's just going to be slow.
Yes? AUDIENCE: [INAUDIBLE] JUSTIN: Yeah, that's right.
So actually, I don't know in this class.
I guess, the interface and the way that we've described it here is dynamic.
We can just keep adding stuff to it.
In that case, remember this amortized argument from Erik's lecture says that on average that it will take order n time.
AUDIENCE: [INAUDIBLE] JUSTIN: What was that? AUDIENCE: [INAUDIBLE] JUSTIN: Oh, that's true.
That's an even better-- sorry.
Even if it weren't dynamic.
If I wanted to replace an existing key-- like, for some reason, two students had the same ID.
This is a terrible analogy.
I'm sorry.
But in any event, if I wanted to replace an object with a new one, well, what would I have to do? I'd have to search for that object first, and then replace it.
And that search is going to take order n time from our argument before.
Thank you.
OK.
So in some sense, we're done.","I was slightly confused at ~ 13:30: Inserting a new element (at the end) should take Theta(1) amortized time (and not O(n) amortized time, as the instructor suggested). However, since we have to search through the entire dyn. array on every insert to check if an element with that key already exists, the insert operation runs in O(n)."
o_i5F1zGPLs,"Um, the second thing we do and this relates to, um, question about importance sampling.
Um, I- is we have this second quantity in here, where this is the probability of an action under our new policy.
Um, we do have access to that, in the sense that, if someone gives us a state we can tell, um, we can say exactly what our probability would be under all the actions.
But again, this often can be a continuous set.
And so instead of doing sort of this continuous set, we are just going to say we're gonna use importance sampling and we can take samples.
This is typically goi- going to be from pi old.
So we look at what times we have taken an action given our current policy and we re-weight them according to the probability we would have taken those actions that drive the new policy.
So it allows us to approximate that expectation using data that we have.
And then the third substitution is switching the advantage back to the Q function, and it's just important to note that all of these three substitutions don't change the solution to the object- to the optimization problem.
These are all sort of taking at, uh, these different substitutions or different ways to evaluate these quantities, okay? So we end up with the following: um, uh, we have this objective function that we are optimizing.
This is after we've done the substitutions I just mentioned, and we have this constraint on how far away we can be.
Um, and empirically, they generally just sample, um, this sort of alternative sampling distribution Q is just your existing old policy.
So there's a bunch of other stuff in the paper.
It's a really nice paper.
Um, a lot of really interesting ideas.","29:02, why are you switching from A to Q?"
p61QzJakQxg,"The idea here is d is the dimension of your original data and p is some high-dimensional space, potentially infinite, right? So, uh, so this is the, um, um, update rule that we get.
And now, uh, imagine our Phi to be, um, a feature map like - like this, like Phi of x equals, there's just one example, right? 1, x_1, x_2, and all the - then x_1 square, x_1 x_2, x_1 x_3, and so on.
So I write x_1 cubed, x_1 square x2, and so on, right? So basically, a set of all monomial terms of order less than equal to 3, right? Now, what we see is, uh, the number of - the dimension of the feature vector, in this case, will be - p will be approximately already cubed, right? It's going to be, um, um, cubic times for all the three order terms, and some two order terms, one order term, but overall it's going to be, you know, - the - the cubic term is gonna dominate and it's gonna be, uh, approximately d cubed number of, uh, features, which means, um, to perform each gradient, uh, uh, descent update, we now move from calculating dot products in d dimension.
For example, if d was 1,000 - d was 1,000, right? This dot product would take about, uh, order d, uh, order d, right? Whereas, this dot product is gonna take about order d cubed, so which means if - if you had d equals to 1,000, this will take about, say, a - a - a - a 1,000 time-steps, whereas, this would take about 1,000 cubed, would be like a billion time-steps, right? So potentially, each - performing each update rule can be a million times slower.
And that expense is mostly because we chose - we just happened to choose a higher dimensional feature space, right? Now, let's make a few observations.","why did u take x1, x2,, x3... at 15:32, input vector is just in terms of x ryt?"
ptuGllU5SQQ,"So by their dimensionality d by d over h, where h is the number of heads.
So they're going to still apply to the X matrix, but they're going to transform it to a smaller dimensionality, d by h.
And then each attention head is going to perform attention independently, it's like you just did it a whole bunch of times, right? And so output l is equal to softmax of, here's your QK but now it's in l form, times XVl and now you have sort of these indexed outputs.
And in order to sort of have the output dimensionality be equal to the input dimensionality and sort of mix things around, combine all the information from the different heads, you concatenate the heads.
So that's output 1 through output h, stack them together.
Now, the dimensionality of this, is equal to the dimensionality of X again.","Not sure, but on 44:49 looks like 'output_l' should have Txd/h dimension. and final output 'Y' should have Txd dimension"
ptuGllU5SQQ,"We've been saying nonlinearities, abstract features, they're great deep learning, end to end learning of representations is awesome.
But right now, we're just doing weighted averages.
And so what is our solution going to be? I mean, it's not going to be all that complex.
So all we're doing right now, is re-averaging vectors.
So you've got sort of the self attention here and if you just stacked another one, you just keep sort of averaging projections of vectors.
But what if we just add a feed forward network for every individual word? So within this layer, each of these feed forward neural networks shares parameters, but it gets in just the output of self attention for this word as we defined it, processes it, and admits something else.
And so you have output i from self attention, which we saw slides ago.
Apply a feed forward layer, where you take the output, multiply it by a matrix, non-linearity of the matrix.
And the intuition here you can think of at least, is well, you know, something like the feed forward network processes the result of the attention for each thing.","for 30:00, doesn't self attention include softmax which is non-linear?"
qGZy1CRoZdE,"This loop counts, too, but it doesn't include V.
All this loop tells me is that the voltage drop across this element is equivalent to the voltage drop across this element.
Or, the voltage drop in this direction across that element is equal to the voltage drop in this direction across this element.
That's Kirchhoff's voltage law.
Kirchhoff's current law is that the current flow into a particular node is equal to 0.
Or, if you take all of the current flows in and out of a particular node and sum them, they should sum to 0.","Hi! Thank you for all these excellent courses (and beautiful teacher) it's explained very very well:))) it's the best course I ever had!!! I just wanted to make a remark: is there a little mistake there 06:32 or I don't understand very well? (it should be opposite in that direction, not equal), just before that she said it right (06:27),
She is wrong at 6:32. The voltage is highest from the top through both resistors so she was technically wrong when she said, ""...in this direction..."" The voltage drop across each element is indeed equal but only in the same direction either from top to bottom on both, or bottom to top on both. You can see the proof of this in the next video (lecture 10) at 6:25."
qaRIBNE-4Ho,"And whatever is- is in the intersection, that is the answer to our question.
So in- our, um, the answer to our question would be, uh, Fulvestrant and, uh, uh, Paclitaxel, um, uh, drug, right? So the point is, um, that we have now, uh, two entities that are answer to our query, if we think of it as a knowledge graph, uh, uh, traversal, uh, type task.
And of course, similarly to what I was saying before, is a given- if some of the links on the path are missing, which is usually the case, then the- then a given entity would not be, uh, will not be able to predict or identify that it is the answer to our query.
So for example, if, uh, we don't know that, uh, ESR2 is associated with breast cancer, then the- then there is no way for us to discover that Fulvestrant is actually the answer, uh, to our question.
So, uh, again, if the knowledge graphs are incomplete, knowledge graph traversal, um, won't work.
So the question then becomes, how can we use embeddings to, uh, in some sense, implicitly impute these missing relations, um, and also, uh, how would we even be able to figure out that, you know, in this case, uh, you know, that there should be a link between ESR2 and breast cancer? And the hope is, right, that our method who will take a look at the entire knowledge graph will see that basically, uh, ESR2 here is also associated with, um, uh, ESR1 and, uh, uh, BRCA1, right? And we see that there are kind of these strong relations here.",In 09:30 Short of Breath is not caused by Paclitaxel! So why you made a link between them ?
r4-cftqTcdI,"Or I might write that as a for loop-- for i equals n, n minus 1.
This is the order that I would compute my problems because the suffix starting at n is the empty suffix.
The suffix starting at 0, that's the one I actually want to compute.
That's the final suffix I should be computing.
And then we have a b for base case, which is that first case, b of n equals 0, because there's no pins.
So I don't get any points.
Sad.
OK, so this is it.
We just take these components, plug them into this recursive, memoized algorithm, and we have a linear time algorithm.
I want to briefly mention a different way you could plug together those pieces, which is called bottom up dp, which is-- let's do it for this example.
So if I have-- let's see.
Let me start with the base case, b of n equals 0.
But now it's an assignment.
And I'm going to do for loop from the topological order for i equals n, n minus 1 to 0.
Now I'm going to do the relation, b of i equals max of b of i plus 1 and b of i plus 1 plus bi and b of i plus 2 plus di vi plus 1.",52:32 Add one more base case V(n)=0
r4KjHEgg9Wg,"So basically what the idea is that I mean you've got this bounds table here and it's got a bunch of entries.
But it basically needs entries to cover all of p size, all the allocation size.
OK, so in this case it was very simple because basically this is just one slot, due to the size.
Here it's multiple slot sizes, right.
So what's going to happen is that imagine then that we had a pointer that's moving in the range of p.
You have to have some of the back end table slot for each one of those places where p [INAUDIBLE], right.
And so it's this second piece that makes the paper a little bit confusing I think.
But it doesn't really go into depth about that, but this is how that works.","1:22:00 When you check whether p' is out of bound, you actually have access to p. That means you can get the binary logarithm of the allocation size just from the table[p>>slot_size]. Why do you want the table to cover all allocation size of p?"
rMq21iY61SE,"So the way we are going to do this, um, is that we are going to represent, uh, a graph, as a- as a- with an adjacency matrix.
Um, and we are going to think of this, um, in terms of its adjacency matrix, and we are not going to assume any feature, uh, uh, represe- features or attributes, uh, on the nodes, uh, of the network.
So we are just going to- to think of this as a- as a- as a set of, um, as a- as an adjacency matrix that we wanna- that we wanna analyze.","In 5:02 it is stated that, for simplicity, when creating node embeddings ""no node features"" are used. If nodes had features, would it be valid to just concatenate them to the feature representation embedding (i.e vector in R^d) obtained from applying, for example, node2vec and then use this ""augmented"" vector as the input to the downstream prediction task?"
rMq21iY61SE,"We are then, uh, creating structure- structured features or structural features, uh, of this graph so that then we can apply our learning algorithm and make, uh, prediction.
And generally most of the effort goes here into the feature engineering, uh, where, you know, we are as, uh, engineers, humans, scientists, we are trying to figure out how to best describe, uh, this particular, um, network so that, uh, it would be most useful, uh, for, uh, downstream prediction task.
Um, and, uh, the question then becomes, uh, can we do this automatically? Can we kind of get away from, uh, feature engineer? So the idea behind graph representation learning is that we wanna alleviate this need to do manual feature engineering every single time, every time for, uh, every different task, and we wanna kind of automatically learn the features, the structure of the network, um, in- that we are interested in.
And this is what is called, uh, representation learning so that no manual, uh, feature engineering is, uh, necessary, uh, anymore.
So the idea will be to do efficient task-independent feature learning for machine learning with the graphs.
Um, the idea is that for example, if we are doing this at the level of individual nodes, that for every node, we wanna learn how to map this node in a d-dimensional space ha- and represent it as a vector of d numbers.
And we will call this vector of d numbers as feature representation, or we will call it, um, an embeding.",In 2:16 I think it should be f: V -> R^d instead of f: u -> R^d Just a grammatical math error tho
rUxP7TM8-wo,"And then they said, well, since they're falling at random, the ratio of the needles in the circle to needles in the square should exactly equal the area of the square over the area of the circle, exactly, if you did an infinite number of needles.
Does that make sense? Now, given that, you can do some algebra and solve for the area of the circle and say it has to be the area of the square times the number of needles in the circle divided by the needles in the square.
And since we know that the area of the circle is pi that tells us that pi is going to equal four times the needles in the circle.
That's 4 is the area of the square divided by the number of needles in the square.
And so the argument was you can just drop a bunch of these needles, see where they land, add them up and from that you would magically now know the actual value of pi.
Well, we tried a simulation one year in class but rather than using needles we had an archer and we blindfolded him so he would shoot arrows at random and we would see where they ended up.",37:07 I am confused with the equation needle in circle/needle in square = area of circle/area of square
rUxP7TM8-wo,"Now, why would I want to look at the fraction within approximately 200 of the mean? What is that going to correspond to in this case? Well, if I divide 200 by 2 I get 100.
Which happens to be the standard deviation.
So in this case, what I'm going to be looking at is what fraction of the values fall within two standard deviations of the mean? Kind of a check on the empirical rule, right? All right, when I run the code I get this.
So it is a discrete approximation to the probability density function.
You'll notice, unlike the previous picture I showed you which was nice and smooth, this is jaggedy.
You would expect it to be.
And again, you can see it's very nice that the peak is what we said the mean should be, 0.
And then it falls off.
And indeed, slightly more than 95% fall within two standard deviations of the mean.
I'm not even surprised that it's a little bit more than 95% because, remember the magic number is 1.96, not 2.
But since this is only a finite sample, I only want it to be around 95.
I'm not going to worry too much whether it's bigger or smaller.
All right? So random.gauss does a nice job of giving us Gaussian values.
We plotted them and now you can see that I've got the relative frequency.",Can someone tell me the code v[0][30:70] means? 6:14
rUxP7TM8-wo,"So now let's look at it for a normal distribution.
You may recall from the last lecture this rather exotic looking formula which defines a PDF for a normal distribution.
And here's some code that's as straight forward an implementation as one could imagine of this formula, OK.
So that now is a value that, given a mu and a sigma and an x, gives me the x associated with that mu and sigma, OK? And you'll notice there's nothing random about this.
All right, it is giving me the value.
Now, let's go down here.
And I'm going to, for a set of x's, get the set of y's corresponding to that and then plot it.
I'm going to set mu to 0 and sigma to 1, the so-called standard normal distribution.
And I'm going to look at the distribution from minus 4 to 4.
Nothing magic about that other than, as you'll see, it's kind of a place where it asymptotes near 0.
So while x is less than 4, I'll get the x-value, I'll get the y-value corresponding to that x by calling Gaussian, increment x by 0.05 and do that until I'm done.","8:55 PDF formula in red rectangle is missing / in code, factor2 is correct"
rUxP7TM8-wo,"So that is something we believe is true by the math we've been looking at for the last two lectures.
Next statement, with a probability of 0.95, the actual value of pi is between these two things.
Is that true? In fact, if I were to say, with a probability of 1, the actual value is pi is between those two values, would it be true? Yes.
So they are both true facts.
However, only the first of these can be inferred from our simulation.
While the second fact is true, we can't infer it from the simulation.
And to show you that, statistically valid is not the same as true, we'll look at this.
I've introduced a bug in my simulation.
I've replaced the 4 that we saw we needed by 2, now, an easy kind of mistake to make.
And now, if we go to the code-- well, what do you think will happen if we go to the code and run it? We'll try it.
We'll go down here to the code.
We'll make that a 2.
And what you'll see as it runs is that once again we're getting very nice confidence intervals, but totally bogus values of pi.
So the statistics can tell us something about how reproducible our simulation is but not whether the simulation is an actually, accurate model of reality.","46:31 The slide says ""both are factually correct"". But i don't understand how the 2nd statement is true. Is it correct to say that the value of pi is between X and Y with probability 0.95, when in fact we know that the value of pi is between those X and Y with a probability of 1 ? The 2nd statement implies that the value of pi is not between X and Y with a probability of 0.05, which is false."
rUxP7TM8-wo,"So the first bin might be, well, let's say we only had values ranging from 0 to 100.
The first bin would be all the 0's, all the 1's up to all the 99's.
And it weights each value in the bin by 1.
So if the bin had 10 values falling in it, the y-axis would be a 10.
If the bin had 50 values falling in it, the y-axis would go up to 50.
You can tell it how much you want to weight each bin, the elements in the bins.
And say, no, I don't want them each to count as 1, I want them to count as a half or a quarter, and that will change the y-axis.
So that's what I've done here.
What I've said is I've created a list and I want to say for each of the bins-- in this case I'm going to weigh each of them the same way-- the weight is going to be 1 over the number of samples.
I'm multiplying it by the len of dist, that will be how many items I have.
And that will tell me how much each one is being weighted.
So for example, if I have, say, 1,000 items, I could give 1,000 values and say, I want this item weighted by 1, and I want this item over here weighted by 12 or a half.",what is a bin? 3:15
rUxP7TM8-wo,"What are the values on the y-axis? We kind of would like to interpret them as probabilities, right? But we could be pretty suspicious about that and then if we take this one point that's up here, we say the probability of that single point is 0.4.
Well, that doesn't make any sense because, in fact, we know the probability of any particular point is 0 in some sense, right? So furthermore, if I chose a different value for sigma, I can actually get this to go bigger than 1 on the y-axis.
So if you take sigma to be say, 0.1-- I think the y-axis goes up to something like 40.
So we know we don't have probabilities in the range 40.
So if these aren't probabilities, what are they? What are the y values? Well, not too surprising since I claimed this was a probability density function, they're densities.
Well, what's a density? This makes sense.
I'll say it and then I'll try and explain it.
It's a derivative of the cumulative distribution function.
Now, why are we talking about derivatives in the first place? Well, remember what we're trying to say.
If we want to ask, what's the probability of a value falling between here and here, we claim that that was going to be the area under this curve, the integral.","11:44 could you expand on ""the probability of any particular point is 0""?"
rmVRLeJRkl4,"And so at that point it's sort of interesting thing has happened that we've ended up getting straight back exactly the softmax formula probability that we saw when we started.
And we can just rewrite that more conveniently as saying this equals U0 minus the sum over X equals 1 to V of the probability of X given C times UX.
And so what we have at that moment is this thing here is an expectation.
And so this isn't an average over all the context vectors waited by their probability according to the model.
And so it's always the case with these softmax style models, that what you get out for the derivatives is you get the observed minus the expected.
So our model is good if our model on average predicts exactly the word vector that we actually see.","This might be silly but, at after 55:00 when we take ux out of the derivative, why do we lose the transpose operator?
Calculus noob question: But why don't the two [for w from 1 to V Sum over u_x^T*v_c cancel out at 55:10
How are the initial probabilities of the context word vectors calculated? They are mentioned at 55:29 but not how they are determined."
rmVRLeJRkl4,"And so at that point we have to actually remember something, we have to remember that the derivative of the log is the 1 on X function.
So this is going to be equal to the 1 on X for Z.
So that's then going to be 1 over the sum of W equals 1 to V of exp of UTVC multiplied by the derivative of the inner function.
So the derivative of the part that is remaining, I'm getting this, the sum of-- and this one trick here at this point we do want to have a change of index.
So we want to say the sum of X equals 1 to V of exp of U of X VC.
Since we can get into trouble if we don't change that variable to be using a different one.","51:39 how to get it? I don't understand.
Why is the change in variable at 51:38 necessary? Does it not represent the same quantity whether we use uw or ux?"
rmVRLeJRkl4,"OK, so at that point we're making some progress, but we still want to work out the derivative of this.
And so what we want to do is apply the chain rule once more.
So now here is our F and in here is our new Z equals G of VC.
And so, we then sort of repeat over, so we can move the derivative inside a sum always.
So we then taking the derivative of this, and so then the derivative of exp is itself, so we're going to just have exp of the UXTVC times there's is a sum of X equals 1 to V times the derivative of UX TVC.
OK, and so then this is what we've worked out before, we can just rewrite as UX.
OK, so now we're making progress.
So if we start putting all of that together, what we have is the derivative, well the partial derivatives with VC of this log probability.
All right, we have the numerator, which was just U0 minus-- we then had the sum of the numerator, sum over X equals 1 to V of exp UXTVC times U of X, then that was multiplied by our first term that came from the 1 on X, which gives you the sum of W equals 1 to V of the exp of UWTVC.
And this, the fact that we changed the variables became important.
And so by just sort of rewriting that a little, we can get that equals U0 minus the sum V equals sorry-- X equals 1 to V of this X, V of XTVC over the sum of W equals to V of exp UWTVC times U of X.","When we take chain derivative, why do we lose transpose operation? For example, on 53:06 there is just u_x, not the u_x^T, why?"
rmVRLeJRkl4,"So dot product is a natural measure for similarity between words because in any particular mention of opposite, you'll get a component that adds to that dot product sum.
If both are negative, it'll add a lot to the dot product sum.
If one's positive and one's negative, it'll subtract from the similarity measure.
Both of them are zero, won't change the similarity.
So it sort of seems sort of plausible idea to just take a dot product and thinking, well, if two words have a larger dot product, that means they're more similar.
And so then after that, we sort of really doing nothing more than OK, we want to use dot products to represent words similarity.",34:49 what is uo and vc
rmVRLeJRkl4,"So to start to make that a bit more concrete, this is what we're doing.
So we have a piece of text, we choose our center word which is here in two and then we say, well, if a model of predicting the probability of context words given the center word and this model, we'll come to in a minute, but it's defined in terms of our word vectors.
So let's see what probability it gives to the words that actually occurred in the context of this word.
It gives them some probability, but maybe it'd be nice if the probability it assigned was higher.
So then how can we change our word vectors to raise those probabilities? And so we'll do some calculations with into being the center word, and then we'll just go on to the next word and then we'll do the same kind of calculations, and keep on chunking.
So the big question then is, well what are we doing for working out the probability of a word occurring in the context of the center word? And so that's the central part of what we develop as the word2vec object.
So this is the overall model that we want to use.
So for each position in our corpus, our body of text, we want to predict context words within a window of fixed size M, given the center word WJ.
And we want to become good at doing that, so we want to give high probability to words that occur in the context.
And so what we're going to do is we're going to work out what's formerly the data likelihood as to how good a job we do at predicting words in the context of other words.
And so formally that likelihood is going to be defined in terms of our word vectors.
So they're the parameters of our model, and it's going to be calculated as taking the product of using each word as the center word, and then the product of each word and a window around that of the probability of predicting that context word in the center word.",Isn't wt the center word instead of wj on slide 23 (30:52)?
rmVRLeJRkl4,"So when you want to collapse two vectors for the same work, did you usually take the average? Different people have done different things.
But the most common practice is after you've-- there's still a bit more I have to cover about running word2vec that we didn't really get through today.
So I still got a bit more work to do on Thursday, but once you've run your word2vec algorithm and you sort of your output is two vectors for each word and kind of when it's center and when its context, and so typically people just average those two vectors and say OK, that's the representation of the word croissant, and that's what appears in the sort of word vectors file, like the one I loaded.
Makes sense, thank you.
I think-- so my question is, if a word to have two different meanings or multiple different meanings, can we still represent it as the same single vector? Yes, that's a very good question.
And actually there is some content on that in Thursday's lecture, so I can say more about that.
But yeah, the first reaction is you kind of should be scared because something I've said nothing about at all is most words, especially short common words have lots of meanings.
So if you have a word like star, that can be astronomical object or it can be a film star, a Hollywood star, or it can be something like the gold stars that you got in elementary school.","1:10:50 Why would you average both vectors together, wouldn't it be useful to keep both of the vectors depending on the different tasks that need to be done?"
sh3EPjhhd40,"ROFESSOR PATRICK WINSTON: You know, some of you who for instance-- I don't know, Sonya, Krishna, Shoshana-- some of you I can count on being here every time.
Some of you show up once in a while.
The ones of you who show up once in a while happen to be very lucky if you picked today, because what we're going to do today is I'm going to tell you stuff that might make a big difference in your whole life.
Because I'm going to tell you how you can make yourself smarter.
No kidding.
And I'm also going to tell you how you can package your ideas so you'll be the one that's picked instead of some other slug.
So that's what we're going to do today.
It's the most important lecture of the semester.
The sleep lecture is only the second most important.","0:50 what is meant by sleep lecture?
What is the ""sleep lecture"" he refers to at 0:50?"
soZv_KKax3E,"If we look at this-- and I'm looking just for the uniform distribution, but we'll see the same thing for all three-- it more or less doesn't matter.
Quite amazing, right? If you have a bigger population, you don't need more samples.
And it's really almost counterintuitive to think that you don't need any more samples to find out what's going to happen if you have a million people or 100 million people.
And that's why, when we look at, say, political polls, they're amazingly small.
They poll 1,000 people and claim they're representative of Massachusetts.
This is good news.
So to estimate the mean of a population, given a single sample, we choose a sample size based upon some estimate of skew in the population.","Well, when he is comparing different distributions vs population size... why the uniform has at sample size 25 more or less a difference of 7.5% ( 37:21 ) and in the next slide about 25% ( 39:12 )?"
soZv_KKax3E,"So here's what they look like.
Quite different, right? We've looked at uniform and we've looked at Gaussian before.
And here we see an exponential, which basically decays and will asymptote towards zero, never quite getting there.
But as you can see, it is certainly not very symmetric around the mean.
All right, so let's see what happens.
If we run the experiment on these three distributions, each of 100,000 point examples, and look at different sample sizes, we actually see that the difference between the standard deviation and the sample standard deviation of the population standard deviation is not the same.
We see, down here-- this looks kind of like what we saw before.
But the exponential one is really quite different.
You know, its worst case is up here at 25.
The normal is about 14.
So that's not too surprising, since our temperatures were kind of normally distributed when we looked at it.
And the uniform is, initially, much better an approximation.
And the reason for this has to do with a fundamental difference in these distributions, something called skew.
Skew is a measure of the asymmetry of a probability distribution.
And what we can see here is that skew actually matters.","Well, when he is comparing different distributions vs population size... why the uniform has at sample size 25 more or less a difference of 7.5% ( 37:21 ) and in the next slide about 25% ( 39:12 )?"
soZv_KKax3E,"Well, we can again run the experiment.
I did run the experiment.
I changed the sample size from 100 to 200.
And, again, you can run this if you want.
And if you run it, you'll get a result-- maybe not exactly this, but something very similar-- that, indeed, as I increase the size of the sample rather than the number of the samples, the standard deviation drops fairly dramatically, in this case from 0.94 0.66.
So that's a good thing.
I now want to digress a little bit before we come back to this and look at how you can visualize this-- Because this is a technique you'll want to use as you write papers and things like that-- is how do we visualize the variability of the data? And it's usually done with something called an error bar.",17:15 Increase size of sample than number of sample. What does he mean?
tKwnms5iRBU,"Either way, let's just suppose than an edge e-- I should mention, I guess I didn't say, this graph is undirected.
A minimum spanning tree doesn't quite make sense with directed graphs.
There are other versions of the problem but here, the graph is undirected.
So probably, I should write this as a unordered set, u, v.
And there are possibly many minimum spanning trees.
There could be many solutions with the same weight.
For example, if all of these edges have weight 1, all of these trees are actually minimum.
If all the edges have weight 1, every spanning tree is minimum, because every spanning tree has exactly n minus 1 edges.
But let's suppose we know an edge that's guaranteed to be in some minimum spanning tree, at least one.
What I would like to do is take this, so let me draw a picture.
I have a graph.
We've identified some edge in the graph, e, that lives in some minimum spanning tree.
I'm going to draw some kind of tree structure here.
OK.
The wiggly lines are the tree.
There are some other edges in here, which I don't want to draw too many of them because it's ugly.
Those are other edges in the graph.
Who knows where they are? They could be all sorts of things.
OK? But I've highlighted the graph in a particular way.
Because the minimum spanning tree is a tree, if I delete e from the tree, then I get two components.
Every edge I remove-- I'm minimally connected.
So if I delete an edge, I disconnect into two parts, so I've drawn that as the left circle and the right circle.
It's just a general way to think about a tree.
Now there are other unused edges in this picture, who knows where they live? OK? What I would like to do is somehow simplify this graph and get a smaller problem, say a graph with fewer edges.
Any suggestions on how to do that? I don't actually know where all these white edges are, but what I'd like to do is-- I'm supposing I know where e is, and that's an edge in my minimum spanning tree.
So how could I get rid of it? Yeah.
AUDIENCE: Find the minimum weight spanning tree of the two edges.","Are both vertices sets from the example starting 14:20 supposed to be connected on the spanning tree, with paths that do not include the edge e?"
tKwnms5iRBU,"In fact, the minimum spanning tree, T star, has to connect vertex u to vertex v, somehow.
It doesn't use e, but there's got to be-- it's a tree, so in fact, there has to be a unique path from u to v in the minimum spanning tree.
And now u is in S, v is not in S.
So if you look at that path, for a while, you might stay in S, but eventually you have to leave S, which means there has to be an edge like this one, which I'll call it e prime, which transitions from S to V minus S.
So there must be an edge e prime in the minimum spanning tree that crosses the cut, because u and v are connected by a path and that path starts in S, ends not in S, so it's got to transition at least once.
It might transition many times, but there has to be at least one such edge.
And now what I'm going to do is cut and paste.
I'm going to remove e prime and add an e instead.
So I'm going to look at T star minus e prime plus e.
I claim that is a minimum spanning tree.
First I want to claim, this is maybe the more annoying part, that it is a spanning tree.
This is more of a graph theory thing.
I guess one comforting thing is that you've preserved the number of edges, so it should still be if you get one property, you get the other, because I remove one edge, add in one edge, I'm still going to have n minus 1 edges.","I believe that Professor Demaine made a slight error at 36:25 relevant to your question in his writing of a proof that every least weight edge that crosses a cut of an (undirected) graph is part of a minimum spanning tree of that graph. Just before that, he talked about how, given a possibly different minimum spanning tree, T*, there was a unique path from u to v in T* and that that unique path also must cross the cut, but then he did not use that lemma in writing the proof. In his proof, he wrote that we could choose any edge e' in T* that crosses the cut. I think he meant to put a further condition on e' that it must also be one of the edges in the path from u to v within T*. That way, removing e' disconnects the subgraphs containing u and v, and then adding e reconnects them without creating a cycle."
tKwnms5iRBU,"There's no point in keeping the higher weight one, so I'm just going to throw away the higher weight one.
Take them in.
So this is a particular form of edge contraction and graphs.
And I claim it's a good thing to do, in the sense that if I can find a minimum spanning tree in this new graph-- this is usually called a G slash e, slash instead of negative, to remove e.","doesn't the picture 19:20 says that the graph is cyclic? But spanning trees are acyclic. Looks like a case of a bad example, or am I missing something?"
tKwnms5iRBU,"We assumed that e is in there.
So I'm basically removing, I'm just deleting that edge, maybe I should call it minus e.
Then that should be a spanning tree of G slash e.
So when I contract the edge in the graph, if I throw away the edge from this spanning tree, I should still have a spanning tree, and I don't know whether it's minimum.
Probably, it is, but we won't prove that right now.
I claim it's still a spanning tree.
What would that take? It still hits all the vertices, because if I removed the edge, things would not be connected together.
But this edge was in the spanning tree, and then I fused those two vertices together, so whatever spanning-- I mean, whatever was connected before is still connected.",25:41 isn't that T* was cut to two subtrees since he was deleting the edge instead of merging it? can we say two trees is a spanning tree then?
tOsdeaYDCMk,"Now I just play a nice trick to get these 2 exponents to look alike.
I say that the depth of s is less than or equal to the max of the depth of s and the depth of t, and likewise for the depth of t.
So in both of those terms here, I can replace the exponent or replace the depth of s by the max of depth s and t, and likewise here.
Now I've got the same term twice, so I can say that it's simply twice the max depth.
And of course, that is equal to, by definition of the depth of r, twice 2 to the depth of r, which is of course 2 to the depth of r plus 1.","At 6:58 he says max(d(s),d(t))+1 = d(r). But at 1:55 he said it was max(1+d(s),d(t)). These two are not necessarily equal, am I missing something here?"
tOsdeaYDCMk,"This is a somewhat interesting one.
Let's define the depth of a string as follows, and the idea is it's how deeply nested are the successive pairs of left and right brackets.
Well, the depth of the empty string is 0.
You got to start somewhere, and it's got no brackets, we'll call it depth 0.
Now, what about the depth of the constructor putting brackets around s and then following it by t? Well, putting brackets around s gives you a string that's 1 deeper than s is, and then you follow it by t, and it's as deep as t is.
So the result is that the depth of the constructor is a string which is a number which is equal to 1 plus the depth of s and the depth of t, whichever is larger.
The max of 1 plus depth of s and depth of t, and that's our recursive definition of depth.
Let's look at maybe another even more familiar example of recursive definition.
Let's define the nth power of an integer or real number k.
The zeroth power k is defined to be 1, and the n plus first power of k is defined to be k times the nth power of k, and this would be an executable definition of the exponentiation function in a lot of programming languages.
And my point here is that this familiar definition, recursive definition on a nonnegative integer n, is in fact a structural induction using the fact that the nonnegative integers can be defined recursively as follows.
0 is a nonnegative integer, and if n is a nonnegative integer, then n plus 1 is a nonnegative integer.","At 6:58 he says max(d(s),d(t))+1 = d(r). But at 1:55 he said it was max(1+d(s),d(t)). These two are not necessarily equal, am I missing something here?
Believe max(d(s),d(t))+1 = d(r) is correct, and what he says at 1:55 is wrong because it assumes that for d(r) the brackets for d(r) enclose d(s) but they could actually also enclose d(t), therefore it makes more sense that the depth is max(d(s),d(t))+1 for d(r) because the brackets for d(r) could either enclose d(s) or d(t).... The proof also works for max(d(s),d(t))+1 = d(r) so safe to assume this correct..."
tkJd2B98hII,"Why? This is basically the Rademacher complexity of the hypothesis class h, but not the family of losses, right? Before we were talking about a hypothesis class of the family of losses.
And now you're talking about exactly the Rademacher complexity of the hypothesis class h.
So basically, this is saying that for binary-- I think I'm missing something.
I'm missing 1/2 here.
Where did the 1/2 go? Yeah, I think I lost the 1/2.
Sorry.
I think I lost-- oh, I have the 1/2 in the notes.
It's just I forgot to copy it.
So 1/2.
So basically, it's the 1/2 times the Rademacher complexity.
So what we achieved is that the Rademacher complexity of f in this special case of binary classification and 0, 1 loss is equal to 1/2 times the Rademacher complexity of the hypothesis class.
So that's a slightly simpler way of thinking about this.
Because what's this? This is basically saying that how well h can memorize the random label.
You can think of sigma 1 up to sigma n as random label.
And R and h is big when you can-- there exist an h in the capital H such that h of xi is equals to sigma i.
This is the best situation, right? This has the strongest correlation.
So basically, if you can memorize all the random label with some hypothesis, hypothesis class, that means your Rademacher complexity is the biggest.
And that gives you the worst generalization bound.
And vice versa, if you cannot memorize, then you get better generalization bound.
Right.
OK.
I have a question [INAUDIBLE].
So [INAUDIBLE] yi [INAUDIBLE].
But [INAUDIBLE]? I see.","At 32:40 the prof mentioned R_n(H) describes ""how well H can memorize random labels"", what does ""memorize"" mean here? Still kind of confused..."
tutlI9YzJ2g,"And in practice, uh, this means it requires tuning the learning rate.
And this SGD idea is kind of a common core idea that then many other, um, optimizers, uh, improve on, like ada- adagrad, adadelta, M RMSprop and so on.
Essentially, all use this core idea of selecting the subsets of the- subset of data, evaluating the gradient over it and making the steps.
Now- now the details, uh, vary in terms of what data points you select, how big of a step you make, how do you decide on the step size, um, and so on and so forth.
But essentially, this minibatch stochastic gradient descent is the core of, um, optimization in deep learning.
So now that we have discussed the objective function, we discussed the notion of a minibatch, we discussed the notion of a stochastic gradient descent, now, we need to, uh, talk about how is- is this actually done? How are these, um, gradients, uh, computed, evaluated, right.
Because in the old days, pre-deep learning, you actually had to write down the model with the set of equations and then you have to do by hand computed these gradients essentially, you know, like we did it in high school, uh, many of you are computing the gradients by hand- by hand on the whiteboard.",17:10 Why is the notation of the jacobian of W W^T? W^T usually stands for the transopose of W.
tutlI9YzJ2g,"And now I want to evaluate f, uh, with respect to the loss.
So we have this kind of, uh, nesting or chaining of functions.
And if I wanna do back-propagation now, back-propagation means I have to now compute the derivative, the gradient, and I wanna work backward.
So what does this means is that if these are my model parameters, I start from the loss and compute gradients backwards.
So I would start with a loss, for example, and I'm interested to compute the gradient of the loss, uh, with respect to W2.
Then I have to go from the loss, compute- take the derivative with respect to f, and then I have to take, uh, f and take a derivative with respect to W2, right? So I went from lost to f to W, uh,2.
While, for example, to compute the derivative of the loss, uh, with the- with respect to W_1, I have to take the- the loss compute f of the derivative with respect to f.
Take f compute the derivative with respect to W_2, and then kind of take the result of that W_2, take a derivative, uh, with respect to, uh, W_1.
And you can see kind of how I'm working backwards and how, uh, as I go deeper into the network, I can kind of re-use, uh, uh, previous computations.
And this is why this is called a back-propagation because I kind of- kind of, uh, working backwards, um, uh, from the output all the way towards the, uh- the input.","At slide 24 (24:40) when we compute the derivative of the loss w.r.t. W_1 (bottom right formula), shouldn't the W_2 terms in the formula be replaced by the function h instead?"
tw6cmL5STuY,"So now you've gone from, um, I guess n parameters to just one parameter, right? Uh, and this means that you are constraining the covariance matrix to- you are constraining the Gaussian you use to have circular contours.
So this is an example where you can model.
Uh, and this would be another example, right? And this is- I guess this is another example, okay? So you can model things like this, where every feature, not only is every feature uncorrelated but every feature further has the same variance as every other feature.
Um, and the maximum likelihood is this, okay? And again, not, not, not a huge surprise, just the average over, uh, the previous values.
So what we'd like to do is, um, not quite use either of these options, right? Which assumes- really, the biggest problem is it assumes the features are uncorrelated.
Um, and what I'd like to do is build the model that you can fit even when you have very high dimensional data and a relatively small number of examples, um, but that allows you to capture some of the correlations, right? So if you have 30 temperature sensors in this room, you know, probably there are some correlations, right? Probably, this side of the room temperature is gonna be correlated, and that side of the room temperature is gonna be correlated and maybe the ambient temperature in this whole building.
The, the temperature of this room really goes up and down as a whole, but maybe some of the lamps on the side heat up that side of the room a bit more, so different, the different.
There are correlations but maybe you don't need a full covariance matrix either.
So what [NOISE], what factor analysis will do is, um, give us a model that you can fit even when you have, you know, [NOISE] 100 dimensional data and 30 examples.
They capture some of the correlations but that doesn't run into the a, a- uninvertible, um, covariance matrixes is that the naive Gaussian model does, okay? All right.
So let me- just check any- let me, let me describe the model, let me just check, any questions before I move on? Okay.",34:09 - Is this related to singular value decomposition?
uK5yvoXnkSk,"Because we know that the availability, available weight has to have that cost subtracted from it.
And then we'll add to withVal next item dot getValue.
So that's a value if we do take it.
Then we'll explore the right branch-- what happens if we don't take it? And then we'll choose the better branch.
So it's a pretty simple recursive algorithm.
We just go all the way to the bottom and make the right choice at the bottom, and then percolate back up, like so many recursive algorithms.
We have a simple program to test it.
I better start a console now if I'm going to run it.
And we'll testGreedys on foods.",Wonderful lecture and I learned a lot. In the accompanying Python code the lists for values and calories are missing an element. They have 8 elements each while the names list has 9 (See 15:03).
uK5yvoXnkSk,"So we see this here.
Notice that what we're doing is trading time for space.
It takes some space to store the old results, but negligible related to the time we save.
So here's the trick.
We're going to create a table to record what we've done.
And then before computing fib of x, we'll check if the value has already been computed.
If so, we just look it up and return it.
Otherwise, we'll compute it-- it's the first time-- and store it in the table.
Here is a fast implementation of Fibonacci that does that.
It looks like the old one, except it's got an extra argument-- memo-- which is a dictionary.
The first time we call it, the memo will be empty.
It tries to return the value in the memo.
If it's not there, an exception will get raised, we know that.
And it will branch to here, compute the result, and then store it in the memo and return it.
It's the same old recursive thing we did before but with the memo.
Notice, by the way, that I'm using exceptions not as an error handling mechanism, really, but just as a flow of control.
To me, this is cleaner than writing code that says, if this is in the keys, then do this, otherwise, do that.
It's slightly fewer lines of code, and for me, at least, easier to read to use try-except for this sort of thing.
Let's see what happens if we run this one.
Get rid of the slow fib and we'll run fastFib.
Wow.
We're already done with fib 120.
Pretty amazing, considering last time we got stuck around 40.
It really works, this memoization trick.","At 33:02 on line 129 of the code, it should return n instead of 1 because if it returns 1 you will be calculating one step more than what you asked for. fib(5) should return 5 but the code will return you 8 which is fib(6)."
uK5yvoXnkSk,"You'll notice it will always be the case that the leftmost leaf of this tree has got all the possible items in it, and the rightmost leaf none.
And then I just check which of these leaves meets the constraint and what are the values.
And if I compute the value and the calories in each one, and if our constraint was 750 calories, then I get to choose the winner, which is-- I guess, it's the pizza and the burger.
Is that right? The most value under 750.
That's the way I go through.
It's quite a straightforward algorithm.
And I don't know why we draw our trees with the root at the top and the leaves at the bottom.
My only conjecture is computer scientists don't spend enough time outdoors.","A little bit late to the party, but at 6:36 it seems like the calorie values are messed up. If we're using the same code as before, beer should be 154 instead of 145. Also, Beer+Pizza+Burger cannot be the same as Beer+Pizza, but they both have 766 calories. I think it should be: 766, 412*, 508, 154*, 612, 258, 354, 0 (* = wrong). The answer should be Beer and Burger.
for 06:37 , should the best option be beer and burger with value 140 and cal under 750 instead of choosing pizza and burger(value 80)?"
uXt8qF2Zzfo,"And these will tell us how much improvement we're getting by making a little movement in those directions, right? How much a change is given that we're just going right along the axis.
So maybe what we ought to do is if this guy is much bigger than this guy, it would suggest we mostly want to move in this direction, or to put it in 1801 terms, what we're going to do is we're going to follow the gradient.
And so the change in the w vector is going to equal to this partial derivative times i plus this partial derivative times j.
So what we're going to end up doing in this particular case by following that formula is moving off in that direction right up to the steepest part of the hill.
And how much we move is a question.
So let's just have a rate constant R that decides how big our step is going to be.
And now you think we were done.
Well, too bad for our side.
We're not done.
There's a reason why we can't use-- create ascent, or in the case that I've drawn our gradient, descent if we take the performance function the other way.","hi sboby, you seem pretty familiar with neural net. i have a question in terms of backprop. I've understand that we wanna minimaze our errorfunktion, therefore we calculate the partiell derivatives of the weights W_1,..., W_n. My question is, how do we use stochastic gradient descent to find the best weights? Is it like you explained in 21:23 ?
Thanks for uploading such an awesome lecture. One point I did not get, though: Could anyone please explain what i and j are in the function to calculate the delta of the weights at 21:24 ? Did I miss where the professor explains where this comes from?"
uXt8qF2Zzfo,"And we need to look at the picture.
And the reason I turned this guy around was actually because from a point of view of letting the math sing to us, this piece here is the same as this piece here.
So part of what we needed to do to calculate the partial derivative with respect to w1 has already been done when we calculated the partial derivative with respect to w2.
And not only that, if we calculated the partial wit respect to these green w's at both levels, what we would discover is that sort of repetition occurs over and over again.
And now, I'm going to try to give you an intuitive idea of what's going on here rather than just write down the math and salute it.
And here's a way to think about it from an intuitive point of view.
Whatever happens to this performance function that's back of these p's here, the stuff over there can influence p only by going through, and influence performance only going through this column of p's.
And there's a fixed number of those.
So it depends on the width, not the depth of the network.
So the influence of that stuff back there on p is going to end up going through these guys.
And it's going to end up being so that we're going to discover that a lot of what we need to compute in one column has already been computed in the column on the right.
So it isn't going to explode exponentially, because the influence-- let me say it one more time.
The influences of changes of changes in p on the performance is all we care about when we come back to this part of the network, because this stuff cannot influence the performance except by going through this column of p's.","Between 46:00 and 49:00, dynamic programming also uses similar concept to avoid exponential blowup. Maybe back propagation is also a kind of dynamic programming."
uXt8qF2Zzfo,"No, the output is z.
So it's z time 1 minus e.
So whenever we see the derivative of one of these sigmoids with respect to its input, we can just write the output times one minus alpha, and we've got it.
So that's why it's mathematically convenient.
It's mathematically convenient because when we do this differentiation, we get a very simple expression in terms of the output.
We get a very simple expression.
That's all we really need.
So would you like to see a demonstration? It's a demonstration of the world's smallest neural net in action.
Where is neural nets? Here we go.
So there's our neural net.
And what we're going to do is we're going to train it to do absolutely nothing.",Why is it z(1-z) instead of P2(1-P2) ? in 38:50
uXt8qF2Zzfo,"So it's not going to blow up exponentially.
We're going to be able to reuse a lot of the computation.
So it's the reuse principle.
Have we ever seen the reuse principle at work before.
Not exactly.
But you remember that little business about the extended list? We know that we've seen-- we know we've seen something before.
So we can stop computing.
It's like that.
We're going to be able to reuse the computation.
We've already done it to prevent an exponential blowup.
By the way, for those of you who know about fast Fourier transform-- same kind of idea-- reuse of partial results.
So in the end, what can we say about this stuff? In the end, what we can say is that it's linear in depth.","Between 46:00 and 49:00, dynamic programming also uses similar concept to avoid exponential blowup. Maybe back propagation is also a kind of dynamic programming."
uXt8qF2Zzfo,"So that's going like a zipper down that string of variables expanding each by using the chain rule until we got to the end.
So there are some expressions that provide those partial derivatives.
But now, if you'll forgive me, it was convenient to write them out that way.
That matched the intuition in my head.
But I'm just going to turn them around.
It's just a product.
I'm just going to turn them around.
So partial p2, partial w2, times partial of z, partial p2, times the partial of p with respect to z-- same thing.
And now, this one.
Keep me on track, because if there's a mutation here, it will be fatal.
Partial of p1-- partial of w1, partial of y, partial p1, partial of p2, partial of y, partial of z.",32:47 why did the y change to a partial y?
uXt8qF2Zzfo,"So we can test the other way like so.
And we can see that the desired output is pretty close to the actual output in that case too.
And it took 694 iterations to get that done.
Let's try it again.
To 823-- of course, this is all a consequence of just starting off with random weights.
By the way, if you started with all the weights being the same, what would happen? Nothing because it would always stay the same.
So you've got to put some randomization in in the beginning.
So it took a long time.
Maybe the problem is our rate constant is too small.
So let's crank up the rate counts a little bit and see what happens.
That was pretty fast.
Let's see if it was a consequence of random chance.
Run.
No, it's pretty fast there-- 57 iterations-- third try-- 67.
So it looks like at my initial rate constant was too small.
So if 0.5 was not as good as 5.0, why don't we crank it up to 50 and see what happens.
Oh, in this case, 124-- let's try it again.
Ah, in this case 117-- so it's actually gotten worse.
And not only has it gotten worse.
You'll see there's a little a bit of instability showing up as it courses along its way toward a solution.",at 41:00 .. Starting off with weights being the same would not necessarily mean they remain the same. it would if they were in same layer but here the neurons are not.. am i missing something?
uXt8qF2Zzfo,"So we got rid of it.
Now, we're a situation where we can actually take those partial derivatives, and see if it gives us a way of training the neural net so as to bring the actual output into alignment with what we desire.
So to deal with that, we're going to have to work with the world's simplest neural net.
Now, if we've got one neuron, it's not a net.
But if we've got two-word neurons, we've got a net.
And it turns out that's the world's simplest neuron.
So we're going to look at it-- not 60 million parameters, but just a few, actually, just two parameters.
So let's draw it out.
We've got input x.
That goes into a multiplier.
And it gets multiplied times w1.
And that goes into a sigmoid box like so.
We'll call this p1, by the way, product number one.
Out here comes y.
Y gets multiplied times another weight.
We'll call that w2.
The neck produces another product which we'll call p2.
And that goes into a sigmoid box.
And then that comes out as z.
And z is the number that we use to determine how well we're doing.
And our performance function p is going to be one half minus one half, because I like things are going in a direction, times the difference between the desired output and the actual output squared.","At 28:00 why did he divide by 2? Is it because of two neurons?
Does anyone know where the 1/2 comes from at 28:00?
what do we call this diagram at 27:55 ?"
uXt8qF2Zzfo,"Think about it.
If this is t, and this is minus 1, then this is minus t.
And so this thing ought to fire if everything's over-- if the sum is over 0.
So it makes sense.
And it gets rid of the threshold thing for us.
So now we can just think about weights.
But still, we've got that step function there.
And that's not good.
So what we're going to do is we're going to smooth that guy out.
So this is trick number two.
Instead of a step function, we're going to have this thing we lovingly call a sigmoid function, because it's kind of from an s-type shape.
And the function we're going to use is this one-- one, well, better make it a little bit different-- 1 over 1 plus e to the minus whatever the input is.
Let's call the input alpha.
Does that makes sense? Is alpha is 0, then it's 1 over 1 plus 1 plus one half.
If alpha is extremely big, then even the minus alpha is extremely small.
And it becomes one.
It goes up to an asymptotic value of one here.
On the other hand, if alpha is extremely negative, than the minus alpha is extremely positive.
And it goes to 0 asymptotically.
So we got the right look to that function.
It's a very convenient function.
Did God say that neurons ought to be-- that threshold ought to work like that? No, God didn't say so.
Who said so? The math says so.
It has the right shape and look and the math.
And it turns out to have the right math, as you'll see in a moment.
So let's see.
Where are we? We decided that what we'd like to do is take these partial derivatives.
We know that it was awkward to have those thresholds.
So we got rid of them.
And we noted that it was impossible to have the step function.","24:35 The input should be +1 surely?
why did he choose to smooth out the step function in 25:00?"
uXt8qF2Zzfo,"Why can't we use it? AUDIENCE: Local maxima.
PATRICK WINSTON: The remark is local maxima.
And that is certainly true.
But it's not our first obstacle.
Why doesn't gradient ascent work? AUDIENCE: So you're using a step function.
PATRICK WINSTON: Ah, there's something wrong with our function.
That's right.
It's non-linear, but rather, it's discontinuous.
So gradient ascent requires a continuous space, continuous surface.
So too bad our side.
It isn't.
So what to do? Well, nobody knew what to do for 25 years.
People were screwing around with training neural nets for 25 years before Paul Werbos sadly at Harvard in 1974 gave us the answer.
And now I want to tell you what the answer is.
The first part of the answer is those thresholds are annoying.
They're just extra baggage to deal with.
What we really like instead of c being a function of xw and t was we'd like c prime to be a function f prime of x and the weights.",at 22:28 which function is not continuous?
uXt8qF2Zzfo,"he following content is provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free.
To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at fsae@mit.edu.
PATRICK WINSTON: It was in 2010, yes, that's right.
It was in 2010.
We were having our annual discussion about what we would dump fro 6034 in order to make room for some other stuff.
And we almost killed off neural nets.
That might seem strange because our heads are stuffed with neurons.
If you open up your skull and pluck them all out, you don't think anymore.
So it would seem that neural nets would be a fundamental and unassailable topic.
But many of us felt that the neural models of the day weren't much in the way of faithful models of what actually goes on inside our heads.
And besides that, nobody had ever made a neural net that was worth a darn for doing anything.
So we almost killed it off.
But then we said, well, everybody would feel cheated if they take a course in artificial intelligence, don't learn anything about neural nets, and then they'll go off and invent them themselves.
And they'll waste all sorts of time.
So we kept the subject in.
Then two years later, Jeff Hinton from the University of Toronto stunned the world with some neural network he had done on recognizing and classifying pictures.
And he published a paper from which I am now going to show you a couple of examples.
Jeff's neural net, by the way, had 60 million parameters in it.
And its purpose was to determine which of 1,000 categories best characterized a picture.
So there it is.","4:46 The 'correct' categorization and all of the estimations are wrong? That's a lemur.
at 4:10 seems he misspoke about misclassified examples by Geoffrey Hinton's U Toronto NN. Appears the right answers (aka labels) are shaded red (second choice for the first two photos). Labels are set by the researcher for the training set - so they chose cherry instead of dalmatian in picture #3.
He got the classification errors wrong at 4:10 lol. E.g. the 'right' answer was supposed to be grille but the classifier classified it as a convertible. The image was poorly labelled."
vIFKGFl1Cn8,"We're going to see examples later on where, in fact, minimizing things where you minimize that distance is the right thing to do.
When we do machine learning, that is how you find what's called a classifier or a separator.
But actually here we're going to pick y, and the reason is important.
I'm trying to predict the dependent value, which is the y value, given an independent new x value.
And so the displacement, the uncertainty is, in fact, the vertical displacement.
And so I'm going to use y.
That displacement is the thing I'm going to measure as the distance.
How do I find this? I need an objective function that's going to tell me what is the closeness of the fit.
So here's how I'm going to do it.
I'm going to have some set of observed values.
Think of it as an array.
I've got some index into them, so the indices are giving me the x values.
And the observed values are the things I've actually measured.","Out of curiosity, at 19:01, what would trying to minimize the area of the triangle result in? as opposed to minimizing the distance y?"
vVspolIKPgc,"We'll- we'll get to that later.
But the idea is, so we're gonna use the same symbol just without even thinking about it, it's gonna represent both possibly a number, um, or a- a vector itself.
Um, now if we really want to say what the size is, we'll put a subscript.
So 0, if I write 0_3, right? Then that's the following vector.
It's the three vector with all entries 0.
That's the- that's the three vector.
Okay.
Um, uh, so we're gonna write it that way.
Um, now, there's another vector.
This- the 0 vector is absolutely standard throughout all of mathematics, um, and- and mathematical applications.
Um, now, the next vector that we're gonna en- encounter is something called the one vector or sometimes the ones vector because there are many entries in it.
Um, that's the ones vector.
And, uh, now that we are gonna distinguish a little bit and we're gonna use this bold one to represent that, right? So here- here, for example, is one, I'll try to make that bold, and this is the- the ones vector of size 2, and it's just a- a two vector with both entries equal to one, okay? So that's the ones vector.
Oh, I should add that, you know, whatever computer language you're using will have operations to create, for example, zero vectors and ones vectors and things like that.
So I'm not gonna go into that, but- and they'll be- they're may be different from our notation, but you'll see something like that.
For example, it would be ones of- of five and that would automatically give you a vect- a ones vector- five vector with all entries of one, okay? A unit vector, that's nothing that's gonna come up.",16:09 unit vector definition is missing/incorrect. A unit vector also has a magnitude of 1 such as [ 3/5 4/5 ]
wEKFGdo4Sck,"Because I've already argued to you that it's never optimal to.
I can check this.
It's not going to be optimal.
It's going to be more optimal to play some time over here.
But how far do I have to check? Well, maybe I have to check up to 7.
Does that work? Not quite.
So let's say I played here, and I played here, and I played here, and I played here, I actually can't play here, here, here, here, here.
I'm not allowed to play those.
I guess these should be O's.
I played there.
And I'm not allowed to play here, 1, 2, 3, 4, 5.
But I am allowed to play anywhere in here.
So I basically want to shrink this until these X's collide with each other.
Because then it's possible that an optimal solution would require me to pick these two and then require me to pick these two way over there.
So this is 10 things in the middle.
I only have to go up to, at most, 11.
It's 11.
Now, you can move any constant above 11 and get the same running time bound.
But that's my analysis.
OK, so we have our recursive relation.
And so what am I doing? I'm just looping over my choices of next day to play.
I'm rehearsing on this thing where I actually do play on that day.
But I'm remembering the information about what I'm allowed to play next by limiting, based on what my previous value was.
So that's the kind of key thing.
I'm remembering something further in advance-- or I'm remembering what happened in the past by describing it as a restriction of something in the future.
So this was a pretty difficult problem.
I think it was one of our first dynamic programming problems on that term.
It was probably a little ambitious.
AUDIENCE: Are you saying this recurrence goes up to 11? JASON KU: Yes, the recurrence goes up to 11, not 10, 11.",Can someone explain 36:26 to me
wEKFGdo4Sck,"Either I-- if I didn't match Ai, I may match Bj in the future.
So I only want to reduce Ai.
So xi plus 1, I leave everything else the same.
Assuming if i less than n, I don't want to move off the end of this thing, or x, i, j plus 1, ki, kj if j is less than n.
So those are my four choices.
If I match the letter n, great.
Otherwise, I decrease the size of my subproblem and I recurse.
So fun recursion, topological sort, these subproblems only depend on what? Larger i? Not quite.
Larger j? Not quite.
Changing k or-- these don't even change here.
So we're going to use depend on larger, I guess, strictly-- that's kind of an important thing, i plus j.
Because at least one of these two things is increasing.
And then the nice thing about that is it kind of tells us when we should stop.
We should stop when either i or j get to n.
We should know enough, at that point, to be able to determine if we succeeded or not, possibly.
So we have our base case.
What's the easy base case? When we succeeded.
When have we succeeded? If we have nothing left in A and B And we have nothing left in C.
I have nothing left to match.
So I have nn.
And I don't need a match anything else.
That's just going to be true.
All roads point to this subproblem to get to a true solution.
Otherwise, we have some false base cases.","1:04:22 I wonder why not x(i+1, j+1, k_i, k_j), in case we match neither of A[i] nor B[j]?"
wfr4XbR5VP8,"It doesn't hurt at all for the theorem that r1 and r2 are different.
And so there are two roots.
But ambiguity can be problematic.
And let me give you an illustration of that.
When there's ambiguity, I can do things like proving easily that 1 is equal to minus 1.
Here's the proof.
And I will let you contemplate that and try to figure out just where in this reasoning that step by step seems pretty reasonable, but nevertheless I've concluded that 1 is equal to minus 1, which is absurd.
It's taking advantage of the fact that you don't know whether the square root of minus 1 means i or minus i.
So the moral of all of this is that, first of all, be sure that you are applying the rules properly.
There's an assumption of an algebraic rule in there that isn't true.
And again, that kind of mindless calculation is risky when you don't really understand what you're doing, you don't have a clear memory of what the exact rules are.
So it's understanding that bails you out of this kind of blunder.
Let's look at 1 equals minus 1 a little because it lets us wrap up with an amusing remark.
What's terrible about 1 equals minus 1? Well, it's false, and you don't want to ever conclude something that's false.
That's worrisome.
It's disastrous when you conclude that something is false.
Let me give you an illustration of why.",Good afternon Professor. I guess that mathematically 1= -1 ( 4:55)?? The modules of |-3|=3. i didnt understand the point...
wr9gUr-eWdA,"All right, and so this would be your region R_p right now, right? And so then you can split it into these two other regions, right? Say R_1 and R_2.
And say that what you've achieved now is you have the 700 positive, 100 negatives on this side versus, uh, 200 positives and 0 negatives on this side, okay? Now, this seems like a pretty good split since you're getting out some more examples.
But what you can see is that, if you just drew the same thing again, right, R_p with 900 and 100, split split, and say in this case, instead, you've got 400 positives over here, 100 negatives, and 500 positives and 0 negatives.
So most people would argue that this bright decision boundary is better than the left one because you're basically isolating out even more positives in this case.
However, if you're just looking at your misclassification loss, it turns out that on this left one here, let's call this R_1 and R_2 versus this right one, Let's call this R_1 prime, R_2 prime, okay? So your loss of R_1 plus R_2, on this left case it's just 100 plus 0.","Why is the L(R_1) + L(R_2) = 100 + 0 at 14:50? I thought that p-hat-c is the proportion of examples in R that are of class C, not the actual number of examples in class C. Based on the misclassification loss definition L(R_1) + L(R_2) = (1 - 700/800) + (1 - 200/200) = 0.125 and L(R_1') + L(R_2') = (1 - 400/500) + (1 - 500/500) = 0.1."
wr9gUr-eWdA,"All right, so it's just 100.
And then on the right side here, it's actually still just the same, right? And in fact, if you'd look at the original loss of your parent it's also just 100, right? So you haven't really according to this loss metric changed anything at all.
And so that sort of brings up one problem with the misclassification loss is that, it's not really sensitive enough, okay? So like instead what we can do is we can define this cross-entropy loss, okay? So which we'll define as L_cross.
Let me just write this out here.
And so really what you're doing is you're just summing over the classes and it's the probability- that the proportion of elements in that class times the log of the proportion in that class.","He defines the classification loss as L(R) = 1 - max(p^c) (over all c in C classes) at 10:19. Next, he defines L(parent) -sum(L(children)) , initially stating that it needs to be minimized at 12:05 but later changing it to be maximized at 18:46. However, when comparing the two trees, he disregards the mentioned concepts and simply sums the values that will be misclassified for each child (100 + 0 for the first tree and 100 + 0 for the second) at 15:00. This approach of summing misclassified values without considering proportions is not accurate. Taking proportions into account according to L(R) = 1 - max(p^c), it turns out that the quantity to be maximized won't be the same for the two trees. For the first tree: L(R_1) + L(R_2) = (1 - 700/800) - (1 - 200/200) = 0.125. For the second tree: L(R_1') + L(R_2') = (1 - 400/500) + (1 - 500/500) = 0.1. The reason these values differ is because the quantity he defined as L(parent) - sum(L(children)) is incorrect. To have them both be the same value, the weighted average of the number of points in each should be taken. Thus we ditch misclassification loss and go for cross entropy."
wr9gUr-eWdA,"And so what I just did here is for any- this curve just represents for any p-hat c, what the cross entropy loss would look like.
Okay.
And so we can come back to this, for example, right? And if we look at this parent here right, this guy has a 10%, right? It's sort of like p-hat, p-hat for this guy is 0.1, it's 10% basically or, or I guess no, in this case, would be 0.9 sorry.
And then versus here, in these two cases, right, your p-hat, in this case, is 1 since you've got them all right, all right, and then, in this case, it's 0.8, okay? So you can sort of see since these are equal, there's the same number of examples in both of these, the p-hat of the parent is just the average of the p-hat's of the children.
Okay.
And so that's how we can sort of take this LR_parent, this LR_parent is just halfway, if we projected this down, all right.",Also please refer to 25:30 in this lecture. He calculates the p(hat) instead of the absolute loss.
wr9gUr-eWdA,"And so what is useful to define is a loss on a region, okay? So define your loss L on R, loss on R.
And so for now let's define our loss as something fairly obvious, is your misclassification loss.
It's how many examples in your region you get wrong.
And so assuming that you have, uh, given C classes total, you can define P hat c to be the proportion of examples in R that are of class c.","He defines the classification loss as L(R) = 1 - max(p^c) (over all c in C classes) at 10:19. Next, he defines L(parent) -sum(L(children)) , initially stating that it needs to be minimized at 12:05 but later changing it to be maximized at 18:46. However, when comparing the two trees, he disregards the mentioned concepts and simply sums the values that will be misclassified for each child (100 + 0 for the first tree and 100 + 0 for the second) at 15:00. This approach of summing misclassified values without considering proportions is not accurate. Taking proportions into account according to L(R) = 1 - max(p^c), it turns out that the quantity to be maximized won't be the same for the two trees. For the first tree: L(R_1) + L(R_2) = (1 - 700/800) - (1 - 200/200) = 0.125. For the second tree: L(R_1') + L(R_2') = (1 - 400/500) + (1 - 500/500) = 0.1. The reason these values differ is because the quantity he defined as L(parent) - sum(L(children)) is incorrect. To have them both be the same value, the weighted average of the number of points in each should be taken. Thus we ditch misclassification loss and go for cross entropy."
wr9gUr-eWdA,"And yeah, it turns out it doesn't really matter, which, um, which way you put it.
It just- basically, you're trying to either minimize the loss of the children or maximize the gain in information, basically.
[inaudible].
Yeah.
Let's see.
Yeah, you're right.
That should actually be a max.
Let me fix that really quick.
Because you start with your parent loss, and then you're subtracting out your children's loss, and so the amount left, let's see, the higher this loss is- yeah.
So you really want to maximize this guy.
Makes sense, everyone? Thanks for that.
Okay, so I've sort of given this like, hand-wavy- Oh, sure, what's up? [inaudible].
So that would be log-based.
The question is, for the cross-entropy loss, is it log base 2 or log base c? It's log base 2.
Okay, here, I can write that out.
Yep.
[inaudible].
Oh, sorry, I didn't quite hear that.
[inaudible].
Okay.
Um, so the question is can- uh, what is the proportion that are correct versus incorrect for these two examples we've worked through here? Um, and so, yeah- basically, what we're starting with is, we're starting with we have 900/100, 900 positives and 100 negatives.
All right, so you can imagine that if you just stopped at this point, right, you would just cla- classify everything as positive, right, and so you get 100 negatives incorrect.","He defines the classification loss as L(R) = 1 - max(p^c) (over all c in C classes) at 10:19. Next, he defines L(parent) -sum(L(children)) , initially stating that it needs to be minimized at 12:05 but later changing it to be maximized at 18:46. However, when comparing the two trees, he disregards the mentioned concepts and simply sums the values that will be misclassified for each child (100 + 0 for the first tree and 100 + 0 for the second) at 15:00. This approach of summing misclassified values without considering proportions is not accurate. Taking proportions into account according to L(R) = 1 - max(p^c), it turns out that the quantity to be maximized won't be the same for the two trees. For the first tree: L(R_1) + L(R_2) = (1 - 700/800) - (1 - 200/200) = 0.125. For the second tree: L(R_1') + L(R_2') = (1 - 400/500) + (1 - 500/500) = 0.1. The reason these values differ is because the quantity he defined as L(parent) - sum(L(children)) is incorrect. To have them both be the same value, the weighted average of the number of points in each should be taken. Thus we ditch misclassification loss and go for cross entropy."
wr9gUr-eWdA,"So really all you're trying to do is minimize this negative sum of losses of your children, okay? So let's move to the next board here.
[NOISE] So I started to find this misclassification loss.
Let's get a little bit into actually why misclassification loss isn't actually the right loss to use for this problem, so, okay? And so for a simple example, let's pretend- So I've sort of drawn out a tree like this, Let's pretend that instead we have another setup here where we're coming into a decision node.
And at this point we have 900 positives and 100 negatives, okay? So this is sort of a misclassification loss, of 100 in this case because you'd predict the most common class and end up with 100 misclassified examples.","He defines the classification loss as L(R) = 1 - max(p^c) (over all c in C classes) at 10:19. Next, he defines L(parent) -sum(L(children)) , initially stating that it needs to be minimized at 12:05 but later changing it to be maximized at 18:46. However, when comparing the two trees, he disregards the mentioned concepts and simply sums the values that will be misclassified for each child (100 + 0 for the first tree and 100 + 0 for the second) at 15:00. This approach of summing misclassified values without considering proportions is not accurate. Taking proportions into account according to L(R) = 1 - max(p^c), it turns out that the quantity to be maximized won't be the same for the two trees. For the first tree: L(R_1) + L(R_2) = (1 - 700/800) - (1 - 200/200) = 0.125. For the second tree: L(R_1') + L(R_2') = (1 - 400/500) + (1 - 500/500) = 0.1. The reason these values differ is because the quantity he defined as L(parent) - sum(L(children)) is incorrect. To have them both be the same value, the weighted average of the number of points in each should be taken. Thus we ditch misclassification loss and go for cross entropy."
x-Ik9YAFAPo,"It's like, you don't need to have a negation gadget.
So why not skip it? But there you go.
Now I'll just mention I'm not a fan of the word monotone here because here we have monotone to mean all positive or all negative.
Here we mean all positive.
Not ideal reuse of terminology.
I think that's why sometimes this is all positive 1-in-3SAT.
Anyway, it's a bit of a mess.
But that is the state of the literature.
So you get it all.
All right.
Here's another problem.
Monotone not-exactly-1 3SAT.
I should not have any suspense here.
This is NP.
OK? This is NP-complete.
This is also NP-complete.
But not-exactly-1 3SAT is polynomial.
So I think you know what it means.
A clause specifies that-- again, we take an and of clauses.
And we want zero, two, or three of three variables are true.
In other words, exactly one of them is false.
No.
I don't mean that.
Exactly one of them being false would be 1-in-3SAT again, just by negating everything, which we're allowed to do if we want to.
But this is different.
This is saying, it could be everything's false, or it could be one thing is false, or it could be zero things are false.
But not two things are false.
OK? This turns out to be polynomial.
And do I have-- oh.
There's one funny thing here, which is if all your clauses look like this, you can set all your variables to false.
So this is sort of a trivial problem.
But to make it more interesting, you can say x1 equals true, just to get you started.
So there's no trivial solution then and it still turns out this is easy.
Because if you think about this long enough, as I did yesterday, this will look something like-- if you have three variables, either they're all false-- then fine-- or if one of them is true, then you better have another one true.","Hello, there is an error in the transcript at 26:14. The transcript says ""This is NP"", however the professor is noting that the problem is in P, not in NP."
xFzvwTugIto,"But you can have variable n.
OK, thank you.
And then, asked in the chat, is task grouping considered a challenge? We see task grouping as a challenge in multi-task learning.
And I was reading a recent paper that comes up with a task grouping framework to deal with it.
Yeah, so different tasks may be more similar to one another.
Like, you could construct a task distribution that has medical images and natural images, for example.
And it may actually be helpful to learn a grouping of those tasks, if you find that your algorithm isn't finding common ground between those two sets of tasks.
And yeah, that's definitely a challenge that people have looked at, especially when you observe negative transfer between different sets of tasks.
Cool, and then one more question from [AUDIO OUT]..
What do we have, what to do if we have varying k for each task? Yeah, so you can also have varying k.","Thank you for the excellent content and for addressing the questions. In 49:05, aren't medical and natural images of different distributions? Any help to clarify the assumption that they are from i.i.d will be appreciated. Personally, they look further apart than sinusoidal and the rest of the periodic functions."
xGDUYQGvRac,"And then, how is this-- how are we going to stop the generation-- if the edge level RNN is going to output the end of sequence at step 1, we know that no edges are connected to the new node, and we are going to stop the generation.
So it's actually the edge level RNN, and that, we have decided will determine whether we stop generating the graph or not.
So let me give you now an example.
So that you see how all this fits together.
So this is what is going to happen under the training time.
For, let's say, a given training-- observed training graph.
We are starting with the start of sequence and a hidden state.
The node level RNN will add the node, and then, the edge level RNN will be asked, shall this node that has just been added, shall it link to the previous nodes? Yes or no.
It will update the probability.
And we are then going to flip a coin that will determine-- with this given bias that will determine whether the edge is added or not.
And then, and then, we'll take this and use it as an input-- as an initialization back to the node level RNN who's now going to add the second node, and this would be node number 3.
And then, the edge level RNN is going to tell us, will node 3 link to node 1, will node 3 link to node 2.
And again, it's outputting probabilities, we are flipping the coins, whatever is the output of that coin is the input to the next level RNN.
So here are the probabilities 0.6.
Perhaps you were lucky, the output was 1, so this is the input for the next state.
And then, after we have traversed with all the previous edges, we are going over all the previous nodes, we are again going to ask node RNN to generate a new node.","22:12 - ""whatever the output of that coin is the input to the next level"". Is it a mistake? As we are using teacher forcing we should use real values (edge exists/or not) as I anderstand."
xMWcIb6XGVA,"OK, so ADE-- if I had ADE, I could do B-- that seems OK, D seems bad-- F, or H.
So it would look like B, F, H.
OK, ADG.
A, D, G.
It looks like H is my only option.
ABCF.
A, B, C, F.
Looks like I could do E or I.
Finally.
Now the only question is whether I got the right number of states.
Let's assume I did.
So 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 -- which happens to be the right answer.",Mistake at 57:34. Should be ADEH.
z0lJ2k0sl1g,"But it's going to take some work to do that.
So this assumption requires assuming that the keys are random.
And this is what we would call an average case analysis.
You might think that average case analysis is necessary for randomized algorithms, but that's not true.
And we saw that last week with quicksort.
Quicksort, if you say I will always choose a of 1 to be my partition element, that's what the textbook calls basic quicksort, then for an average input that will do really well.
If you have a uniform random permutation of items and you sort with the method of always choosing the first item as your partition, then that will be n log n on average if your data is average.
But we saw we could avoid that assumption by choosing a random pivot.
If you choose a random pivot, then you don't need to assume anything about the input.
You just need to assume that the pivots are random.
So it's a big difference between assuming your inputs are random versus assuming your coin flips are random.","10:55 Correction, when he says a[1] he means to say a[0] which is the first element in the array."
z0lJ2k0sl1g,"So remember dot products are just the sum from i equals 0 to r minus 1 of a1 times ki.
I want to do all of that modulo m.
We'll worry about how long this takes to compute in a moment I guess.
Maybe very soon.
But the hash family h is just all of these ha's for all possible choices of a.
a was a key so it comes from the universe u.
And so what that means is to do universal hashing, I want to choose one of these ha's uniformly at random.
How do I do that? I just choose a uniformly at random.
Pretty easy.
It's one random value from one random key.
So that should take constant time and constant space to store one number.
In general we're in a world called the Word RAM model.
This is actually-- I guess m stands for model so I shouldn't write model.
Random access machine which you may have heard.
The word RAM assumes that in general we're manipulating integers.
And the integers fit in a word.
And the computational assumption is that manipulating a constant number of words and doing essentially any operation you want on constant number of words takes constant time.","Dear All, I just checked the CLRS book and also listened to this lecture. I think there is one step they did not elaborate: how can universal hashing be repeatable? after I map a particular key k-particular to a slot, if I look it up in the future, how do I get the ""a"" that was choosed randomly to do dot product with k-particular? ps. ""a"" here means : 569 00:35:40,680 --> 00:35:45,690 But the hash family h is just all of these ha's 570 00:35:45,690 --> 00:35:48,560 for all possible choices of a. 571 00:35:52,276 --> 00:35:56,860 a was a key so it comes from the universe u."
z0lJ2k0sl1g,"We'll rewrite it.
The probability is over a.
I'm choosing a uniformly at random.
I want another probability that that maps k and k' to the same slot.
So let me just write out the definition.
It's probability over a that the dot product of a and k is the same thing as when I do the dot product with k' mod m.
These two, that sum should come out the same, mod m.
So let me move this part over to this side because in both cases we have the same ai.
So I can group terms and say this is the probability-- probability sum over i equals 0 to r minus 1 of ai times ki minus ki prime equals 0.
Mod m.
OK, no pun intended.
Now we care about this digit d.
d is a place where we know that this is non-zero.
So let me separate out the terms for d and everything but d.
So this is the same as ability of, let's do the d term first, so we have ad times kd minus kd prime.
That's one term.
I'm going to write the summation of i not equal to d of ai ki minus ki prime.
These ones, some of them might be zero.
Some are not.
We're not going to worry about it.
It's enough to just isolate one term that is non-zero.
So this thing we know does not equal zero.
Cool.
Here's where I'm going to use a little bit of number theory.
I haven't yet used that m is prime.
I required m is prime because when you're working modulo m, you have multiplicative inverses.","44:12 those two sums only need to be equivalent modulo m, not equal. So I dont get your demonstration."
zM5MW5NKZJg,"Come back, go to this vertex, leave it.
And so, observe that, whenever you're making this drawing, you go to a vertex, and you leave it.
Every time you hit a vertex, you leave it.
Since there's the circuit, you just loop around, and every time you enter a vertex, you'll have to leave it.
What that means is that, even though this is not directed, if you drew out the actual path, you would see, the number of edges going into a vertex is equal the number of ones leaving, which means that every degree has to be even.
So if you go and look at this graph, which is that of lobes, this degree is not even.
Neither is this, neither is this, neither is this.
They're all 5, I think, yeah.
They're all 5, which means that this can never have an Euler circuit.
So a graph can only have an Euler circuit if it has even degrees for every vertex.
And the other way is also true.
If a graph has even degrees on every vertex, then it must have an Euler circuit.
That's not hard to prove, but there's a constructive algorithm you can use.
So, let's say, you're given this graph, for instance.
You would simply go to the graph, just start at some random node, and then go through, and keep following edges until you can no longer following edges.
So, let's say, you stop here.
Then, you pick another edge, and start, and so on.
And, then, you can splice these cycles together at some point.
So it's kind of a [INAUDIBLE] argument, but it should be sort of intuitive why you can construct another path, given an even degree.
You just perform searches.
You just create cycles, and you splice them together.
But, for now, just take it as fact that a graph is an Euler circuit, as in you can draw it without lifting your pen off, if, and only if, every vertex has even degree, so why is that interesting? So, let's say, we could add some edges to our tree and turn it into one of those nice graphs.","He makes 3 mistakes, just one is a major mistake and the mistake is in 19:17, if understood properly he said that euler circuit only has solution if and only if every vertex has even degree which is false. e.g: A house with an X in the middle [X]> the bottom vertex has 3 edges and the other two has 4 edges and the last one has 2 edges. And there is an euler circuit. Please check the theoream on wiki to verify."
zM5MW5NKZJg,"Realize that this is not a valid cycle, but this is a valid cycle.
So, now, we need to show that this is somehow bounded by a star G.
So how do you do that? Well, look at H star G.
What is H star G? H star G is just a cycle, which goes through the optimal cycle, which goes through all the vertices and comes back to the parent vertex.
So this is H star of G.
This is the optimal thing.
Now, you can take an edge, e, here, and delete it.
And then you'll get a spanning tree, because this is your optimal cycle.
Remove one edge, and you get a spanning tree.
So let's call that T dash.
Does that make sense, why that is a spanning tree? Because you had a cycle, and you remove one edge, so it touches all the vertices, and it's a tree.
So it's a spanning tree, but it's not the minimum spanning tree.
So you know that H star G, the cost of H star of G, is greater than equal to the cost of H star of G minus the edge we removed, is greater than equal to the cost of T.
Make sense? So you remove one edge, and then that is still greater than the minimum spanning tree.
So, now, combining this guy and this guy, you get cost of C dash is less than equal to 2 [INAUDIBLE]..
We know that cost of C is less than cost of H of G, so you get a 2-approximation.
So does that make sense? So that was a 2-approximation.
That was pretty straightforward, We just constructed a spanning tree.
You did a DFS traversal and removed duplicates, and you have a nice path.","Super great Video! Thanks Amartya Shankha Biswas! Btw, in around 11:40 when he derives the 2-approximation it should be C' and not C. C' is the solution derived by the approximation algorithm."
zUazLXZZA2U,"What is this term? Let's go there.
Yeah.
Sigmoid.
So Sigmoid.
I'm just going to write it a_2 times 1 minus a_2.
Does that make sense? Sigmoid times 1 minus Sigmoid.
What is this term? Uh, oh sorry my bad.
That's not the right one.
This one, this one is that.
This one is Sigmoid.
a_2 is Sigmoid of z_2.
So this result comes from this term.
Was- what about this term? w_3.
Sorry.
w_3.
w_3.
Is it w_3 or no? I heard transpose.
How do we know if it's w_3 or w_3 transpose? So let's look at the shape of this.
What's z_3? One by one.
It's one by one.
It's a scalar.
It's the linear part of the last neuron.
What's the shape of that? This is 2, 1.
We have two neurons in the layer.
w_3.
We said that it was a 1 by 2 matrix, so we have to transpose it.
So the result of that is w_3 transpose.
And how about the last term? Same as here.
One layer before.
Yeah, someone said they won't transpose.
Okay.
Yeah? The numbers are [inaudible] that one.
This one? Yeah.
There is a transpose here.
[inaudible] w_5.
Oh yeah, yeah.
You're correct.
You're correct.
Thank you.
That's what you mean? Yeah.
Yeah.
This one was from the z_3, to w_2.
We didn't end up using that because we will get stuck, so there's no a_2 transpose here.
Thanks.
Any other questions or remarks? So that's cool.
Let's, let's, let's write- let's write down our derivative cleanly on the board.
So we have derivative of our loss function with respect to w_2, which seems to be equal to a_3 minus y, from the first term.
The second term seems to be equal to, uh, w_3 transpose.
Then we have a term which is a_2 times 1 minus a_2.","26:00 - this ""using the shape"" makes me nervous. Will it always be the case that that will give you the right answer? Are their possible architectures in which some of these matrices might be square? In that case the transpose has the same dimension as the original. I'd like to just know what I'm doing well enough to know what to do without needing a ""cue"" of that sort."
zUazLXZZA2U,"You just will compute the forward propagation, compute the backdrop, look at the direction and go to that direction.
What momentum is going to say is look at the past updates that you did and try to consider these past updates in order to find the right way to go.
So if you look at the past update and you take an average of the past update.
You would take an average of these update going up and the update after it going down.
The average on the vertical side is going to be small, because one went up, one went down.
But on the horizontal axis, both went to the same direction.
So the update will not change too much on the vert- on- on this axis.
So you're most likely to do something like that if you use momentum.
Does it make sense the intuition behind it? So that's the intuition why we want to use momentum.
And for those of you who do physics, sometimes you can think of momentum as friction.
You know like- like if you- if you launch a rocket and you wanna move it quickly around.
It's not gonna move, because the rocket has a certain weight and has a certain momentum.
You cannot change its direction very, very noisily.
[NOISE] So let's see the implementation of- of- of momentum gradient descent.
Oh, and I believe we- we're almost done, right? Yeah.
Okay.
[NOISE] So let's look at the- the implementation quickly.
So gradient descent was w equals w minus Alpha, derivative of the loss with respect to w.
What we are going to do is we're going to use another variable called velocity, which is going to be the average of the previous velocity and the current weight updates.
So we're going to use that, and instead of the updates being the derivative directly, we're going to update the velocity.
So the velocity is going to be a variable that tracks the direction that we should take regarding the current update and also the past updates with a factor Beta that is be- going to be the weights.","1:14:30 - Well, the analogy of the rocket was a good one, but the momentum method is not like friction in any way. Friction never ""keeps you going"" - it only ever tries to slow you down (or keep you from starting to move). It universally opposes whatever motion is going on or trying to go on."
zcvsyL7GtH4,"There's one axiom that covers things adequately, and that is that if two sets have the same members, then they are members of the same sets.
So if all the members of x and y are the same, then x and y are members of exactly the same thing, which we could say this way, for every x, y is an x, if and only if z is an x.
So that is one of the basic axioms of Set Theory, maybe the starting one.
Another one is the Power Set axiom, which simply says that every set has a power set.
How would you say that in the language of predicate set theory? Well, you'd say that for every x, there is a p, which is going to be the power set effects, such that for every set s, s is a subset of x, if and only if s is a member of p.","Is it really ""x and y are members of the same set"" in the axiom of extentionality? Shouldn't it be ""y and z""? It doesn't make sense the way it is now. 1:30 And it's a bit confusing with X in both parts because X on the top is not the same as X on the bottom as far as I understand."
